{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Required Embeddings\n",
    "\n",
    "Note: This section can be skipped if embeddings are already prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "# sys.setdefaultencoding() does not exist, here!\n",
    "# reload(sys)  # Reload does the trick!\n",
    "# sys.setdefaultencoding('UTF8')\n",
    "\n",
    "# sys.setdefaultencoding('UTF8')\n",
    "\n",
    "data_file= \"data/drug_review/drugsComTrain_raw.tsv\"        # default='', help='a .txt file containing the corpus'\n",
    "emb_file= \"embeddings/embeddings.txt\"                      #default='embeddings.txt', help='file to save the word embeddings'\n",
    "dim_rho= 300                                               #default=300, help='dimensionality of the word embeddings'\n",
    "min_count= 2                                               #default=2, help='minimum term frequency (to define the vocabulary)'\n",
    "sg= 1                                                      # default=1, help='whether to use skip-gram'\n",
    "workers= 6                                                 #default=25, help='number of CPU cores'\n",
    "negative_samples= 10                                       # default=10, help='number of negative samples'\n",
    "window_size= 4                                             # default=4, help='window size to determine context'\n",
    "iters= 50                                                  #default=50, help='number of iterationst'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class for a memory-friendly iterator over the dataset\n",
    "class MySentences(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.file_type = 'text'\n",
    " \n",
    "    def __iter__(self):\n",
    "        if self.file_type == 'text':\n",
    "            for line in open(self.filename,encoding=\"utf8\"):\n",
    "                yield line.split()\n",
    "        elif self.file_type == 'csv':\n",
    "            for line in self.reviews.values:\n",
    "                yield line.split()\n",
    "                \n",
    "    def __init__(self, filename,col,delimiter = \"\\t\"):\n",
    "        self.filename = filename\n",
    "        data = pd.read_csv(filename,delimiter=delimiter)\n",
    "        self.reviews = data[col][:1000]\n",
    "        self.file_type = 'csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MySentences(data_file,\"review\") # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(sentences, min_count=min_count, sg=sg, size=dim_rho, \n",
    "    iter=iters, workers=workers, negative=negative_samples, window=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Write the embeddings to a file\n",
    "with open(emb_file, 'w') as f:\n",
    "    for v in list(model.wv.vocab):\n",
    "        vec = list(model.wv.__getitem__(v))\n",
    "        f.write(v + ' ')\n",
    "        vec_str = ['%.9f' % val for val in vec]\n",
    "        vec_str = \" \".join(vec_str)\n",
    "        f.write(vec_str + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# abc = pickle.load(\"data/20ng/vocab.pkl\")\n",
    "# abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import data\n",
    "import scipy.io\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from etm import ETM\n",
    "from utils import nearest_neighbors, get_topic_coherence, get_topic_diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/drug_review/drugsComTrain_raw.tsv\",delimiter=\"\\t\")[:1000]\n",
    "# df.to_csv(\"data/drug_review/drugs_train_1000.csv\",index=None)\n",
    "# reviews = df.review\n",
    "# with open(\"train_file.txt\", 'w',encoding='utf8') as f:\n",
    "#     for review in reviews.values:\n",
    "#         f.write(review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"20ng\"\n",
    "\n",
    "dataset =   \"train_file.txt\"                      #default='20ng', help='name of corpus'\n",
    "data_path = 'data/drug_review/'#default='data/20ng', help='directory containing data'\n",
    "emb_path = 'embeddings/embeddings.txt'#default='data/20ng_embeddings.txt', help='directory containing word embeddings'\n",
    "save_path = './results'#default='./results', help='path to save results'\n",
    "batch_size = 100 #default=1000, help='input batch size for training'\n",
    "\n",
    "### model-related arguments\n",
    "num_topics = 25   #default=50, help='number of topics'\n",
    "rho_size = 300    #default=300, help='dimension of rho'\n",
    "emb_size = 300    #default=300, help='dimension of embeddings'\n",
    "t_hidden_size = 800 #default=800, help='dimension of hidden space of q(theta)'\n",
    "theta_act = 'relu' #default='relu', help='tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu)'\n",
    "train_embeddings = 0 #default=0, help='whether to fix rho or train it'\n",
    "\n",
    "### optimization-related arguments\n",
    "lr = 0.05 # default=0.005, help='learning rate'\n",
    "lr_factor =4.0  #default=4.0, help='divide learning rate by this...'\n",
    "epochs = 20 # default=20, help='number of epochs to train...150 for 20ng 100 for others'\n",
    "mode = 'train'# default='train', help='train or eval model'\n",
    "optimizer = 'adam'# default='adam', help='choice of optimizer'\n",
    "seed = 2019# default=2019, help='random seed (default: 1)\n",
    "enc_drop = 0.0# default=0.0, help='dropout rate on encoder'\n",
    "clip = 0.0# default=0.0, help='gradient clipping'\n",
    "nonmono = 10# default=10, help='number of bad hits allowed'\n",
    "wdecay = 1.2e-6# default=1.2e-6, help='some l2 regularization'\n",
    "anneal_lr = 0#  default=0, help='whether to anneal the learning rate or not'\n",
    "bow_norm = 1# default=1, help='normalize the bows or not'\n",
    "\n",
    "### evaluation, visualization, and logging-related arguments\n",
    "num_words = 10  # default=10, help='number of words for topic viz' \n",
    "log_interval = 2 # default=2, help='when to log training'\n",
    "visualize_every = 1 # default=10, help='when to visualize results'\n",
    "eval_batch_size = 1000 # default=1000, help='input batch size for evaluation'\n",
    "load_from = 'results/etm_20ng_K_50_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_1'# default='', help='the name of the ckpt to eval from'\n",
    "tc = 0# default=0, help='whether to compute topic coherence or not'\n",
    "td = 0# default=0, help='whether to compute topic diversity or not'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20b1dc03b10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")#(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('\\n')\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, train, valid, test = data.get_data(os.path.join(data_path))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 1. training data\n",
    "train_tokens = train['tokens']\n",
    "train_counts = train['counts']\n",
    "num_docs_train = len(train_tokens)\n",
    "\n",
    "# 2. dev set\n",
    "valid_tokens = valid['tokens']\n",
    "valid_counts = valid['counts']\n",
    "num_docs_valid = len(valid_tokens)\n",
    "\n",
    "# 3. test data\n",
    "test_tokens = test['tokens']\n",
    "test_counts = test['counts']\n",
    "num_docs_test = len(test_tokens)\n",
    "# test_1_tokens = test['tokens_1']\n",
    "# test_1_counts = test['counts_1']\n",
    "# num_docs_test_1 = len(test_1_tokens)\n",
    "# test_2_tokens = test['tokens_2']\n",
    "# test_2_counts = test['counts_2']\n",
    "# num_docs_test_2 = len(test_2_tokens)\n",
    "\n",
    "embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n"
     ]
    }
   ],
   "source": [
    "#/usr/bin/python\n",
    "## get data\n",
    "# 1. vocabulary\n",
    "\n",
    "if not train_embeddings:\n",
    "    emb_path = emb_path\n",
    "    vect_path = os.path.join(data_path.split('/')[0], 'vocab.pkl')   \n",
    "    vectors = {}\n",
    "    with open(emb_path, 'rb') as f:\n",
    "        for l in f:\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "            if word in vocab:\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                vectors[word] = vect\n",
    "    embeddings = np.zeros((vocab_size, emb_size))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(vocab):\n",
    "        try: \n",
    "            embeddings[i] = vectors[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            embeddings[i] = np.random.normal(scale=0.6, size=(emb_size, ))\n",
    "    embeddings = torch.tensor(embeddings).to(device)\n",
    "    embeddings_dim = embeddings.size()\n",
    "\n",
    "print('=*'*100)\n",
    "# print('Training an Embedded Topic Model on {} with the following settings: {}'.format(dataset.upper()))\n",
    "print('=*'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embeddings/embeddings.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.0)\n",
      "  (theta_act): ReLU()\n",
      "  (alphas): Linear(in_features=300, out_features=25, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=87165, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=25, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=25, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## define checkpoint\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "if mode == 'eval':\n",
    "    ckpt = load_from\n",
    "else:\n",
    "    ckpt = os.path.join(save_path, \n",
    "        'etm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_trainEmbeddings_{}'.format(\n",
    "        dataset, num_topics, t_hidden_size, optimizer, clip, theta_act, \n",
    "            lr, batch_size, rho_size, train_embeddings))\n",
    "\n",
    "## define model and optimizer\n",
    "model = ETM(num_topics, vocab_size, t_hidden_size, rho_size, emb_size, \n",
    "                theta_act, embeddings, train_embeddings, enc_drop).to(device)\n",
    "\n",
    "print('model: {}'.format(model))\n",
    "\n",
    "if optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'asgd':\n",
    "    optimizer = optim.ASGD(model.parameters(), lr=lr, t0=0, lambd=0., weight_decay=wdecay)\n",
    "else:\n",
    "    print('Defaulting to vanilla SGD')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    acc_loss = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    cnt = 0\n",
    "    indices = torch.randperm(num_docs_train)\n",
    "    indices = torch.split(indices, batch_size)\n",
    "    for idx, ind in enumerate(indices):\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        data_batch = data.get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "        sums = data_batch.sum(1).unsqueeze(1)\n",
    "        if bow_norm:\n",
    "            normalized_data_batch = data_batch / sums\n",
    "        else:\n",
    "            normalized_data_batch = data_batch\n",
    "        recon_loss, kld_theta = model(data_batch, normalized_data_batch)\n",
    "        total_loss = recon_loss + kld_theta\n",
    "        total_loss.backward()\n",
    "\n",
    "        if clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_loss += torch.sum(recon_loss).item()\n",
    "        acc_kl_theta_loss += torch.sum(kld_theta).item()\n",
    "        cnt += 1\n",
    "\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            cur_loss = round(acc_loss / cnt, 2) \n",
    "            cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "            cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "\n",
    "            print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                epoch, idx, len(indices), optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    \n",
    "    cur_loss = round(acc_loss / cnt, 2) \n",
    "    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "    cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "    print('*'*100)\n",
    "    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "            epoch, optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(m, show_emb=True):\n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "\n",
    "    m.eval()\n",
    "\n",
    "    queries = ['skin','cycle','effects','price','worst','best','neuropsychologist','efficacy','performance','cancer']\n",
    "\n",
    "    ## visualize topics using monte carlo\n",
    "    with torch.no_grad():\n",
    "        print('#'*100)\n",
    "        print('Visualize topics...')\n",
    "        topics_words = []\n",
    "        gammas = m.get_beta()\n",
    "        for k in range(num_topics):\n",
    "            gamma = gammas[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if show_emb:\n",
    "            ## visualize word embeddings by using V to get nearest neighbors\n",
    "            print('#'*100)\n",
    "            print('Visualize word embeddings by using output embedding matrix')\n",
    "            try:\n",
    "                embeddings = m.rho.weight  # Vocab_size x E\n",
    "            except:\n",
    "                embeddings = m.rho         # Vocab_size x E\n",
    "            neighbors = []\n",
    "            for word in queries:\n",
    "                print('word: {} .. neighbors: {}'.format(\n",
    "                    word, nearest_neighbors(word, embeddings, vocab)))\n",
    "            print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(m, source, tc=False, td=False):\n",
    "    \"\"\"Compute perplexity on document completion.\n",
    "    \"\"\"\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        if source == 'val':\n",
    "            indices = torch.split(torch.tensor(range(num_docs_valid)), eval_batch_size)\n",
    "            tokens = valid_tokens\n",
    "            counts = valid_counts\n",
    "        else: \n",
    "            indices = torch.split(torch.tensor(range(num_docs_test)), eval_batch_size)\n",
    "            tokens = test_tokens\n",
    "            counts = test_counts\n",
    "\n",
    "        ## get \\beta here\n",
    "        beta = m.get_beta()\n",
    "\n",
    "        ### do dc and tc here\n",
    "        acc_loss = 0\n",
    "        cnt = 0\n",
    "        indices_1 = torch.split(torch.tensor(range(num_docs_test_1)), eval_batch_size)\n",
    "        for idx, ind in enumerate(indices_1):\n",
    "            ## get theta from first half of docs\n",
    "            data_batch_1 = data.get_batch(test_1_tokens, test_1_counts, ind, vocab_size, device)\n",
    "            sums_1 = data_batch_1.sum(1).unsqueeze(1)\n",
    "            if bow_norm:\n",
    "                normalized_data_batch_1 = data_batch_1 / sums_1\n",
    "            else:\n",
    "                normalized_data_batch_1 = data_batch_1\n",
    "            theta, _ = m.get_theta(normalized_data_batch_1)\n",
    "\n",
    "            ## get prediction loss using second half\n",
    "            data_batch_2 = data.get_batch(test_2_tokens, test_2_counts, ind, vocab_size, device)\n",
    "            sums_2 = data_batch_2.sum(1).unsqueeze(1)\n",
    "            res = torch.mm(theta, beta)\n",
    "            preds = torch.log(res)\n",
    "            recon_loss = -(preds * data_batch_2).sum(1)\n",
    "            \n",
    "            loss = recon_loss / sums_2.squeeze()\n",
    "            loss = loss.mean().item()\n",
    "            acc_loss += loss\n",
    "            cnt += 1\n",
    "        cur_loss = acc_loss / cnt\n",
    "        ppl_dc = round(math.exp(cur_loss), 1)\n",
    "        print('*'*100)\n",
    "        print('{} Doc Completion PPL: {}'.format(source.upper(), ppl_dc))\n",
    "        print('*'*100)\n",
    "        if tc or td:\n",
    "            beta = beta.data.cpu().numpy()\n",
    "            if tc:\n",
    "                print('Computing topic coherence...')\n",
    "                get_topic_coherence(beta, train_tokens, vocab)\n",
    "            if td:\n",
    "                print('Computing topic diversity...')\n",
    "                get_topic_diversity(beta, 25)\n",
    "        return ppl_dc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Visualizing model quality before training...\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0: ['dancequotany', 'reviewxx', 'touching', 'implore', 'beggars', 'withdrawalsand', 'bequotmorequot', 'becaus', 'stigmatizm']\n",
      "Topic 1: ['flarei', 'hypoxmia', 'yyaaaa', 'janumetx', 'arthroc', 'unwind', 'someweird', 'primitive', 'milkywhite']\n",
      "Topic 2: ['relearned', 'regulari', 'cod', 'invegai', 'quotenduringquot', 'periodspotting', 'sleepynessauditory', 'lsquojollyrsquo', 'dizzinessdid']\n",
      "Topic 3: ['quotunderwaterquot', 'cruch', 'quotwillpower', 'benzoids', 'inhe', 'laundry', 'scapula', 'finances', 'contolling']\n",
      "Topic 4: ['genericpregnancy', 'tabletsmg', 'jesting', 'quotzilchesquot', 'maralthia', 'sciatica', 'wellaches', 'quotserenquot', 'venlafazxine']\n",
      "Topic 5: ['craigadkinslipozene', 'lightskinned', 'workincreased', 'selfcritical', 'neuropsychologist', 'advert', 'optomistic', 'chancequot', 'reheats']\n",
      "Topic 6: ['distrophy', 'lastbut', 'offset', 'twisthaler', 'shotnothing', 'neurotransmission', 'awarness', 'aorta', 'phent']\n",
      "Topic 7: ['fuzzier', 'quottypical', 'crushes', 'workingits', 'linagliptin', 'sorrytmii', 'plastic', 'province', 'vinegarperoxide']\n",
      "Topic 8: ['botch', 'deficient', 'laproscopic', 'monthshighly', 'mirvoso', 'agrivating', 'sleepyhead', 'perfumy', 'painfullyunbearably']\n",
      "Topic 9: ['survived', 'hpts', 'dysfunction', 'thisamazing', 'antiboitics', 'shoulda', 'bruisingi', 'sooiantra', 'discussed']\n",
      "Topic 10: ['themand', 'paragardsince', 'rejoiced', 'goiter', 'ahr', 'thickening', 'drinkingto', 'centuries', 'bueano']\n",
      "Topic 11: ['intructions', 'ambiencr', 'pry', 'trimethoprim', 'emotionalno', 'knit', 'luetin', 'walgreens', 'spared']\n",
      "Topic 12: ['fatheri', 'hopein', 'aroundi', 'infectionacne', 'alice', 'situationps', 'smartly', 'brochitis', 'lyza']\n",
      "Topic 13: ['unlikely', 'clavulan', 'sps', 'desires', 'hypertension', 'depressionhowever', 'represcribed', 'mildbut', 'oss']\n",
      "Topic 14: ['unasisted', 'quotoutofitquot', 'bubbly', 'withdrawalsnd', 'robin', 'fogfatigue', 'sleepssecond', 'sacyllic', 'childrengrandchildren']\n",
      "Topic 15: ['contipation', 'papsmear', 'downout', 'godsendafib', 'quotstabbingquot', 'stuffiness', 'afterall', 'overproduce', 'mghelped']\n",
      "Topic 16: ['relieving', 'hormonesso', 'dramaticallyit', 'xmgdaily', 'draped', 'tim', 'antideprssing', 'jeez', 'meani']\n",
      "Topic 17: ['milddid', 'sadangry', 'restasismy', 'memorylithium', 'twirled', 'ocassions', 'menstrul', 'helpat', 'quotbang']\n",
      "Topic 18: ['amoxocillian', 'estraceestradoil', 'controlwithout', 'fiirst', 'clobetason', 'hamper', 'preiud', 'horizant', 'treclin']\n",
      "Topic 19: ['suspicion', 'magnificent', 'sincetheres', 'visiblemy', 'healedweak', 'quotboilsquot', 'dropsquothypo', 'techinal', 'annabelle']\n",
      "Topic 20: ['restwat', 'nailed', 'seasonsique', 'qualificationsquot', 'angerirritated', 'abort', 'quoteverquot', 'ranted', 'prolific']\n",
      "Topic 21: ['daybeing', 'plusesno', 'ovule', 'legsmy', 'heavytoo', 'metronizole', 'beginningstart', 'psychiatristi', 'intrathecal']\n",
      "Topic 22: ['branches', 'betterabout', 'bag', 'remissionafter', 'magenta', 'seroquelall', 'simmered', 'inconsistencies', 'teamed']\n",
      "Topic 23: ['onedr', 'disneylandonce', 'flatly', 'stethoscopequot', 'avalox', 'comprehension', 'canister', 'paterner', 'daysmy']\n",
      "Topic 24: ['quotrecipequot', 'conflict', 'tightfeeling', 'irreversibly', 'egweek', 'kepprage', 'mfr', 'xgevia', 'bicalutamide']\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "vectors:  (87165, 300)\n",
      "query:  (300,)\n",
      "word: skin .. neighbors: ['skin', 'red', 'forehead', 'blotches', 'burns', 'rod', 'ankles', 'broken', 'clearer', 'clear', 'turns', 'looks', 'grown', 'irritated', 'uterus', 'washed', 'itchy', 'kid', 'face', 'oily']\n",
      "vectors:  (87165, 300)\n",
      "query:  (300,)\n",
      "word: cycle .. neighbors: ['cycle', 'thrush', 'breakthrough', 'occur', 'foot', 'implanon', 'cheek', 'boobs', 'stages', 'milk', 'cystic', 'teens', 'bloody', 'asthma', 'rod', 'bottle', 'appointment', 'fertile', 'chin', 'bc']\n",
      "vectors:  (87165, 300)\n",
      "query:  (300,)\n",
      "word: effects .. neighbors: ['effects', 'side', 'sedative', 'adverse', 'affects', 'hangover', 'hairs', 'ridiculously', 'sexual', 'thus', 'besides', 'wishes', 'heartburn', 'benefit', 'sweats', 'reasons', 'debilitating', 'etc', 'whatsoever', 'apart']\n",
      "vectors:  (87165, 300)\n",
      "query:  (300,)\n",
      "word: price .. neighbors: ['price', 'orgasm', 'wash', 'expensive', 'promise', 'paying', 'uric', 'heals', 'bullet', 'scary', 'company', 'admit', 'stressing', 'costs', 'balm', 'option', 'flow', 'palpitations', 'performance', 'cuticles']\n",
      "vectors:  (87165, 300)\n",
      "query:  (300,)\n",
      "word: worst .. neighbors: ['worst', 'orsythia', 'part', 'best', 'incredibly', 'anyways', 'convenient', 'ring', 'heartburn', 'decision', 'preventing', 'urgency', 'colon', 'flared', 'checked', 'favorite', 'purchased', 'dehydrated', 'violent', 'quality']\n",
      "vectors:  (87165, 300)\n",
      "query:  (300,)\n",
      "word: best .. neighbors: ['best', 'favorite', 'solution', 'purchased', 'chill', 'fewer', 'worst', 'heard', 'linked', 'phenomenal', 'admit', 'bullet', 'skeptical', 'approved', 'ability', 'attribute', 'excellent', 'overall', 'exercising', 'idea']\n",
      "vectors:  (87165, 300)\n",
      "query:  (300,)\n",
      "word: neuropsychologist .. neighbors: ['neuropsychologist', 'perviously', 'sensationt', 'siaticia', 'snot', 'haveany', 'favorably', 'nousea', 'creactive', 'swingsdepressionanxiety', 'theanine', 'upheavalafter', 'allso', 'massage', 'dribbles', 'infliximabremicade', 'picolinate', 'benzodiapines', 'officeemotional', 'crystallized']\n",
      "vectors:  (87165, 300)\n",
      "query:  (300,)\n",
      "word: efficacy .. neighbors: ['efficacy', 'sexapril', 'helplessnessdepression', 'dischargesomething', 'awaking', 'spiderlike', 'anomalies', 'axe', 'spasmsit', 'arthritisquot', 'afteran', 'dryheaving', 'effects', 'retardant', 'antiinflammatoryyou', 'monthsonly', 'publicationi', 'world', 'thirsty', 'trinza']\n",
      "vectors:  (87165, 300)\n",
      "query:  (300,)\n",
      "word: performance .. neighbors: ['performance', 'rapidly', 'concentration', 'backing', 'moved', 'flashes', 'disorientation', 'muscles', 'horribly', 'fade', 'voices', 'wishes', 'shaky', 'sadness', 'swing', 'include', 'swallowing', 'hot', 'flow', 'sweats']\n",
      "vectors:  (87165, 300)\n",
      "query:  (300,)\n",
      "word: cancer .. neighbors: ['cancer', 'behind', 'disk', 'degenerative', 'fentanyl', 'colon', 'disc', 'bloody', 'spasms', 'tumor', 'teacher', 'student', 'disease', 'cord', 'colitis', 'intermittent', 'treating', 'chronic', 'fibroid', 'stenosis']\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Epoch: 1 .. batch: 2/1452 .. LR: 0.05 .. KL_theta: 7877.56 .. Rec_loss: 874.15 .. NELBO: 8751.71\n",
      "Epoch: 1 .. batch: 4/1452 .. LR: 0.05 .. KL_theta: 4727.36 .. Rec_loss: 837.73 .. NELBO: 5565.09\n",
      "Epoch: 1 .. batch: 6/1452 .. LR: 0.05 .. KL_theta: 3379.57 .. Rec_loss: 815.37 .. NELBO: 4194.94\n",
      "Epoch: 1 .. batch: 8/1452 .. LR: 0.05 .. KL_theta: 2628.95 .. Rec_loss: 795.09 .. NELBO: 3424.04\n",
      "Epoch: 1 .. batch: 10/1452 .. LR: 0.05 .. KL_theta: 2151.27 .. Rec_loss: 778.14 .. NELBO: 2929.41\n",
      "Epoch: 1 .. batch: 12/1452 .. LR: 0.05 .. KL_theta: 1820.44 .. Rec_loss: 767.93 .. NELBO: 2588.37\n",
      "Epoch: 1 .. batch: 14/1452 .. LR: 0.05 .. KL_theta: 1577.79 .. Rec_loss: 749.94 .. NELBO: 2327.73\n",
      "Epoch: 1 .. batch: 16/1452 .. LR: 0.05 .. KL_theta: 1392.32 .. Rec_loss: 741.98 .. NELBO: 2134.3\n",
      "Epoch: 1 .. batch: 18/1452 .. LR: 0.05 .. KL_theta: 1245.85 .. Rec_loss: 732.91 .. NELBO: 1978.76\n",
      "Epoch: 1 .. batch: 20/1452 .. LR: 0.05 .. KL_theta: 1127.26 .. Rec_loss: 723.37 .. NELBO: 1850.63\n",
      "Epoch: 1 .. batch: 22/1452 .. LR: 0.05 .. KL_theta: 1029.29 .. Rec_loss: 717.32 .. NELBO: 1746.61\n",
      "Epoch: 1 .. batch: 24/1452 .. LR: 0.05 .. KL_theta: 947.05 .. Rec_loss: 710.29 .. NELBO: 1657.34\n",
      "Epoch: 1 .. batch: 26/1452 .. LR: 0.05 .. KL_theta: 877.04 .. Rec_loss: 702.85 .. NELBO: 1579.89\n",
      "Epoch: 1 .. batch: 28/1452 .. LR: 0.05 .. KL_theta: 816.75 .. Rec_loss: 697.1 .. NELBO: 1513.85\n",
      "Epoch: 1 .. batch: 30/1452 .. LR: 0.05 .. KL_theta: 764.23 .. Rec_loss: 690.41 .. NELBO: 1454.64\n",
      "Epoch: 1 .. batch: 32/1452 .. LR: 0.05 .. KL_theta: 718.08 .. Rec_loss: 681.76 .. NELBO: 1399.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 .. batch: 34/1452 .. LR: 0.05 .. KL_theta: 677.27 .. Rec_loss: 677.13 .. NELBO: 1354.4\n",
      "Epoch: 1 .. batch: 36/1452 .. LR: 0.05 .. KL_theta: 640.8 .. Rec_loss: 673.0 .. NELBO: 1313.8\n",
      "Epoch: 1 .. batch: 38/1452 .. LR: 0.05 .. KL_theta: 608.16 .. Rec_loss: 666.82 .. NELBO: 1274.98\n",
      "Epoch: 1 .. batch: 40/1452 .. LR: 0.05 .. KL_theta: 578.64 .. Rec_loss: 662.3 .. NELBO: 1240.94\n",
      "Epoch: 1 .. batch: 42/1452 .. LR: 0.05 .. KL_theta: 551.99 .. Rec_loss: 660.37 .. NELBO: 1212.36\n",
      "Epoch: 1 .. batch: 44/1452 .. LR: 0.05 .. KL_theta: 527.61 .. Rec_loss: 655.88 .. NELBO: 1183.49\n",
      "Epoch: 1 .. batch: 46/1452 .. LR: 0.05 .. KL_theta: 505.35 .. Rec_loss: 651.2 .. NELBO: 1156.55\n",
      "Epoch: 1 .. batch: 48/1452 .. LR: 0.05 .. KL_theta: 484.89 .. Rec_loss: 646.62 .. NELBO: 1131.51\n",
      "Epoch: 1 .. batch: 50/1452 .. LR: 0.05 .. KL_theta: 466.05 .. Rec_loss: 643.35 .. NELBO: 1109.4\n",
      "Epoch: 1 .. batch: 52/1452 .. LR: 0.05 .. KL_theta: 448.64 .. Rec_loss: 640.35 .. NELBO: 1088.99\n",
      "Epoch: 1 .. batch: 54/1452 .. LR: 0.05 .. KL_theta: 432.49 .. Rec_loss: 638.79 .. NELBO: 1071.28\n",
      "Epoch: 1 .. batch: 56/1452 .. LR: 0.05 .. KL_theta: 417.47 .. Rec_loss: 635.95 .. NELBO: 1053.42\n",
      "Epoch: 1 .. batch: 58/1452 .. LR: 0.05 .. KL_theta: 403.52 .. Rec_loss: 633.51 .. NELBO: 1037.03\n",
      "Epoch: 1 .. batch: 60/1452 .. LR: 0.05 .. KL_theta: 390.41 .. Rec_loss: 630.51 .. NELBO: 1020.92\n",
      "Epoch: 1 .. batch: 62/1452 .. LR: 0.05 .. KL_theta: 378.2 .. Rec_loss: 627.85 .. NELBO: 1006.05\n",
      "Epoch: 1 .. batch: 64/1452 .. LR: 0.05 .. KL_theta: 366.74 .. Rec_loss: 625.16 .. NELBO: 991.9\n",
      "Epoch: 1 .. batch: 66/1452 .. LR: 0.05 .. KL_theta: 355.91 .. Rec_loss: 622.7 .. NELBO: 978.61\n",
      "Epoch: 1 .. batch: 68/1452 .. LR: 0.05 .. KL_theta: 345.74 .. Rec_loss: 620.6 .. NELBO: 966.34\n",
      "Epoch: 1 .. batch: 70/1452 .. LR: 0.05 .. KL_theta: 336.14 .. Rec_loss: 617.88 .. NELBO: 954.02\n",
      "Epoch: 1 .. batch: 72/1452 .. LR: 0.05 .. KL_theta: 327.04 .. Rec_loss: 615.7 .. NELBO: 942.74\n",
      "Epoch: 1 .. batch: 74/1452 .. LR: 0.05 .. KL_theta: 318.46 .. Rec_loss: 614.82 .. NELBO: 933.28\n",
      "Epoch: 1 .. batch: 76/1452 .. LR: 0.05 .. KL_theta: 310.37 .. Rec_loss: 612.54 .. NELBO: 922.91\n",
      "Epoch: 1 .. batch: 78/1452 .. LR: 0.05 .. KL_theta: 302.61 .. Rec_loss: 610.43 .. NELBO: 913.04\n",
      "Epoch: 1 .. batch: 80/1452 .. LR: 0.05 .. KL_theta: 295.27 .. Rec_loss: 609.32 .. NELBO: 904.59\n",
      "Epoch: 1 .. batch: 82/1452 .. LR: 0.05 .. KL_theta: 288.28 .. Rec_loss: 608.34 .. NELBO: 896.62\n",
      "Epoch: 1 .. batch: 84/1452 .. LR: 0.05 .. KL_theta: 281.61 .. Rec_loss: 606.41 .. NELBO: 888.02\n",
      "Epoch: 1 .. batch: 86/1452 .. LR: 0.05 .. KL_theta: 275.25 .. Rec_loss: 604.96 .. NELBO: 880.21\n",
      "Epoch: 1 .. batch: 88/1452 .. LR: 0.05 .. KL_theta: 269.18 .. Rec_loss: 603.56 .. NELBO: 872.74\n",
      "Epoch: 1 .. batch: 90/1452 .. LR: 0.05 .. KL_theta: 263.38 .. Rec_loss: 602.74 .. NELBO: 866.12\n",
      "Epoch: 1 .. batch: 92/1452 .. LR: 0.05 .. KL_theta: 257.83 .. Rec_loss: 601.88 .. NELBO: 859.71\n",
      "Epoch: 1 .. batch: 94/1452 .. LR: 0.05 .. KL_theta: 252.49 .. Rec_loss: 600.61 .. NELBO: 853.1\n",
      "Epoch: 1 .. batch: 96/1452 .. LR: 0.05 .. KL_theta: 247.42 .. Rec_loss: 599.38 .. NELBO: 846.8\n",
      "Epoch: 1 .. batch: 98/1452 .. LR: 0.05 .. KL_theta: 242.51 .. Rec_loss: 598.56 .. NELBO: 841.07\n",
      "Epoch: 1 .. batch: 100/1452 .. LR: 0.05 .. KL_theta: 237.8 .. Rec_loss: 597.26 .. NELBO: 835.06\n",
      "Epoch: 1 .. batch: 102/1452 .. LR: 0.05 .. KL_theta: 233.3 .. Rec_loss: 596.38 .. NELBO: 829.68\n",
      "Epoch: 1 .. batch: 104/1452 .. LR: 0.05 .. KL_theta: 228.93 .. Rec_loss: 594.93 .. NELBO: 823.86\n",
      "Epoch: 1 .. batch: 106/1452 .. LR: 0.05 .. KL_theta: 224.75 .. Rec_loss: 594.37 .. NELBO: 819.12\n",
      "Epoch: 1 .. batch: 108/1452 .. LR: 0.05 .. KL_theta: 220.72 .. Rec_loss: 594.22 .. NELBO: 814.94\n",
      "Epoch: 1 .. batch: 110/1452 .. LR: 0.05 .. KL_theta: 216.83 .. Rec_loss: 592.95 .. NELBO: 809.78\n",
      "Epoch: 1 .. batch: 112/1452 .. LR: 0.05 .. KL_theta: 213.08 .. Rec_loss: 592.85 .. NELBO: 805.93\n",
      "Epoch: 1 .. batch: 114/1452 .. LR: 0.05 .. KL_theta: 209.48 .. Rec_loss: 592.48 .. NELBO: 801.96\n",
      "Epoch: 1 .. batch: 116/1452 .. LR: 0.05 .. KL_theta: 205.97 .. Rec_loss: 591.52 .. NELBO: 797.49\n",
      "Epoch: 1 .. batch: 118/1452 .. LR: 0.05 .. KL_theta: 202.62 .. Rec_loss: 590.61 .. NELBO: 793.23\n",
      "Epoch: 1 .. batch: 120/1452 .. LR: 0.05 .. KL_theta: 199.35 .. Rec_loss: 589.66 .. NELBO: 789.01\n",
      "Epoch: 1 .. batch: 122/1452 .. LR: 0.05 .. KL_theta: 196.19 .. Rec_loss: 589.17 .. NELBO: 785.36\n",
      "Epoch: 1 .. batch: 124/1452 .. LR: 0.05 .. KL_theta: 193.15 .. Rec_loss: 588.38 .. NELBO: 781.53\n",
      "Epoch: 1 .. batch: 126/1452 .. LR: 0.05 .. KL_theta: 190.17 .. Rec_loss: 587.61 .. NELBO: 777.78\n",
      "Epoch: 1 .. batch: 128/1452 .. LR: 0.05 .. KL_theta: 187.29 .. Rec_loss: 587.15 .. NELBO: 774.44\n",
      "Epoch: 1 .. batch: 130/1452 .. LR: 0.05 .. KL_theta: 184.53 .. Rec_loss: 586.78 .. NELBO: 771.31\n",
      "Epoch: 1 .. batch: 132/1452 .. LR: 0.05 .. KL_theta: 181.83 .. Rec_loss: 585.72 .. NELBO: 767.55\n",
      "Epoch: 1 .. batch: 134/1452 .. LR: 0.05 .. KL_theta: 179.2 .. Rec_loss: 584.93 .. NELBO: 764.13\n",
      "Epoch: 1 .. batch: 136/1452 .. LR: 0.05 .. KL_theta: 176.68 .. Rec_loss: 584.15 .. NELBO: 760.83\n",
      "Epoch: 1 .. batch: 138/1452 .. LR: 0.05 .. KL_theta: 174.23 .. Rec_loss: 584.3 .. NELBO: 758.53\n",
      "Epoch: 1 .. batch: 140/1452 .. LR: 0.05 .. KL_theta: 171.82 .. Rec_loss: 583.79 .. NELBO: 755.61\n",
      "Epoch: 1 .. batch: 142/1452 .. LR: 0.05 .. KL_theta: 169.5 .. Rec_loss: 583.7 .. NELBO: 753.2\n",
      "Epoch: 1 .. batch: 144/1452 .. LR: 0.05 .. KL_theta: 167.24 .. Rec_loss: 583.39 .. NELBO: 750.63\n",
      "Epoch: 1 .. batch: 146/1452 .. LR: 0.05 .. KL_theta: 165.04 .. Rec_loss: 583.25 .. NELBO: 748.29\n",
      "Epoch: 1 .. batch: 148/1452 .. LR: 0.05 .. KL_theta: 162.89 .. Rec_loss: 582.55 .. NELBO: 745.44\n",
      "Epoch: 1 .. batch: 150/1452 .. LR: 0.05 .. KL_theta: 160.81 .. Rec_loss: 582.12 .. NELBO: 742.93\n",
      "Epoch: 1 .. batch: 152/1452 .. LR: 0.05 .. KL_theta: 158.78 .. Rec_loss: 581.52 .. NELBO: 740.3\n",
      "Epoch: 1 .. batch: 154/1452 .. LR: 0.05 .. KL_theta: 156.8 .. Rec_loss: 581.27 .. NELBO: 738.07\n",
      "Epoch: 1 .. batch: 156/1452 .. LR: 0.05 .. KL_theta: 154.88 .. Rec_loss: 580.7 .. NELBO: 735.58\n",
      "Epoch: 1 .. batch: 158/1452 .. LR: 0.05 .. KL_theta: 153.0 .. Rec_loss: 580.23 .. NELBO: 733.23\n",
      "Epoch: 1 .. batch: 160/1452 .. LR: 0.05 .. KL_theta: 151.17 .. Rec_loss: 580.21 .. NELBO: 731.38\n",
      "Epoch: 1 .. batch: 162/1452 .. LR: 0.05 .. KL_theta: 149.38 .. Rec_loss: 580.26 .. NELBO: 729.64\n",
      "Epoch: 1 .. batch: 164/1452 .. LR: 0.05 .. KL_theta: 147.63 .. Rec_loss: 579.7 .. NELBO: 727.33\n",
      "Epoch: 1 .. batch: 166/1452 .. LR: 0.05 .. KL_theta: 145.94 .. Rec_loss: 579.43 .. NELBO: 725.37\n",
      "Epoch: 1 .. batch: 168/1452 .. LR: 0.05 .. KL_theta: 144.27 .. Rec_loss: 579.39 .. NELBO: 723.66\n",
      "Epoch: 1 .. batch: 170/1452 .. LR: 0.05 .. KL_theta: 142.65 .. Rec_loss: 578.84 .. NELBO: 721.49\n",
      "Epoch: 1 .. batch: 172/1452 .. LR: 0.05 .. KL_theta: 141.08 .. Rec_loss: 578.76 .. NELBO: 719.84\n",
      "Epoch: 1 .. batch: 174/1452 .. LR: 0.05 .. KL_theta: 139.52 .. Rec_loss: 578.29 .. NELBO: 717.81\n",
      "Epoch: 1 .. batch: 176/1452 .. LR: 0.05 .. KL_theta: 138.01 .. Rec_loss: 577.86 .. NELBO: 715.87\n",
      "Epoch: 1 .. batch: 178/1452 .. LR: 0.05 .. KL_theta: 136.53 .. Rec_loss: 577.73 .. NELBO: 714.26\n",
      "Epoch: 1 .. batch: 180/1452 .. LR: 0.05 .. KL_theta: 135.08 .. Rec_loss: 577.49 .. NELBO: 712.57\n",
      "Epoch: 1 .. batch: 182/1452 .. LR: 0.05 .. KL_theta: 133.67 .. Rec_loss: 577.39 .. NELBO: 711.06\n",
      "Epoch: 1 .. batch: 184/1452 .. LR: 0.05 .. KL_theta: 132.29 .. Rec_loss: 576.96 .. NELBO: 709.25\n",
      "Epoch: 1 .. batch: 186/1452 .. LR: 0.05 .. KL_theta: 130.93 .. Rec_loss: 576.77 .. NELBO: 707.7\n",
      "Epoch: 1 .. batch: 188/1452 .. LR: 0.05 .. KL_theta: 129.6 .. Rec_loss: 576.33 .. NELBO: 705.93\n",
      "Epoch: 1 .. batch: 190/1452 .. LR: 0.05 .. KL_theta: 128.3 .. Rec_loss: 575.9 .. NELBO: 704.2\n",
      "Epoch: 1 .. batch: 192/1452 .. LR: 0.05 .. KL_theta: 127.03 .. Rec_loss: 575.61 .. NELBO: 702.64\n",
      "Epoch: 1 .. batch: 194/1452 .. LR: 0.05 .. KL_theta: 125.79 .. Rec_loss: 575.62 .. NELBO: 701.41\n",
      "Epoch: 1 .. batch: 196/1452 .. LR: 0.05 .. KL_theta: 124.56 .. Rec_loss: 575.44 .. NELBO: 700.0\n",
      "Epoch: 1 .. batch: 198/1452 .. LR: 0.05 .. KL_theta: 123.37 .. Rec_loss: 575.23 .. NELBO: 698.6\n",
      "Epoch: 1 .. batch: 200/1452 .. LR: 0.05 .. KL_theta: 122.21 .. Rec_loss: 574.87 .. NELBO: 697.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 .. batch: 202/1452 .. LR: 0.05 .. KL_theta: 121.05 .. Rec_loss: 574.59 .. NELBO: 695.64\n",
      "Epoch: 1 .. batch: 204/1452 .. LR: 0.05 .. KL_theta: 119.92 .. Rec_loss: 574.06 .. NELBO: 693.98\n",
      "Epoch: 1 .. batch: 206/1452 .. LR: 0.05 .. KL_theta: 118.82 .. Rec_loss: 573.63 .. NELBO: 692.45\n",
      "Epoch: 1 .. batch: 208/1452 .. LR: 0.05 .. KL_theta: 117.74 .. Rec_loss: 573.04 .. NELBO: 690.78\n",
      "Epoch: 1 .. batch: 210/1452 .. LR: 0.05 .. KL_theta: 116.67 .. Rec_loss: 572.9 .. NELBO: 689.57\n",
      "Epoch: 1 .. batch: 212/1452 .. LR: 0.05 .. KL_theta: 115.62 .. Rec_loss: 572.78 .. NELBO: 688.4\n",
      "Epoch: 1 .. batch: 214/1452 .. LR: 0.05 .. KL_theta: 114.61 .. Rec_loss: 572.3 .. NELBO: 686.91\n",
      "Epoch: 1 .. batch: 216/1452 .. LR: 0.05 .. KL_theta: 113.61 .. Rec_loss: 572.02 .. NELBO: 685.63\n",
      "Epoch: 1 .. batch: 218/1452 .. LR: 0.05 .. KL_theta: 112.62 .. Rec_loss: 572.03 .. NELBO: 684.65\n",
      "Epoch: 1 .. batch: 220/1452 .. LR: 0.05 .. KL_theta: 111.64 .. Rec_loss: 571.33 .. NELBO: 682.97\n",
      "Epoch: 1 .. batch: 222/1452 .. LR: 0.05 .. KL_theta: 110.69 .. Rec_loss: 570.92 .. NELBO: 681.61\n",
      "Epoch: 1 .. batch: 224/1452 .. LR: 0.05 .. KL_theta: 109.76 .. Rec_loss: 570.86 .. NELBO: 680.62\n",
      "Epoch: 1 .. batch: 226/1452 .. LR: 0.05 .. KL_theta: 108.84 .. Rec_loss: 570.8 .. NELBO: 679.64\n",
      "Epoch: 1 .. batch: 228/1452 .. LR: 0.05 .. KL_theta: 107.95 .. Rec_loss: 570.82 .. NELBO: 678.77\n",
      "Epoch: 1 .. batch: 230/1452 .. LR: 0.05 .. KL_theta: 107.07 .. Rec_loss: 570.72 .. NELBO: 677.79\n",
      "Epoch: 1 .. batch: 232/1452 .. LR: 0.05 .. KL_theta: 106.19 .. Rec_loss: 570.6 .. NELBO: 676.79\n",
      "Epoch: 1 .. batch: 234/1452 .. LR: 0.05 .. KL_theta: 105.34 .. Rec_loss: 570.44 .. NELBO: 675.78\n",
      "Epoch: 1 .. batch: 236/1452 .. LR: 0.05 .. KL_theta: 104.49 .. Rec_loss: 570.18 .. NELBO: 674.67\n",
      "Epoch: 1 .. batch: 238/1452 .. LR: 0.05 .. KL_theta: 103.67 .. Rec_loss: 569.92 .. NELBO: 673.59\n",
      "Epoch: 1 .. batch: 240/1452 .. LR: 0.05 .. KL_theta: 102.86 .. Rec_loss: 569.75 .. NELBO: 672.61\n",
      "Epoch: 1 .. batch: 242/1452 .. LR: 0.05 .. KL_theta: 102.06 .. Rec_loss: 569.48 .. NELBO: 671.54\n",
      "Epoch: 1 .. batch: 244/1452 .. LR: 0.05 .. KL_theta: 101.27 .. Rec_loss: 569.0 .. NELBO: 670.27\n",
      "Epoch: 1 .. batch: 246/1452 .. LR: 0.05 .. KL_theta: 100.49 .. Rec_loss: 568.94 .. NELBO: 669.43\n",
      "Epoch: 1 .. batch: 248/1452 .. LR: 0.05 .. KL_theta: 99.73 .. Rec_loss: 569.14 .. NELBO: 668.87\n",
      "Epoch: 1 .. batch: 250/1452 .. LR: 0.05 .. KL_theta: 98.99 .. Rec_loss: 569.08 .. NELBO: 668.07\n",
      "Epoch: 1 .. batch: 252/1452 .. LR: 0.05 .. KL_theta: 98.25 .. Rec_loss: 568.91 .. NELBO: 667.16\n",
      "Epoch: 1 .. batch: 254/1452 .. LR: 0.05 .. KL_theta: 97.53 .. Rec_loss: 568.69 .. NELBO: 666.22\n",
      "Epoch: 1 .. batch: 256/1452 .. LR: 0.05 .. KL_theta: 96.82 .. Rec_loss: 568.48 .. NELBO: 665.3\n",
      "Epoch: 1 .. batch: 258/1452 .. LR: 0.05 .. KL_theta: 96.11 .. Rec_loss: 568.3 .. NELBO: 664.41\n",
      "Epoch: 1 .. batch: 260/1452 .. LR: 0.05 .. KL_theta: 95.42 .. Rec_loss: 568.24 .. NELBO: 663.66\n",
      "Epoch: 1 .. batch: 262/1452 .. LR: 0.05 .. KL_theta: 94.74 .. Rec_loss: 568.06 .. NELBO: 662.8\n",
      "Epoch: 1 .. batch: 264/1452 .. LR: 0.05 .. KL_theta: 94.07 .. Rec_loss: 567.96 .. NELBO: 662.03\n",
      "Epoch: 1 .. batch: 266/1452 .. LR: 0.05 .. KL_theta: 93.41 .. Rec_loss: 567.87 .. NELBO: 661.28\n",
      "Epoch: 1 .. batch: 268/1452 .. LR: 0.05 .. KL_theta: 92.75 .. Rec_loss: 567.45 .. NELBO: 660.2\n",
      "Epoch: 1 .. batch: 270/1452 .. LR: 0.05 .. KL_theta: 92.11 .. Rec_loss: 567.21 .. NELBO: 659.32\n",
      "Epoch: 1 .. batch: 272/1452 .. LR: 0.05 .. KL_theta: 91.47 .. Rec_loss: 566.99 .. NELBO: 658.46\n",
      "Epoch: 1 .. batch: 274/1452 .. LR: 0.05 .. KL_theta: 90.85 .. Rec_loss: 566.81 .. NELBO: 657.66\n",
      "Epoch: 1 .. batch: 276/1452 .. LR: 0.05 .. KL_theta: 90.24 .. Rec_loss: 566.68 .. NELBO: 656.92\n",
      "Epoch: 1 .. batch: 278/1452 .. LR: 0.05 .. KL_theta: 89.63 .. Rec_loss: 566.64 .. NELBO: 656.27\n",
      "Epoch: 1 .. batch: 280/1452 .. LR: 0.05 .. KL_theta: 89.04 .. Rec_loss: 566.55 .. NELBO: 655.59\n",
      "Epoch: 1 .. batch: 282/1452 .. LR: 0.05 .. KL_theta: 88.45 .. Rec_loss: 566.41 .. NELBO: 654.86\n",
      "Epoch: 1 .. batch: 284/1452 .. LR: 0.05 .. KL_theta: 87.87 .. Rec_loss: 566.11 .. NELBO: 653.98\n",
      "Epoch: 1 .. batch: 286/1452 .. LR: 0.05 .. KL_theta: 87.3 .. Rec_loss: 566.08 .. NELBO: 653.38\n",
      "Epoch: 1 .. batch: 288/1452 .. LR: 0.05 .. KL_theta: 86.73 .. Rec_loss: 565.65 .. NELBO: 652.38\n",
      "Epoch: 1 .. batch: 290/1452 .. LR: 0.05 .. KL_theta: 86.17 .. Rec_loss: 565.57 .. NELBO: 651.74\n",
      "Epoch: 1 .. batch: 292/1452 .. LR: 0.05 .. KL_theta: 85.63 .. Rec_loss: 565.57 .. NELBO: 651.2\n",
      "Epoch: 1 .. batch: 294/1452 .. LR: 0.05 .. KL_theta: 85.09 .. Rec_loss: 565.45 .. NELBO: 650.54\n",
      "Epoch: 1 .. batch: 296/1452 .. LR: 0.05 .. KL_theta: 84.56 .. Rec_loss: 565.48 .. NELBO: 650.04\n",
      "Epoch: 1 .. batch: 298/1452 .. LR: 0.05 .. KL_theta: 84.03 .. Rec_loss: 565.47 .. NELBO: 649.5\n",
      "Epoch: 1 .. batch: 300/1452 .. LR: 0.05 .. KL_theta: 83.51 .. Rec_loss: 565.4 .. NELBO: 648.91\n",
      "Epoch: 1 .. batch: 302/1452 .. LR: 0.05 .. KL_theta: 82.99 .. Rec_loss: 565.4 .. NELBO: 648.39\n",
      "Epoch: 1 .. batch: 304/1452 .. LR: 0.05 .. KL_theta: 82.49 .. Rec_loss: 565.08 .. NELBO: 647.57\n",
      "Epoch: 1 .. batch: 306/1452 .. LR: 0.05 .. KL_theta: 81.99 .. Rec_loss: 565.08 .. NELBO: 647.07\n",
      "Epoch: 1 .. batch: 308/1452 .. LR: 0.05 .. KL_theta: 81.5 .. Rec_loss: 565.25 .. NELBO: 646.75\n",
      "Epoch: 1 .. batch: 310/1452 .. LR: 0.05 .. KL_theta: 81.01 .. Rec_loss: 565.19 .. NELBO: 646.2\n",
      "Epoch: 1 .. batch: 312/1452 .. LR: 0.05 .. KL_theta: 80.53 .. Rec_loss: 564.93 .. NELBO: 645.46\n",
      "Epoch: 1 .. batch: 314/1452 .. LR: 0.05 .. KL_theta: 80.05 .. Rec_loss: 564.84 .. NELBO: 644.89\n",
      "Epoch: 1 .. batch: 316/1452 .. LR: 0.05 .. KL_theta: 79.59 .. Rec_loss: 564.78 .. NELBO: 644.37\n",
      "Epoch: 1 .. batch: 318/1452 .. LR: 0.05 .. KL_theta: 79.13 .. Rec_loss: 564.65 .. NELBO: 643.78\n",
      "Epoch: 1 .. batch: 320/1452 .. LR: 0.05 .. KL_theta: 78.67 .. Rec_loss: 564.57 .. NELBO: 643.24\n",
      "Epoch: 1 .. batch: 322/1452 .. LR: 0.05 .. KL_theta: 78.22 .. Rec_loss: 564.71 .. NELBO: 642.93\n",
      "Epoch: 1 .. batch: 324/1452 .. LR: 0.05 .. KL_theta: 77.78 .. Rec_loss: 564.55 .. NELBO: 642.33\n",
      "Epoch: 1 .. batch: 326/1452 .. LR: 0.05 .. KL_theta: 77.34 .. Rec_loss: 564.41 .. NELBO: 641.75\n",
      "Epoch: 1 .. batch: 328/1452 .. LR: 0.05 .. KL_theta: 76.91 .. Rec_loss: 564.55 .. NELBO: 641.46\n",
      "Epoch: 1 .. batch: 330/1452 .. LR: 0.05 .. KL_theta: 76.48 .. Rec_loss: 564.77 .. NELBO: 641.25\n",
      "Epoch: 1 .. batch: 332/1452 .. LR: 0.05 .. KL_theta: 76.06 .. Rec_loss: 564.71 .. NELBO: 640.77\n",
      "Epoch: 1 .. batch: 334/1452 .. LR: 0.05 .. KL_theta: 75.63 .. Rec_loss: 564.65 .. NELBO: 640.28\n",
      "Epoch: 1 .. batch: 336/1452 .. LR: 0.05 .. KL_theta: 75.22 .. Rec_loss: 564.65 .. NELBO: 639.87\n",
      "Epoch: 1 .. batch: 338/1452 .. LR: 0.05 .. KL_theta: 74.82 .. Rec_loss: 564.67 .. NELBO: 639.49\n",
      "Epoch: 1 .. batch: 340/1452 .. LR: 0.05 .. KL_theta: 74.41 .. Rec_loss: 564.48 .. NELBO: 638.89\n",
      "Epoch: 1 .. batch: 342/1452 .. LR: 0.05 .. KL_theta: 74.01 .. Rec_loss: 564.46 .. NELBO: 638.47\n",
      "Epoch: 1 .. batch: 344/1452 .. LR: 0.05 .. KL_theta: 73.62 .. Rec_loss: 564.45 .. NELBO: 638.07\n",
      "Epoch: 1 .. batch: 346/1452 .. LR: 0.05 .. KL_theta: 73.23 .. Rec_loss: 564.4 .. NELBO: 637.63\n",
      "Epoch: 1 .. batch: 348/1452 .. LR: 0.05 .. KL_theta: 72.84 .. Rec_loss: 564.28 .. NELBO: 637.12\n",
      "Epoch: 1 .. batch: 350/1452 .. LR: 0.05 .. KL_theta: 72.46 .. Rec_loss: 564.27 .. NELBO: 636.73\n",
      "Epoch: 1 .. batch: 352/1452 .. LR: 0.05 .. KL_theta: 72.08 .. Rec_loss: 563.93 .. NELBO: 636.01\n",
      "Epoch: 1 .. batch: 354/1452 .. LR: 0.05 .. KL_theta: 71.71 .. Rec_loss: 563.7 .. NELBO: 635.41\n",
      "Epoch: 1 .. batch: 356/1452 .. LR: 0.05 .. KL_theta: 71.34 .. Rec_loss: 563.58 .. NELBO: 634.92\n",
      "Epoch: 1 .. batch: 358/1452 .. LR: 0.05 .. KL_theta: 70.98 .. Rec_loss: 563.55 .. NELBO: 634.53\n",
      "Epoch: 1 .. batch: 360/1452 .. LR: 0.05 .. KL_theta: 70.62 .. Rec_loss: 563.44 .. NELBO: 634.06\n",
      "Epoch: 1 .. batch: 362/1452 .. LR: 0.05 .. KL_theta: 70.27 .. Rec_loss: 563.49 .. NELBO: 633.76\n",
      "Epoch: 1 .. batch: 364/1452 .. LR: 0.05 .. KL_theta: 69.92 .. Rec_loss: 563.38 .. NELBO: 633.3\n",
      "Epoch: 1 .. batch: 366/1452 .. LR: 0.05 .. KL_theta: 69.57 .. Rec_loss: 563.19 .. NELBO: 632.76\n",
      "Epoch: 1 .. batch: 368/1452 .. LR: 0.05 .. KL_theta: 69.22 .. Rec_loss: 563.01 .. NELBO: 632.23\n",
      "Epoch: 1 .. batch: 370/1452 .. LR: 0.05 .. KL_theta: 68.88 .. Rec_loss: 562.88 .. NELBO: 631.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 .. batch: 372/1452 .. LR: 0.05 .. KL_theta: 68.54 .. Rec_loss: 562.74 .. NELBO: 631.28\n",
      "Epoch: 1 .. batch: 374/1452 .. LR: 0.05 .. KL_theta: 68.21 .. Rec_loss: 562.72 .. NELBO: 630.93\n",
      "Epoch: 1 .. batch: 376/1452 .. LR: 0.05 .. KL_theta: 67.88 .. Rec_loss: 562.39 .. NELBO: 630.27\n",
      "Epoch: 1 .. batch: 378/1452 .. LR: 0.05 .. KL_theta: 67.55 .. Rec_loss: 562.57 .. NELBO: 630.12\n",
      "Epoch: 1 .. batch: 380/1452 .. LR: 0.05 .. KL_theta: 67.23 .. Rec_loss: 562.52 .. NELBO: 629.75\n",
      "Epoch: 1 .. batch: 382/1452 .. LR: 0.05 .. KL_theta: 66.91 .. Rec_loss: 562.26 .. NELBO: 629.17\n",
      "Epoch: 1 .. batch: 384/1452 .. LR: 0.05 .. KL_theta: 66.59 .. Rec_loss: 562.22 .. NELBO: 628.81\n",
      "Epoch: 1 .. batch: 386/1452 .. LR: 0.05 .. KL_theta: 66.28 .. Rec_loss: 562.25 .. NELBO: 628.53\n",
      "Epoch: 1 .. batch: 388/1452 .. LR: 0.05 .. KL_theta: 65.97 .. Rec_loss: 562.14 .. NELBO: 628.11\n",
      "Epoch: 1 .. batch: 390/1452 .. LR: 0.05 .. KL_theta: 65.66 .. Rec_loss: 562.02 .. NELBO: 627.68\n",
      "Epoch: 1 .. batch: 392/1452 .. LR: 0.05 .. KL_theta: 65.35 .. Rec_loss: 561.88 .. NELBO: 627.23\n",
      "Epoch: 1 .. batch: 394/1452 .. LR: 0.05 .. KL_theta: 65.05 .. Rec_loss: 561.82 .. NELBO: 626.87\n",
      "Epoch: 1 .. batch: 396/1452 .. LR: 0.05 .. KL_theta: 64.76 .. Rec_loss: 561.75 .. NELBO: 626.51\n",
      "Epoch: 1 .. batch: 398/1452 .. LR: 0.05 .. KL_theta: 64.46 .. Rec_loss: 561.36 .. NELBO: 625.82\n",
      "Epoch: 1 .. batch: 400/1452 .. LR: 0.05 .. KL_theta: 64.17 .. Rec_loss: 561.28 .. NELBO: 625.45\n",
      "Epoch: 1 .. batch: 402/1452 .. LR: 0.05 .. KL_theta: 63.88 .. Rec_loss: 561.06 .. NELBO: 624.94\n",
      "Epoch: 1 .. batch: 404/1452 .. LR: 0.05 .. KL_theta: 63.59 .. Rec_loss: 560.99 .. NELBO: 624.58\n",
      "Epoch: 1 .. batch: 406/1452 .. LR: 0.05 .. KL_theta: 63.31 .. Rec_loss: 560.92 .. NELBO: 624.23\n",
      "Epoch: 1 .. batch: 408/1452 .. LR: 0.05 .. KL_theta: 63.03 .. Rec_loss: 560.83 .. NELBO: 623.86\n",
      "Epoch: 1 .. batch: 410/1452 .. LR: 0.05 .. KL_theta: 62.75 .. Rec_loss: 560.74 .. NELBO: 623.49\n",
      "Epoch: 1 .. batch: 412/1452 .. LR: 0.05 .. KL_theta: 62.48 .. Rec_loss: 560.83 .. NELBO: 623.31\n",
      "Epoch: 1 .. batch: 414/1452 .. LR: 0.05 .. KL_theta: 62.21 .. Rec_loss: 560.86 .. NELBO: 623.07\n",
      "Epoch: 1 .. batch: 416/1452 .. LR: 0.05 .. KL_theta: 61.94 .. Rec_loss: 560.78 .. NELBO: 622.72\n",
      "Epoch: 1 .. batch: 418/1452 .. LR: 0.05 .. KL_theta: 61.67 .. Rec_loss: 560.75 .. NELBO: 622.42\n",
      "Epoch: 1 .. batch: 420/1452 .. LR: 0.05 .. KL_theta: 61.41 .. Rec_loss: 560.52 .. NELBO: 621.93\n",
      "Epoch: 1 .. batch: 422/1452 .. LR: 0.05 .. KL_theta: 61.15 .. Rec_loss: 560.5 .. NELBO: 621.65\n",
      "Epoch: 1 .. batch: 424/1452 .. LR: 0.05 .. KL_theta: 60.89 .. Rec_loss: 560.46 .. NELBO: 621.35\n",
      "Epoch: 1 .. batch: 426/1452 .. LR: 0.05 .. KL_theta: 60.63 .. Rec_loss: 560.32 .. NELBO: 620.95\n",
      "Epoch: 1 .. batch: 428/1452 .. LR: 0.05 .. KL_theta: 60.38 .. Rec_loss: 560.24 .. NELBO: 620.62\n",
      "Epoch: 1 .. batch: 430/1452 .. LR: 0.05 .. KL_theta: 60.13 .. Rec_loss: 560.33 .. NELBO: 620.46\n",
      "Epoch: 1 .. batch: 432/1452 .. LR: 0.05 .. KL_theta: 59.88 .. Rec_loss: 560.25 .. NELBO: 620.13\n",
      "Epoch: 1 .. batch: 434/1452 .. LR: 0.05 .. KL_theta: 59.63 .. Rec_loss: 560.18 .. NELBO: 619.81\n",
      "Epoch: 1 .. batch: 436/1452 .. LR: 0.05 .. KL_theta: 59.38 .. Rec_loss: 560.13 .. NELBO: 619.51\n",
      "Epoch: 1 .. batch: 438/1452 .. LR: 0.05 .. KL_theta: 59.14 .. Rec_loss: 560.23 .. NELBO: 619.37\n",
      "Epoch: 1 .. batch: 440/1452 .. LR: 0.05 .. KL_theta: 58.9 .. Rec_loss: 559.93 .. NELBO: 618.83\n",
      "Epoch: 1 .. batch: 442/1452 .. LR: 0.05 .. KL_theta: 58.66 .. Rec_loss: 559.93 .. NELBO: 618.59\n",
      "Epoch: 1 .. batch: 444/1452 .. LR: 0.05 .. KL_theta: 58.42 .. Rec_loss: 559.77 .. NELBO: 618.19\n",
      "Epoch: 1 .. batch: 446/1452 .. LR: 0.05 .. KL_theta: 58.19 .. Rec_loss: 559.81 .. NELBO: 618.0\n",
      "Epoch: 1 .. batch: 448/1452 .. LR: 0.05 .. KL_theta: 57.96 .. Rec_loss: 559.83 .. NELBO: 617.79\n",
      "Epoch: 1 .. batch: 450/1452 .. LR: 0.05 .. KL_theta: 57.73 .. Rec_loss: 559.89 .. NELBO: 617.62\n",
      "Epoch: 1 .. batch: 452/1452 .. LR: 0.05 .. KL_theta: 57.5 .. Rec_loss: 559.84 .. NELBO: 617.34\n",
      "Epoch: 1 .. batch: 454/1452 .. LR: 0.05 .. KL_theta: 57.28 .. Rec_loss: 559.73 .. NELBO: 617.01\n",
      "Epoch: 1 .. batch: 456/1452 .. LR: 0.05 .. KL_theta: 57.06 .. Rec_loss: 559.64 .. NELBO: 616.7\n",
      "Epoch: 1 .. batch: 458/1452 .. LR: 0.05 .. KL_theta: 56.83 .. Rec_loss: 559.55 .. NELBO: 616.38\n",
      "Epoch: 1 .. batch: 460/1452 .. LR: 0.05 .. KL_theta: 56.61 .. Rec_loss: 559.6 .. NELBO: 616.21\n",
      "Epoch: 1 .. batch: 462/1452 .. LR: 0.05 .. KL_theta: 56.4 .. Rec_loss: 559.63 .. NELBO: 616.03\n",
      "Epoch: 1 .. batch: 464/1452 .. LR: 0.05 .. KL_theta: 56.18 .. Rec_loss: 559.64 .. NELBO: 615.82\n",
      "Epoch: 1 .. batch: 466/1452 .. LR: 0.05 .. KL_theta: 55.97 .. Rec_loss: 559.58 .. NELBO: 615.55\n",
      "Epoch: 1 .. batch: 468/1452 .. LR: 0.05 .. KL_theta: 55.75 .. Rec_loss: 559.52 .. NELBO: 615.27\n",
      "Epoch: 1 .. batch: 470/1452 .. LR: 0.05 .. KL_theta: 55.54 .. Rec_loss: 559.39 .. NELBO: 614.93\n",
      "Epoch: 1 .. batch: 472/1452 .. LR: 0.05 .. KL_theta: 55.33 .. Rec_loss: 559.37 .. NELBO: 614.7\n",
      "Epoch: 1 .. batch: 474/1452 .. LR: 0.05 .. KL_theta: 55.13 .. Rec_loss: 559.3 .. NELBO: 614.43\n",
      "Epoch: 1 .. batch: 476/1452 .. LR: 0.05 .. KL_theta: 54.92 .. Rec_loss: 559.14 .. NELBO: 614.06\n",
      "Epoch: 1 .. batch: 478/1452 .. LR: 0.05 .. KL_theta: 54.72 .. Rec_loss: 559.16 .. NELBO: 613.88\n",
      "Epoch: 1 .. batch: 480/1452 .. LR: 0.05 .. KL_theta: 54.52 .. Rec_loss: 559.15 .. NELBO: 613.67\n",
      "Epoch: 1 .. batch: 482/1452 .. LR: 0.05 .. KL_theta: 54.32 .. Rec_loss: 559.02 .. NELBO: 613.34\n",
      "Epoch: 1 .. batch: 484/1452 .. LR: 0.05 .. KL_theta: 54.12 .. Rec_loss: 558.99 .. NELBO: 613.11\n",
      "Epoch: 1 .. batch: 486/1452 .. LR: 0.05 .. KL_theta: 53.92 .. Rec_loss: 558.85 .. NELBO: 612.77\n",
      "Epoch: 1 .. batch: 488/1452 .. LR: 0.05 .. KL_theta: 53.72 .. Rec_loss: 558.8 .. NELBO: 612.52\n",
      "Epoch: 1 .. batch: 490/1452 .. LR: 0.05 .. KL_theta: 53.53 .. Rec_loss: 558.73 .. NELBO: 612.26\n",
      "Epoch: 1 .. batch: 492/1452 .. LR: 0.05 .. KL_theta: 53.34 .. Rec_loss: 558.68 .. NELBO: 612.02\n",
      "Epoch: 1 .. batch: 494/1452 .. LR: 0.05 .. KL_theta: 53.15 .. Rec_loss: 558.61 .. NELBO: 611.76\n",
      "Epoch: 1 .. batch: 496/1452 .. LR: 0.05 .. KL_theta: 52.96 .. Rec_loss: 558.56 .. NELBO: 611.52\n",
      "Epoch: 1 .. batch: 498/1452 .. LR: 0.05 .. KL_theta: 52.77 .. Rec_loss: 558.41 .. NELBO: 611.18\n",
      "Epoch: 1 .. batch: 500/1452 .. LR: 0.05 .. KL_theta: 52.58 .. Rec_loss: 558.52 .. NELBO: 611.1\n",
      "Epoch: 1 .. batch: 502/1452 .. LR: 0.05 .. KL_theta: 52.4 .. Rec_loss: 558.39 .. NELBO: 610.79\n",
      "Epoch: 1 .. batch: 504/1452 .. LR: 0.05 .. KL_theta: 52.21 .. Rec_loss: 558.38 .. NELBO: 610.59\n",
      "Epoch: 1 .. batch: 506/1452 .. LR: 0.05 .. KL_theta: 52.03 .. Rec_loss: 558.34 .. NELBO: 610.37\n",
      "Epoch: 1 .. batch: 508/1452 .. LR: 0.05 .. KL_theta: 51.85 .. Rec_loss: 558.18 .. NELBO: 610.03\n",
      "Epoch: 1 .. batch: 510/1452 .. LR: 0.05 .. KL_theta: 51.67 .. Rec_loss: 558.19 .. NELBO: 609.86\n",
      "Epoch: 1 .. batch: 512/1452 .. LR: 0.05 .. KL_theta: 51.49 .. Rec_loss: 558.03 .. NELBO: 609.52\n",
      "Epoch: 1 .. batch: 514/1452 .. LR: 0.05 .. KL_theta: 51.32 .. Rec_loss: 558.0 .. NELBO: 609.32\n",
      "Epoch: 1 .. batch: 516/1452 .. LR: 0.05 .. KL_theta: 51.14 .. Rec_loss: 558.02 .. NELBO: 609.16\n",
      "Epoch: 1 .. batch: 518/1452 .. LR: 0.05 .. KL_theta: 50.96 .. Rec_loss: 557.89 .. NELBO: 608.85\n",
      "Epoch: 1 .. batch: 520/1452 .. LR: 0.05 .. KL_theta: 50.79 .. Rec_loss: 557.77 .. NELBO: 608.56\n",
      "Epoch: 1 .. batch: 522/1452 .. LR: 0.05 .. KL_theta: 50.62 .. Rec_loss: 557.63 .. NELBO: 608.25\n",
      "Epoch: 1 .. batch: 524/1452 .. LR: 0.05 .. KL_theta: 50.45 .. Rec_loss: 557.55 .. NELBO: 608.0\n",
      "Epoch: 1 .. batch: 526/1452 .. LR: 0.05 .. KL_theta: 50.28 .. Rec_loss: 557.62 .. NELBO: 607.9\n",
      "Epoch: 1 .. batch: 528/1452 .. LR: 0.05 .. KL_theta: 50.11 .. Rec_loss: 557.6 .. NELBO: 607.71\n",
      "Epoch: 1 .. batch: 530/1452 .. LR: 0.05 .. KL_theta: 49.95 .. Rec_loss: 557.48 .. NELBO: 607.43\n",
      "Epoch: 1 .. batch: 532/1452 .. LR: 0.05 .. KL_theta: 49.78 .. Rec_loss: 557.58 .. NELBO: 607.36\n",
      "Epoch: 1 .. batch: 534/1452 .. LR: 0.05 .. KL_theta: 49.62 .. Rec_loss: 557.46 .. NELBO: 607.08\n",
      "Epoch: 1 .. batch: 536/1452 .. LR: 0.05 .. KL_theta: 49.46 .. Rec_loss: 557.45 .. NELBO: 606.91\n",
      "Epoch: 1 .. batch: 538/1452 .. LR: 0.05 .. KL_theta: 49.3 .. Rec_loss: 557.42 .. NELBO: 606.72\n",
      "Epoch: 1 .. batch: 540/1452 .. LR: 0.05 .. KL_theta: 49.14 .. Rec_loss: 557.32 .. NELBO: 606.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 .. batch: 542/1452 .. LR: 0.05 .. KL_theta: 48.98 .. Rec_loss: 557.34 .. NELBO: 606.32\n",
      "Epoch: 1 .. batch: 544/1452 .. LR: 0.05 .. KL_theta: 48.82 .. Rec_loss: 557.42 .. NELBO: 606.24\n",
      "Epoch: 1 .. batch: 546/1452 .. LR: 0.05 .. KL_theta: 48.67 .. Rec_loss: 557.26 .. NELBO: 605.93\n",
      "Epoch: 1 .. batch: 548/1452 .. LR: 0.05 .. KL_theta: 48.51 .. Rec_loss: 557.16 .. NELBO: 605.67\n",
      "Epoch: 1 .. batch: 550/1452 .. LR: 0.05 .. KL_theta: 48.36 .. Rec_loss: 557.04 .. NELBO: 605.4\n",
      "Epoch: 1 .. batch: 552/1452 .. LR: 0.05 .. KL_theta: 48.2 .. Rec_loss: 557.06 .. NELBO: 605.26\n",
      "Epoch: 1 .. batch: 554/1452 .. LR: 0.05 .. KL_theta: 48.05 .. Rec_loss: 556.89 .. NELBO: 604.94\n",
      "Epoch: 1 .. batch: 556/1452 .. LR: 0.05 .. KL_theta: 47.9 .. Rec_loss: 556.75 .. NELBO: 604.65\n",
      "Epoch: 1 .. batch: 558/1452 .. LR: 0.05 .. KL_theta: 47.75 .. Rec_loss: 556.58 .. NELBO: 604.33\n",
      "Epoch: 1 .. batch: 560/1452 .. LR: 0.05 .. KL_theta: 47.6 .. Rec_loss: 556.63 .. NELBO: 604.23\n",
      "Epoch: 1 .. batch: 562/1452 .. LR: 0.05 .. KL_theta: 47.46 .. Rec_loss: 556.69 .. NELBO: 604.15\n",
      "Epoch: 1 .. batch: 564/1452 .. LR: 0.05 .. KL_theta: 47.31 .. Rec_loss: 556.74 .. NELBO: 604.05\n",
      "Epoch: 1 .. batch: 566/1452 .. LR: 0.05 .. KL_theta: 47.16 .. Rec_loss: 556.71 .. NELBO: 603.87\n",
      "Epoch: 1 .. batch: 568/1452 .. LR: 0.05 .. KL_theta: 47.02 .. Rec_loss: 556.67 .. NELBO: 603.69\n",
      "Epoch: 1 .. batch: 570/1452 .. LR: 0.05 .. KL_theta: 46.88 .. Rec_loss: 556.57 .. NELBO: 603.45\n",
      "Epoch: 1 .. batch: 572/1452 .. LR: 0.05 .. KL_theta: 46.73 .. Rec_loss: 556.53 .. NELBO: 603.26\n",
      "Epoch: 1 .. batch: 574/1452 .. LR: 0.05 .. KL_theta: 46.59 .. Rec_loss: 556.5 .. NELBO: 603.09\n",
      "Epoch: 1 .. batch: 576/1452 .. LR: 0.05 .. KL_theta: 46.45 .. Rec_loss: 556.52 .. NELBO: 602.97\n",
      "Epoch: 1 .. batch: 578/1452 .. LR: 0.05 .. KL_theta: 46.31 .. Rec_loss: 556.49 .. NELBO: 602.8\n",
      "Epoch: 1 .. batch: 580/1452 .. LR: 0.05 .. KL_theta: 46.17 .. Rec_loss: 556.57 .. NELBO: 602.74\n",
      "Epoch: 1 .. batch: 582/1452 .. LR: 0.05 .. KL_theta: 46.04 .. Rec_loss: 556.63 .. NELBO: 602.67\n",
      "Epoch: 1 .. batch: 584/1452 .. LR: 0.05 .. KL_theta: 45.9 .. Rec_loss: 556.65 .. NELBO: 602.55\n",
      "Epoch: 1 .. batch: 586/1452 .. LR: 0.05 .. KL_theta: 45.77 .. Rec_loss: 556.52 .. NELBO: 602.29\n",
      "Epoch: 1 .. batch: 588/1452 .. LR: 0.05 .. KL_theta: 45.63 .. Rec_loss: 556.37 .. NELBO: 602.0\n",
      "Epoch: 1 .. batch: 590/1452 .. LR: 0.05 .. KL_theta: 45.5 .. Rec_loss: 556.35 .. NELBO: 601.85\n",
      "Epoch: 1 .. batch: 592/1452 .. LR: 0.05 .. KL_theta: 45.36 .. Rec_loss: 556.41 .. NELBO: 601.77\n",
      "Epoch: 1 .. batch: 594/1452 .. LR: 0.05 .. KL_theta: 45.23 .. Rec_loss: 556.21 .. NELBO: 601.44\n",
      "Epoch: 1 .. batch: 596/1452 .. LR: 0.05 .. KL_theta: 45.1 .. Rec_loss: 556.13 .. NELBO: 601.23\n",
      "Epoch: 1 .. batch: 598/1452 .. LR: 0.05 .. KL_theta: 44.97 .. Rec_loss: 556.17 .. NELBO: 601.14\n",
      "Epoch: 1 .. batch: 600/1452 .. LR: 0.05 .. KL_theta: 44.84 .. Rec_loss: 556.14 .. NELBO: 600.98\n",
      "Epoch: 1 .. batch: 602/1452 .. LR: 0.05 .. KL_theta: 44.71 .. Rec_loss: 556.12 .. NELBO: 600.83\n",
      "Epoch: 1 .. batch: 604/1452 .. LR: 0.05 .. KL_theta: 44.59 .. Rec_loss: 556.21 .. NELBO: 600.8\n",
      "Epoch: 1 .. batch: 606/1452 .. LR: 0.05 .. KL_theta: 44.46 .. Rec_loss: 556.2 .. NELBO: 600.66\n",
      "Epoch: 1 .. batch: 608/1452 .. LR: 0.05 .. KL_theta: 44.34 .. Rec_loss: 556.28 .. NELBO: 600.62\n",
      "Epoch: 1 .. batch: 610/1452 .. LR: 0.05 .. KL_theta: 44.21 .. Rec_loss: 556.21 .. NELBO: 600.42\n",
      "Epoch: 1 .. batch: 612/1452 .. LR: 0.05 .. KL_theta: 44.08 .. Rec_loss: 556.07 .. NELBO: 600.15\n",
      "Epoch: 1 .. batch: 614/1452 .. LR: 0.05 .. KL_theta: 43.96 .. Rec_loss: 556.07 .. NELBO: 600.03\n",
      "Epoch: 1 .. batch: 616/1452 .. LR: 0.05 .. KL_theta: 43.84 .. Rec_loss: 556.11 .. NELBO: 599.95\n",
      "Epoch: 1 .. batch: 618/1452 .. LR: 0.05 .. KL_theta: 43.71 .. Rec_loss: 556.02 .. NELBO: 599.73\n",
      "Epoch: 1 .. batch: 620/1452 .. LR: 0.05 .. KL_theta: 43.59 .. Rec_loss: 555.91 .. NELBO: 599.5\n",
      "Epoch: 1 .. batch: 622/1452 .. LR: 0.05 .. KL_theta: 43.47 .. Rec_loss: 555.84 .. NELBO: 599.31\n",
      "Epoch: 1 .. batch: 624/1452 .. LR: 0.05 .. KL_theta: 43.35 .. Rec_loss: 555.74 .. NELBO: 599.09\n",
      "Epoch: 1 .. batch: 626/1452 .. LR: 0.05 .. KL_theta: 43.23 .. Rec_loss: 555.76 .. NELBO: 598.99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if mode == 'train':\n",
    "    ## train model on data \n",
    "    best_epoch = 0\n",
    "    best_val_ppl = 1e9\n",
    "    all_val_ppls = []\n",
    "    print('\\n')\n",
    "    print('Visualizing model quality before training...')\n",
    "    visualize(model)\n",
    "    print('\\n')\n",
    "    for epoch in range(1, epochs):\n",
    "        train(epoch)\n",
    "        val_ppl = evaluate(model, 'val')\n",
    "        if val_ppl < best_val_ppl:\n",
    "            with open(ckpt, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_epoch = epoch\n",
    "            best_val_ppl = val_ppl\n",
    "        else:\n",
    "            ## check whether to anneal lr\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            if anneal_lr and (len(all_val_ppls) > nonmono and val_ppl > min(all_val_ppls[:-nonmono]) and lr > 1e-5):\n",
    "                optimizer.param_groups[0]['lr'] /= lr_factor\n",
    "        if epoch % visualize_every == 0:\n",
    "            visualize(model)\n",
    "        all_val_ppls.append(val_ppl)\n",
    "    with open(ckpt, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    model = model.to(device)\n",
    "    val_ppl = evaluate(model, 'val')\n",
    "else:   \n",
    "    with open(ckpt, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ## get document completion perplexities\n",
    "        test_ppl = evaluate(model, 'test', tc=tc, td=td)\n",
    "\n",
    "        ## get most used topics\n",
    "        indices = torch.tensor(range(num_docs_train))\n",
    "        indices = torch.split(indices, batch_size)\n",
    "        thetaAvg = torch.zeros(1, num_topics).to(device)\n",
    "        thetaWeightedAvg = torch.zeros(1, num_topics).to(device)\n",
    "        cnt = 0\n",
    "        for idx, ind in enumerate(indices):\n",
    "            try:\n",
    "                data_batch = data.get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "                sums = data_batch.sum(1).unsqueeze(1)\n",
    "                cnt += sums.sum(0).squeeze().cpu().numpy()\n",
    "                if bow_norm:\n",
    "                    normalized_data_batch = data_batch / sums\n",
    "                else:\n",
    "                    normalized_data_batch = data_batch\n",
    "                theta, _ = model.get_theta(normalized_data_batch)\n",
    "                thetaAvg += theta.sum(0).unsqueeze(0) / num_docs_train\n",
    "                weighed_theta = sums * theta\n",
    "                thetaWeightedAvg += weighed_theta.sum(0).unsqueeze(0)\n",
    "                if idx % 100 == 0 and idx > 0:\n",
    "                    print('batch: {}/{}'.format(idx, len(indices)))\n",
    "            except IndexError:\n",
    "                continue\n",
    "        thetaWeightedAvg = thetaWeightedAvg.squeeze().cpu().numpy() / cnt\n",
    "        print('\\nThe 10 most used topics are {}'.format(thetaWeightedAvg.argsort()[::-1][:10]))\n",
    "\n",
    "        ## show topics\n",
    "        beta = model.get_beta()\n",
    "        topic_indices = list(np.random.choice(num_topics, 10)) # 10 random topics\n",
    "        print('\\n')\n",
    "        for k in range(num_topics):#topic_indices:\n",
    "            gamma = beta[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if train_embeddings:\n",
    "            ## show etm embeddings \n",
    "            try:\n",
    "                rho_etm = model.rho.weight.cpu()\n",
    "            except:\n",
    "                rho_etm = model.rho.cpu()\n",
    "            queries = ['andrew', 'woman', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                            'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "            print('\\n')\n",
    "            print('ETM embeddings...')\n",
    "            for word in queries:\n",
    "                print('word: {} .. etm neighbors: {}'.format(word, nearest_neighbors(word, rho_etm, vocab)))\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python main.py --mode train --dataset 20ng --data_path data/20ng --num_topics 50 --train_embeddings 1 --epochs 1000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AZ Social Media Analytics)",
   "language": "python",
   "name": "atsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
