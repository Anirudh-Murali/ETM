{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Required Embeddings\n",
    "\n",
    "Note: This section can be skipped if embeddings are already prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "# sys.setdefaultencoding() does not exist, here!\n",
    "# reload(sys)  # Reload does the trick!\n",
    "# sys.setdefaultencoding('UTF8')\n",
    "\n",
    "# sys.setdefaultencoding('UTF8')\n",
    "\n",
    "data_file= \"data/drug_review/drugsComTrain_raw.tsv\"        # default='', help='a .txt file containing the corpus'\n",
    "\n",
    "dim_rho= 200                                               #default=300, help='dimensionality of the word embeddings'\n",
    "min_count= 4                                               #default=2, help='minimum term frequency (to define the vocabulary)'\n",
    "sg= 1                                                      # default=1, help='whether to use skip-gram'\n",
    "# workers= 6                                                #default=25, help='number of CPU cores'\n",
    "negative_samples= 10                                       # default=10, help='number of negative samples'\n",
    "window_size= 8                                             # default=4, help='window size to determine context'\n",
    "iters= 50                                                  #default=50, help='number of iterationst'\n",
    "\n",
    "emb_file= \"embeddings/embeddings\"+\"_dim_\"+str(dim_rho)+\"_min_count_\"+str(min_count)+\"_sg_\"+str(sg)+\"_negative_samples_\"+str(negative_samples)+\"_window_size_\"+str(window_size)+\"_iters_\"+str(iters)+\".txt\"                      #default='embeddings.txt', help='file to save the word embeddings'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class for a memory-friendly iterator over the dataset\n",
    "class MySentences(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.file_type = 'text'\n",
    " \n",
    "    def __iter__(self):\n",
    "        if self.file_type == 'text':\n",
    "            for line in open(self.filename,encoding=\"utf8\"):\n",
    "                yield line.split()\n",
    "        elif self.file_type == 'csv':\n",
    "            for line in self.reviews.values:\n",
    "                yield line.split()\n",
    "                \n",
    "    def __init__(self, filename,col,delimiter = \"\\t\"):\n",
    "        self.filename = filename\n",
    "        data = pd.read_csv(filename,delimiter=delimiter)\n",
    "        self.reviews = data[col][:10000]\n",
    "        self.file_type = 'csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MySentences(data_file,\"review\") # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(sentences, min_count=min_count, sg=sg, size=dim_rho, \n",
    "    iter=iters, negative=negative_samples, window=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Write the embeddings to a file\n",
    "with open(emb_file, 'w',encoding='utf8') as f:\n",
    "    for v in list(model.wv.vocab):\n",
    "        vec = list(model.wv.__getitem__(v))\n",
    "        f.write(v + ' ')\n",
    "        vec_str = ['%.9f' % val for val in vec]\n",
    "        vec_str = \" \".join(vec_str)\n",
    "        f.write(vec_str + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# abc = pickle.load(\"data/20ng/vocab.pkl\")\n",
    "# abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import data\n",
    "import scipy.io\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from etm import ETM\n",
    "from utils import nearest_neighbors, get_topic_coherence, get_topic_diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/drug_review/drugsComTrain_raw.tsv\",delimiter=\"\\t\")[:1000]\n",
    "# df.to_csv(\"data/drug_review/drugs_train_1000.csv\",index=None)\n",
    "# reviews = df.review\n",
    "# with open(\"train_file.txt\", 'w',encoding='utf8') as f:\n",
    "#     for review in reviews.values:\n",
    "#         f.write(review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"20ng\"\n",
    "\n",
    "dataset =   \"train_file.txt\"                      #default='20ng', help='name of corpus'\n",
    "data_path = 'data/drug_review/'#default='data/20ng', help='directory containing data'\n",
    "emb_path = \"embeddings/embeddings\"+\"_dim_\"+str(dim_rho)+\"_min_count_\"+str(min_count)+\"_sg_\"+str(sg)+\"_negative_samples_\"+str(negative_samples)+\"_window_size_\"+str(window_size)+\"_iters_\"+str(iters)+\".txt\"#default='data/20ng_embeddings.txt', help='directory containing word embeddings'\n",
    "save_path = './results'#default='./results', help='path to save results'\n",
    "batch_size = 100 #default=1000, help='input batch size for training'\n",
    "\n",
    "### model-related arguments\n",
    "num_topics = 15   #default=50, help='number of topics'\n",
    "rho_size = 200    #default=300, help='dimension of rho'\n",
    "emb_size = 200    #default=300, help='dimension of embeddings'\n",
    "t_hidden_size = 800 #default=800, help='dimension of hidden space of q(theta)'\n",
    "theta_act = 'relu' #default='relu', help='tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu)'\n",
    "train_embeddings = 0 #default=0, help='whether to fix rho or train it'\n",
    "\n",
    "### optimization-related arguments\n",
    "lr = 0.005 # default=0.005, help='learning rate'\n",
    "lr_factor =4.0  #default=4.0, help='divide learning rate by this...'\n",
    "epochs = 10 # default=20, help='number of epochs to train...150 for 20ng 100 for others'\n",
    "mode = 'train'# default='train', help='train or eval model'\n",
    "optimizer = 'adam'# default='adam', help='choice of optimizer'\n",
    "seed = 2019# default=2019, help='random seed (default: 1)\n",
    "enc_drop = 0.0# default=0.0, help='dropout rate on encoder'\n",
    "clip = 0.0# default=0.0, help='gradient clipping'\n",
    "nonmono = 10# default=10, help='number of bad hits allowed'\n",
    "wdecay = 1.2e-6# default=1.2e-6, help='some l2 regularization'\n",
    "anneal_lr = 0#  default=0, help='whether to anneal the learning rate or not'\n",
    "bow_norm = 1# default=1, help='normalize the bows or not'\n",
    "\n",
    "### evaluation, visualization, and logging-related arguments\n",
    "num_words = 10  # default=10, help='number of words for topic viz' \n",
    "log_interval = 2 # default=2, help='when to log training'\n",
    "visualize_every = 1 # default=10, help='when to visualize results'\n",
    "eval_batch_size = 1000 # default=1000, help='input batch size for evaluation'\n",
    "load_from = 'results/etm_train_file.txt_K_15_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_100_RhoSize_200_trainEmbeddings_0'\n",
    "tc = 0# default=0, help='whether to compute topic coherence or not'\n",
    "td = 0# default=0, help='whether to compute topic diversity or not'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")#(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('\\n')\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, train, valid, test,test_1,test_2 = data.get_data(os.path.join(data_path))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 1. training data\n",
    "train_tokens = train['tokens']\n",
    "train_counts = train['counts']\n",
    "num_docs_train = len(train_tokens)\n",
    "\n",
    "# 2. dev set\n",
    "valid_tokens = valid['tokens']\n",
    "valid_counts = valid['counts']\n",
    "num_docs_valid = len(valid_tokens)\n",
    "\n",
    "# 3. test data\n",
    "test_tokens = test['tokens']\n",
    "test_counts = test['counts']\n",
    "num_docs_test = len(test_tokens)\n",
    "test_1_tokens = test_1['tokens']\n",
    "test_1_counts = test_1['counts']\n",
    "num_docs_test_1 = len(test_1_tokens)\n",
    "test_2_tokens = test_2['tokens']\n",
    "test_2_counts = test_2['counts']\n",
    "num_docs_test_2 = len(test_2_tokens)\n",
    "\n",
    "embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/usr/bin/python\n",
    "## get data\n",
    "# 1. vocabulary\n",
    "\n",
    "if not train_embeddings:\n",
    "    emb_path = emb_path\n",
    "    vect_path = os.path.join(data_path.split('/')[0], 'vocab.pkl')   \n",
    "    vectors = {}\n",
    "    with open(emb_path, 'rb') as f:\n",
    "        for l in f:\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "            if word in vocab:\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                vectors[word] = vect\n",
    "    embeddings = np.zeros((vocab_size, emb_size))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(vocab):\n",
    "        try: \n",
    "            embeddings[i] = vectors[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            embeddings[i] = np.random.normal(scale=0.6, size=(emb_size, ))\n",
    "    embeddings = torch.tensor(embeddings).to(device)\n",
    "    embeddings_dim = embeddings.size()\n",
    "\n",
    "print('=*'*100)\n",
    "# print('Training an Embedded Topic Model on {} with the following settings: {}'.format(dataset.upper()))\n",
    "print('=*'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## define checkpoint\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "if mode == 'eval':\n",
    "    ckpt = load_from\n",
    "else:\n",
    "    ckpt = os.path.join(save_path, \n",
    "        'etm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_trainEmbeddings_{}'.format(\n",
    "        dataset, num_topics, t_hidden_size, optimizer, clip, theta_act, \n",
    "            lr, batch_size, rho_size, train_embeddings))\n",
    "\n",
    "## define model and optimizer\n",
    "model = ETM(num_topics, vocab_size, t_hidden_size, rho_size, emb_size, \n",
    "                theta_act, embeddings, train_embeddings, enc_drop).to(device)\n",
    "\n",
    "print('model: {}'.format(model))\n",
    "\n",
    "if optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'asgd':\n",
    "    optimizer = optim.ASGD(model.parameters(), lr=lr, t0=0, lambd=0., weight_decay=wdecay)\n",
    "else:\n",
    "    print('Defaulting to vanilla SGD')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    acc_loss = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    cnt = 0\n",
    "    indices = torch.randperm(num_docs_train)\n",
    "    indices = torch.split(indices, batch_size)\n",
    "    for idx, ind in enumerate(indices):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            data_batch = data.get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "            sums = data_batch.sum(1).unsqueeze(1)\n",
    "            if bow_norm:\n",
    "                normalized_data_batch = data_batch / sums\n",
    "            else:\n",
    "                normalized_data_batch = data_batch\n",
    "            recon_loss, kld_theta = model(data_batch, normalized_data_batch)\n",
    "            total_loss = recon_loss + kld_theta\n",
    "            total_loss.backward()\n",
    "\n",
    "            if clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_loss += torch.sum(recon_loss).item()\n",
    "            acc_kl_theta_loss += torch.sum(kld_theta).item()\n",
    "            cnt += 1\n",
    "\n",
    "            if idx % log_interval == 0 and idx > 0:\n",
    "                cur_loss = round(acc_loss / cnt, 2) \n",
    "                cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "                cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "\n",
    "                print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                    epoch, idx, len(indices), optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "        except IndexError:\n",
    "            cnt+=1\n",
    "            continue\n",
    "    cur_loss = round(acc_loss / cnt, 2) \n",
    "    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "    cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "    print('*'*100)\n",
    "    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "            epoch, optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(m, show_emb=True):\n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "\n",
    "    m.eval()\n",
    "\n",
    "    queries = ['skin','cycle','effects','price','worst','best','efficacy','performance','cancer','disease']\n",
    "\n",
    "    ## visualize topics using monte carlo\n",
    "    with torch.no_grad():\n",
    "        print('#'*100)\n",
    "        print('Visualize topics...')\n",
    "        topics_words = []\n",
    "        gammas = m.get_beta()\n",
    "        \n",
    "        for k in range(num_topics):\n",
    "            gamma = gammas[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if show_emb:\n",
    "            ## visualize word embeddings by using V to get nearest neighbors\n",
    "            print('#'*100)\n",
    "            print('Visualize word embeddings by using output embedding matrix')\n",
    "            try:\n",
    "                embeddings = m.rho.weight  # Vocab_size x E\n",
    "            except:\n",
    "                embeddings = m.rho         # Vocab_size x E\n",
    "            neighbors = []\n",
    "            for word in queries:\n",
    "                try:\n",
    "                    print('word: {} .. neighbors: {}'.format(\n",
    "                        word, nearest_neighbors(word, embeddings, vocab)))\n",
    "                except ValueError:\n",
    "                    print(\"querry doesn't exist!!\")\n",
    "            print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(m, source, tc=False, td=False):\n",
    "    \"\"\"Compute perplexity on document completion.\n",
    "    \"\"\"\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        if source == 'val':\n",
    "            indices = torch.split(torch.tensor(range(num_docs_valid)), eval_batch_size)\n",
    "            tokens = valid_tokens\n",
    "            counts = valid_counts\n",
    "        else: \n",
    "            indices = torch.split(torch.tensor(range(num_docs_test)), eval_batch_size)\n",
    "            tokens = test_tokens\n",
    "            counts = test_counts\n",
    "\n",
    "        ## get \\beta here\n",
    "        beta = m.get_beta()\n",
    "\n",
    "        ### do dc and tc here\n",
    "        acc_loss = 0\n",
    "        cnt = 0\n",
    "        indices_1 = torch.split(torch.tensor(range(num_docs_test_1)), eval_batch_size)\n",
    "        for idx, ind in enumerate(indices_1):\n",
    "            try:\n",
    "                ## get theta from first half of docs\n",
    "                data_batch_1 = data.get_batch(test_1_tokens, test_1_counts, ind, vocab_size, device)\n",
    "                sums_1 = data_batch_1.sum(1).unsqueeze(1)\n",
    "                if bow_norm:\n",
    "                    normalized_data_batch_1 = data_batch_1 / sums_1\n",
    "                else:\n",
    "                    normalized_data_batch_1 = data_batch_1\n",
    "                theta, _ = m.get_theta(normalized_data_batch_1)\n",
    "\n",
    "                ## get prediction loss using second half\n",
    "                data_batch_2 = data.get_batch(test_2_tokens, test_2_counts, ind, vocab_size, device)\n",
    "                sums_2 = data_batch_2.sum(1).unsqueeze(1)\n",
    "                res = torch.mm(theta, beta)\n",
    "                preds = torch.log(res)\n",
    "                recon_loss = -(preds * data_batch_2).sum(1)\n",
    "\n",
    "                loss = recon_loss / sums_2.squeeze()\n",
    "                loss = loss.mean().item()\n",
    "                acc_loss += loss\n",
    "                cnt += 1\n",
    "            except IndexError:\n",
    "                cnt+=1\n",
    "                continue\n",
    "        cur_loss = acc_loss / cnt\n",
    "        ppl_dc = round(math.exp(cur_loss), 1)\n",
    "        print('*'*100)\n",
    "        print('{} Doc Completion PPL: {}'.format(source.upper(), ppl_dc))\n",
    "        print('*'*100)\n",
    "        if tc or td:\n",
    "            beta = beta.data.cpu().numpy()\n",
    "            if tc:\n",
    "                print('Computing topic coherence...')\n",
    "                get_topic_coherence(beta, train_tokens, vocab)\n",
    "            if td:\n",
    "                print('Computing topic diversity...')\n",
    "                get_topic_diversity(beta, 25)\n",
    "        return ppl_dc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if mode == 'train':\n",
    "    ## train model on data \n",
    "    best_epoch = 0\n",
    "    best_val_ppl = 1e9\n",
    "    all_val_ppls = []\n",
    "    print('\\n')\n",
    "    print('Visualizing model quality before training...')\n",
    "    visualize(model)\n",
    "    print('\\n')\n",
    "    for epoch in range(1, epochs):\n",
    "        train(epoch)\n",
    "        val_ppl = evaluate(model, 'val')\n",
    "        if val_ppl < best_val_ppl:\n",
    "            with open(ckpt, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_epoch = epoch\n",
    "            best_val_ppl = val_ppl\n",
    "        else:\n",
    "            ## check whether to anneal lr\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            if anneal_lr and (len(all_val_ppls) > nonmono and val_ppl > min(all_val_ppls[:-nonmono]) and lr > 1e-5):\n",
    "                optimizer.param_groups[0]['lr'] /= lr_factor\n",
    "        if epoch % visualize_every == 0:\n",
    "            visualize(model)\n",
    "        all_val_ppls.append(val_ppl)\n",
    "    with open(ckpt, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    model = model.to(device)\n",
    "    val_ppl = evaluate(model, 'val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open(ckpt, 'rb') as f:\n",
    "#     model = torch.load(f)\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     ## get document completion perplexities\n",
    "#     test_ppl = evaluate(model, 'test', tc=tc, td=td)\n",
    "\n",
    "#     ## get most used topics\n",
    "#     indices = torch.tensor(range(num_docs_train))\n",
    "#     indices = torch.split(indices, batch_size)\n",
    "#     thetaAvg = torch.zeros(1, num_topics).to(device)\n",
    "#     thetaWeightedAvg = torch.zeros(1, num_topics).to(device)\n",
    "#     cnt = 0\n",
    "#     for idx, ind in enumerate(indices):\n",
    "#         try:\n",
    "#             data_batch = data.get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "#             sums = data_batch.sum(1).unsqueeze(1)\n",
    "#             cnt += sums.sum(0).squeeze().cpu().numpy()\n",
    "#             if bow_norm:\n",
    "#                 normalized_data_batch = data_batch / sums\n",
    "#             else:\n",
    "#                 normalized_data_batch = data_batch\n",
    "#             theta, _ = model.get_theta(normalized_data_batch)\n",
    "#             thetaAvg += theta.sum(0).unsqueeze(0) / num_docs_train\n",
    "#             weighed_theta = sums * theta\n",
    "#             thetaWeightedAvg += weighed_theta.sum(0).unsqueeze(0)\n",
    "#             if idx % 100 == 0 and idx > 0:\n",
    "#                 print('batch: {}/{}'.format(idx, len(indices)))\n",
    "#         except IndexError:\n",
    "#             continue\n",
    "#     thetaWeightedAvg = thetaWeightedAvg.squeeze().cpu().numpy() / cnt\n",
    "#     print('\\nThe 10 most used topics are {}'.format(thetaWeightedAvg.argsort()[::-1][:10]))\n",
    "\n",
    "#     ## show topics\n",
    "#     beta = model.get_beta()\n",
    "#     topic_indices = list(np.random.choice(num_topics, 10)) # 10 random topics\n",
    "#     print('\\n')\n",
    "#     for k in range(num_topics):#topic_indices:\n",
    "#         gamma = beta[k]\n",
    "#         top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "#         topic_words = [vocab[a] for a in top_words]\n",
    "#         print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "#     if train_embeddings:\n",
    "#         ## show etm embeddings \n",
    "#         try:\n",
    "#             rho_etm = model.rho.weight.cpu()\n",
    "#         except:\n",
    "#             rho_etm = model.rho.cpu()\n",
    "#         queries = ['andrew', 'woman', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "#                         'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "#         print('\\n')\n",
    "#         print('ETM embeddings...')\n",
    "#         for word in queries:\n",
    "#             print('word: {} .. etm neighbors: {}'.format(word, nearest_neighbors(word, rho_etm, vocab)))\n",
    "#         print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python main.py --mode train --dataset 20ng --data_path data/20ng --num_topics 50 --train_embeddings 1 --epochs 1000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AZ Social Media Analytics)",
   "language": "python",
   "name": "atsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
