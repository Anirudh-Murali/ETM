{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Required Embeddings\n",
    "\n",
    "Note: This section can be skipped if embeddings are already prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "# sys.setdefaultencoding() does not exist, here!\n",
    "# reload(sys)  # Reload does the trick!\n",
    "# sys.setdefaultencoding('UTF8')\n",
    "\n",
    "# sys.setdefaultencoding('UTF8')\n",
    "\n",
    "data_file= \"data/drug_review/drugsComTrain_raw.tsv\"        # default='', help='a .txt file containing the corpus'\n",
    "emb_file= \"embeddings/embeddings.txt\"                      #default='embeddings.txt', help='file to save the word embeddings'\n",
    "dim_rho= 300                                               #default=300, help='dimensionality of the word embeddings'\n",
    "min_count= 2                                               #default=2, help='minimum term frequency (to define the vocabulary)'\n",
    "sg= 1                                                      # default=1, help='whether to use skip-gram'\n",
    "workers= 8                                                 #default=25, help='number of CPU cores'\n",
    "negative_samples= 10                                       # default=10, help='number of negative samples'\n",
    "window_size= 4                                             # default=4, help='window size to determine context'\n",
    "iters= 50                                                  #default=50, help='number of iterationst'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class for a memory-friendly iterator over the dataset\n",
    "class MySentences(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.file_type = 'text'\n",
    " \n",
    "    def __iter__(self):\n",
    "        if self.file_type == 'text':\n",
    "            for line in open(self.filename,encoding=\"utf8\"):\n",
    "                yield line.split()\n",
    "        elif self.file_type == 'csv':\n",
    "            for line in self.reviews:\n",
    "                yield line.split()\n",
    "                \n",
    "    def __init__(self, filename,col,delimiter = \"\\t\"):\n",
    "        self.filename = filename\n",
    "        data = pd.read_csv(filename,delimiter=delimiter)\n",
    "        self.reviews = data[col][:1000]\n",
    "        self.file_type = 'csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MySentences(data_file,\"review\") # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(sentences, min_count=min_count, sg=sg, size=dim_rho, \n",
    "    iter=iters, workers=workers, negative=negative_samples, window=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Write the embeddings to a file\n",
    "with open(emb_file, 'w') as f:\n",
    "    for v in list(model.wv.vocab):\n",
    "        vec = list(model.wv.__getitem__(v))\n",
    "        f.write(v + ' ')\n",
    "        vec_str = ['%.9f' % val for val in vec]\n",
    "        vec_str = \" \".join(vec_str)\n",
    "        f.write(vec_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# abc = pickle.load(\"data/20ng/vocab.pkl\")\n",
    "# abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import data\n",
    "import scipy.io\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from etm import ETM\n",
    "from utils import nearest_neighbors, get_topic_coherence, get_topic_diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206461</td>\n",
       "      <td>Valsartan</td>\n",
       "      <td>Left Ventricular Dysfunction</td>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>May 20, 2012</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>Opiate Dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  drugName                     condition  \\\n",
       "0      206461                 Valsartan  Left Ventricular Dysfunction   \n",
       "1       95260                Guanfacine                          ADHD   \n",
       "2       92703                    Lybrel                 Birth Control   \n",
       "3      138000                Ortho Evra                 Birth Control   \n",
       "4       35696  Buprenorphine / naloxone             Opiate Dependence   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"It has no side effect, I take it in combinati...     9.0   \n",
       "1  \"My son is halfway through his fourth week of ...     8.0   \n",
       "2  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "3  \"This is my first time using any form of birth...     8.0   \n",
       "4  \"Suboxone has completely turned my life around...     9.0   \n",
       "\n",
       "                date  usefulCount  \n",
       "0       May 20, 2012           27  \n",
       "1     April 27, 2010          192  \n",
       "2  December 14, 2009           17  \n",
       "3   November 3, 2015           10  \n",
       "4  November 27, 2016           37  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/drug_review/drugsComTrain_raw.tsv\",delimiter=\"\\t\")[:1000]\n",
    "df.to_csv(\"data/drug_review/drugs_train_1000.csv\",index=None)\n",
    "reviews = df.review\n",
    "with open(\"train_file.txt\", 'w',encoding='utf8') as f:\n",
    "    for review in reviews.values:\n",
    "        f.write(review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"20ng\"\n",
    "\n",
    "dataset =   \"20ng\"                      #default='20ng', help='name of corpus'\n",
    "data_path = 'data/20ng'#default='data/20ng', help='directory containing data'\n",
    "emb_path = 'embeddings/embeddings.txt'#default='data/20ng_embeddings.txt', help='directory containing word embeddings'\n",
    "save_path = './results'#default='./results', help='path to save results'\n",
    "batch_size = 1000 #default=1000, help='input batch size for training'\n",
    "\n",
    "### model-related arguments\n",
    "num_topics = 50   #default=50, help='number of topics'\n",
    "rho_size = 300    #default=300, help='dimension of rho'\n",
    "emb_size = 300    #default=300, help='dimension of embeddings'\n",
    "t_hidden_size = 800 #default=800, help='dimension of hidden space of q(theta)'\n",
    "theta_act = 'relu' #default='relu', help='tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu)'\n",
    "train_embeddings = 0 #default=0, help='whether to fix rho or train it'\n",
    "\n",
    "### optimization-related arguments\n",
    "lr = 0.005 # default=0.005, help='learning rate'\n",
    "lr_factor =4.0  #default=4.0, help='divide learning rate by this...'\n",
    "epochs = 20 # default=20, help='number of epochs to train...150 for 20ng 100 for others'\n",
    "mode = 'eval'# default='train', help='train or eval model'\n",
    "optimizer = 'adam'# default='adam', help='choice of optimizer'\n",
    "seed = 2019# default=2019, help='random seed (default: 1)\n",
    "enc_drop = 0.0# default=0.0, help='dropout rate on encoder'\n",
    "clip = 0.0# default=0.0, help='gradient clipping'\n",
    "nonmono = 10# default=10, help='number of bad hits allowed'\n",
    "wdecay = 1.2e-6# default=1.2e-6, help='some l2 regularization'\n",
    "anneal_lr = 0#  default=0, help='whether to anneal the learning rate or not'\n",
    "bow_norm = 1# default=1, help='normalize the bows or not'\n",
    "\n",
    "### evaluation, visualization, and logging-related arguments\n",
    "num_words = 10  # default=10, help='number of words for topic viz' \n",
    "log_interval = 2 # default=2, help='when to log training'\n",
    "visualize_every = 10 # default=10, help='when to visualize results'\n",
    "eval_batch_size = 1000 # default=1000, help='input batch size for evaluation'\n",
    "load_from = 'results/etm_20ng_K_50_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_1'# default='', help='the name of the ckpt to eval from'\n",
    "tc = 0# default=0, help='whether to compute topic coherence or not'\n",
    "td = 0# default=0, help='whether to compute topic diversity or not'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e64bbcebd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")#(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('\\n')\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, train, valid, test = data.get_data(os.path.join(data_path))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 1. training data\n",
    "train_tokens = train['tokens']\n",
    "train_counts = train['counts']\n",
    "num_docs_train = len(train_tokens)\n",
    "\n",
    "# 2. dev set\n",
    "valid_tokens = valid['tokens']\n",
    "valid_counts = valid['counts']\n",
    "num_docs_valid = len(valid_tokens)\n",
    "\n",
    "# 3. test data\n",
    "test_tokens = test['tokens']\n",
    "test_counts = test['counts']\n",
    "num_docs_test = len(test_tokens)\n",
    "test_1_tokens = test['tokens_1']\n",
    "test_1_counts = test['counts_1']\n",
    "num_docs_test_1 = len(test_1_tokens)\n",
    "test_2_tokens = test['tokens_2']\n",
    "test_2_counts = test['counts_2']\n",
    "num_docs_test_2 = len(test_2_tokens)\n",
    "\n",
    "embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n"
     ]
    }
   ],
   "source": [
    "#/usr/bin/python\n",
    "## get data\n",
    "# 1. vocabulary\n",
    "\n",
    "if not train_embeddings:\n",
    "    emb_path = emb_path\n",
    "    vect_path = os.path.join(data_path.split('/')[0], 'vocab.pkl')   \n",
    "    vectors = {}\n",
    "    with open(emb_path, 'rb') as f:\n",
    "        for l in f:\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "            if word in vocab:\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                vectors[word] = vect\n",
    "    embeddings = np.zeros((vocab_size, emb_size))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(vocab):\n",
    "        try: \n",
    "            embeddings[i] = vectors[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            embeddings[i] = np.random.normal(scale=0.6, size=(emb_size, ))\n",
    "    embeddings = torch.from_numpy(embeddings).to(device)\n",
    "    embeddings_dim = embeddings.size()\n",
    "\n",
    "print('=*'*100)\n",
    "# print('Training an Embedded Topic Model on {} with the following settings: {}'.format(dataset.upper()))\n",
    "print('=*'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embeddings/embeddings.txt'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.0)\n",
      "  (theta_act): ReLU()\n",
      "  (alphas): Linear(in_features=300, out_features=50, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=50, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=50, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## define checkpoint\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "if mode == 'eval':\n",
    "    ckpt = load_from\n",
    "else:\n",
    "    ckpt = os.path.join(save_path, \n",
    "        'etm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_trainEmbeddings_{}'.format(\n",
    "        dataset, num_topics, t_hidden_size, optimizer, clip, theta_act, \n",
    "            lr, batch_size, rho_size, train_embeddings))\n",
    "\n",
    "## define model and optimizer\n",
    "model = ETM(num_topics, vocab_size, t_hidden_size, rho_size, emb_size, \n",
    "                theta_act, embeddings, train_embeddings, enc_drop).to(device)\n",
    "\n",
    "print('model: {}'.format(model))\n",
    "\n",
    "if optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'asgd':\n",
    "    optimizer = optim.ASGD(model.parameters(), lr=lr, t0=0, lambd=0., weight_decay=wdecay)\n",
    "else:\n",
    "    print('Defaulting to vanilla SGD')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    acc_loss = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    cnt = 0\n",
    "    indices = torch.randperm(num_docs_train)\n",
    "    indices = torch.split(indices, batch_size)\n",
    "    for idx, ind in enumerate(indices):\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        data_batch = data.get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "        sums = data_batch.sum(1).unsqueeze(1)\n",
    "        if bow_norm:\n",
    "            normalized_data_batch = data_batch / sums\n",
    "        else:\n",
    "            normalized_data_batch = data_batch\n",
    "        recon_loss, kld_theta = model(data_batch, normalized_data_batch)\n",
    "        total_loss = recon_loss + kld_theta\n",
    "        total_loss.backward()\n",
    "\n",
    "        if clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_loss += torch.sum(recon_loss).item()\n",
    "        acc_kl_theta_loss += torch.sum(kld_theta).item()\n",
    "        cnt += 1\n",
    "\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            cur_loss = round(acc_loss / cnt, 2) \n",
    "            cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "            cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "\n",
    "            print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                epoch, idx, len(indices), optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    \n",
    "    cur_loss = round(acc_loss / cnt, 2) \n",
    "    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "    cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "    print('*'*100)\n",
    "    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "            epoch, optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    print('*'*100)\n",
    "\n",
    "def visualize(m, show_emb=True):\n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "\n",
    "    m.eval()\n",
    "\n",
    "    queries = ['andrew', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "\n",
    "    ## visualize topics using monte carlo\n",
    "    with torch.no_grad():\n",
    "        print('#'*100)\n",
    "        print('Visualize topics...')\n",
    "        topics_words = []\n",
    "        gammas = m.get_beta()\n",
    "        for k in range(args.num_topics):\n",
    "            gamma = gammas[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if show_emb:\n",
    "            ## visualize word embeddings by using V to get nearest neighbors\n",
    "            print('#'*100)\n",
    "            print('Visualize word embeddings by using output embedding matrix')\n",
    "            try:\n",
    "                embeddings = m.rho.weight  # Vocab_size x E\n",
    "            except:\n",
    "                embeddings = m.rho         # Vocab_size x E\n",
    "            neighbors = []\n",
    "            for word in queries:\n",
    "                print('word: {} .. neighbors: {}'.format(\n",
    "                    word, nearest_neighbors(word, embeddings, vocab)))\n",
    "            print('#'*100)\n",
    "\n",
    "def evaluate(m, source, tc=False, td=False):\n",
    "    \"\"\"Compute perplexity on document completion.\n",
    "    \"\"\"\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        if source == 'val':\n",
    "            indices = torch.split(torch.tensor(range(num_docs_valid)), eval_batch_size)\n",
    "            tokens = valid_tokens\n",
    "            counts = valid_counts\n",
    "        else: \n",
    "            indices = torch.split(torch.tensor(range(num_docs_test)), eval_batch_size)\n",
    "            tokens = test_tokens\n",
    "            counts = test_counts\n",
    "\n",
    "        ## get \\beta here\n",
    "        beta = m.get_beta()\n",
    "\n",
    "        ### do dc and tc here\n",
    "        acc_loss = 0\n",
    "        cnt = 0\n",
    "        indices_1 = torch.split(torch.tensor(range(num_docs_test_1)), eval_batch_size)\n",
    "        for idx, ind in enumerate(indices_1):\n",
    "            ## get theta from first half of docs\n",
    "            data_batch_1 = data.get_batch(test_1_tokens, test_1_counts, ind, vocab_size, device)\n",
    "            sums_1 = data_batch_1.sum(1).unsqueeze(1)\n",
    "            if bow_norm:\n",
    "                normalized_data_batch_1 = data_batch_1 / sums_1\n",
    "            else:\n",
    "                normalized_data_batch_1 = data_batch_1\n",
    "            theta, _ = m.get_theta(normalized_data_batch_1)\n",
    "\n",
    "            ## get prediction loss using second half\n",
    "            data_batch_2 = data.get_batch(test_2_tokens, test_2_counts, ind, vocab_size, device)\n",
    "            sums_2 = data_batch_2.sum(1).unsqueeze(1)\n",
    "            res = torch.mm(theta, beta)\n",
    "            preds = torch.log(res)\n",
    "            recon_loss = -(preds * data_batch_2).sum(1)\n",
    "            \n",
    "            loss = recon_loss / sums_2.squeeze()\n",
    "            loss = loss.mean().item()\n",
    "            acc_loss += loss\n",
    "            cnt += 1\n",
    "        cur_loss = acc_loss / cnt\n",
    "        ppl_dc = round(math.exp(cur_loss), 1)\n",
    "        print('*'*100)\n",
    "        print('{} Doc Completion PPL: {}'.format(source.upper(), ppl_dc))\n",
    "        print('*'*100)\n",
    "        if tc or td:\n",
    "            beta = beta.data.cpu().numpy()\n",
    "            if tc:\n",
    "                print('Computing topic coherence...')\n",
    "                get_topic_coherence(beta, train_tokens, vocab)\n",
    "            if td:\n",
    "                print('Computing topic diversity...')\n",
    "                get_topic_diversity(beta, 25)\n",
    "        return ppl_dc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "TEST Doc Completion PPL: 1676.2\n",
      "****************************************************************************************************\n",
      "\n",
      "The 10 most used topics are [41 23 18  8 20 49 22 45  9 44]\n",
      "\n",
      "\n",
      "Topic 0: ['writes', 'article', 'ca', 'university', 'distribution', 'world', 'posting', 'reply', 'nntp']\n",
      "Topic 1: ['writes', 'article', 'ca', 'distribution', 'sun', 'posting', 'steve', 'university', 'nntp']\n",
      "Topic 2: ['good', 'time', 'back', 'make', 'problem', 'work', 'lot', 'thing', 'problems']\n",
      "Topic 3: ['mail', 'hp', 'fax', 'phone', 'version', 'newsreader', 'tin', 'mit', 'email']\n",
      "Topic 4: ['people', 'make', 'writes', 'point', 'things', 'person', 'fact', 'wrong', 'article']\n",
      "Topic 5: ['posting', 'host', 'nntp', 'university', 'reply', 'mail', 'distribution', 'computer', 'version']\n",
      "Topic 6: ['university', 'posting', 'nntp', 'host', 'cs', 'cc', 'distribution', 'reply', 'columbia']\n",
      "Topic 7: ['mail', 'hp', 'info', 'email', 'version', 'newsreader', 'fax', 'tin', 'phone']\n",
      "Topic 8: ['drive', 'card', 'scsi', 'system', 'disk', 'video', 'mac', 'hard', 'memory']\n",
      "Topic 9: ['graphics', 'image', 'ftp', 'software', 'package', 'pub', 'data', 'images', 'version']\n",
      "Topic 10: ['water', 'power', 'ground', 'current', 'wire', 'circuit', 'high', 'low', 'supply']\n",
      "Topic 11: ['good', 'time', 'make', 'back', 'writes', 'thing', 'article', 'lot', 'give']\n",
      "Topic 12: ['good', 'time', 'writes', 'make', 'article', 'back', 'lot', 'thing', 'years']\n",
      "Topic 13: ['file', 'output', 'program', 'entry', 'information', 'faq', 'anonymous', 'list', 'email']\n",
      "Topic 14: ['writes', 'article', 'ca', 'distribution', 'posting', 'nntp', 'host', 'world', 'university']\n",
      "Topic 15: ['uiuc', 'state', 'ohio', 'article', 'cwru', 'pitt', 'cso', 'acs', 'cleveland']\n",
      "Topic 16: ['access', 'ibm', 'au', 'net', 'technology', 'digex', 'apr', 'pat', 'usa']\n",
      "Topic 17: ['writes', 'article', 'ca', 'posting', 'distribution', 'nntp', 'host', 'sun', 'university']\n",
      "Topic 18: ['israel', 'israeli', 'jews', 'people', 'turkish', 'armenian', 'world', 'jewish', 'war']\n",
      "Topic 19: ['university', 'host', 'posting', 'nntp', 'cs', 'cc', 'reply', 'distribution', 'columbia']\n",
      "Topic 20: ['windows', 'window', 'file', 'server', 'program', 'files', 'motif', 'screen', 'display']\n",
      "Topic 21: ['posting', 'nntp', 'university', 'host', 'cc', 'cs', 'columbia', 'reply', 'harvard']\n",
      "Topic 22: ['key', 'encryption', 'chip', 'clipper', 'keys', 'government', 'security', 'law', 'system']\n",
      "Topic 23: ['game', 'team', 'year', 'play', 'games', 'hockey', 'season', 'win', 'players']\n",
      "Topic 24: ['people', 'make', 'point', 'question', 'system', 'fact', 'claim', 'case', 'reason']\n",
      "Topic 25: ['information', 'april', 'national', 'research', 'university', 'american', 'states', 'united', 'conference']\n",
      "Topic 26: ['uk', 'ac', 'de', 'andrew', 'cs', 'cmu', 'colorado', 'university', 'umd']\n",
      "Topic 27: ['good', 'time', 'make', 'back', 'writes', 'thing', 'lot', 'years', 'article']\n",
      "Topic 28: ['posting', 'nntp', 'host', 'university', 'distribution', 'computer', 'reply', 'mail', 'usa']\n",
      "Topic 29: ['people', 'fbi', 'children', 'started', 'fire', 'told', 'happened', 'time', 'batf']\n",
      "Topic 30: ['posting', 'writes', 'article', 'host', 'university', 'nntp', 'distribution', 'reply', 'world']\n",
      "Topic 31: ['time', 'good', 'work', 'find', 'writes', 'back', 'make', 'problem', 'netcom']\n",
      "Topic 32: ['writes', 'good', 'make', 'time', 'article', 'people', 'thing', 'give', 'things']\n",
      "Topic 33: ['time', 'good', 'work', 'find', 'problem', 'back', 'problems', 'make', 'long']\n",
      "Topic 34: ['car', 'bike', 'dod', 'cars', 'engine', 'drive', 'road', 'speed', 'front']\n",
      "Topic 35: ['time', 'good', 'netcom', 'back', 'work', 'make', 'problem', 'find', 'years']\n",
      "Topic 36: ['people', 'make', 'writes', 'person', 'things', 'article', 'thing', 'made', 'fact']\n",
      "Topic 37: ['writes', 'article', 'ca', 'sun', 'posting', 'distribution', 'host', 'nntp', 'steve']\n",
      "Topic 38: ['book', 'good', 'books', 'points', 'find', 'time', 'point', 'reference', 'copy']\n",
      "Topic 39: ['writes', 'good', 'time', 'article', 'make', 'thing', 'back', 'netcom', 'give']\n",
      "Topic 40: ['max', 'ah', 'mr', 'tm', 'air', 'sp', 'ma', 'si', 'mi']\n",
      "Topic 41: ['god', 'people', 'jesus', 'christian', 'bible', 'christians', 'church', 'faith', 'religion']\n",
      "Topic 42: ['sale', 'price', 'offer', 'dos', 'shipping', 'printer', 'sell', 'cd', 'condition']\n",
      "Topic 43: ['people', 'system', 'make', 'point', 'question', 'claim', 'fact', 'reason', 'case']\n",
      "Topic 44: ['president', 'mr', 'government', 'people', 'clinton', 'tax', 'jobs', 'house', 'work']\n",
      "Topic 45: ['space', 'nasa', 'gov', 'henry', 'launch', 'toronto', 'earth', 'jpl', 'orbit']\n",
      "Topic 46: ['science', 'health', 'food', 'people', 'medical', 'disease', 'study', 'scientific', 'medicine']\n",
      "Topic 47: ['good', 'make', 'time', 'writes', 'people', 'thing', 'things', 'give', 'article']\n",
      "Topic 48: ['writes', 'article', 'ca', 'sun', 'distribution', 'posting', 'nntp', 'host', 'org']\n",
      "Topic 49: ['gun', 'guns', 'law', 'people', 'firearms', 'crime', 'rights', 'control', 'amendment']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if mode == 'train':\n",
    "    ## train model on data \n",
    "    best_epoch = 0\n",
    "    best_val_ppl = 1e9\n",
    "    all_val_ppls = []\n",
    "    print('\\n')\n",
    "    print('Visualizing model quality before training...')\n",
    "    visualize(model)\n",
    "    print('\\n')\n",
    "    for epoch in range(1, epochs):\n",
    "        train(epoch)\n",
    "        val_ppl = evaluate(model, 'val')\n",
    "        if val_ppl < best_val_ppl:\n",
    "            with open(ckpt, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_epoch = epoch\n",
    "            best_val_ppl = val_ppl\n",
    "        else:\n",
    "            ## check whether to anneal lr\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            if anneal_lr and (len(all_val_ppls) > nonmono and val_ppl > min(all_val_ppls[:-nonmono]) and lr > 1e-5):\n",
    "                optimizer.param_groups[0]['lr'] /= args.lr_factor\n",
    "        if epoch % args.visualize_every == 0:\n",
    "            visualize(model)\n",
    "        all_val_ppls.append(val_ppl)\n",
    "    with open(ckpt, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    model = model.to(device)\n",
    "    val_ppl = evaluate(model, 'val')\n",
    "else:   \n",
    "    with open(ckpt, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ## get document completion perplexities\n",
    "        test_ppl = evaluate(model, 'test', tc=tc, td=td)\n",
    "\n",
    "        ## get most used topics\n",
    "        indices = torch.tensor(range(num_docs_train))\n",
    "        indices = torch.split(indices, batch_size)\n",
    "        thetaAvg = torch.zeros(1, num_topics).to(device)\n",
    "        thetaWeightedAvg = torch.zeros(1, num_topics).to(device)\n",
    "        cnt = 0\n",
    "        for idx, ind in enumerate(indices):\n",
    "            data_batch = data.get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "            sums = data_batch.sum(1).unsqueeze(1)\n",
    "            cnt += sums.sum(0).squeeze().cpu().numpy()\n",
    "            if bow_norm:\n",
    "                normalized_data_batch = data_batch / sums\n",
    "            else:\n",
    "                normalized_data_batch = data_batch\n",
    "            theta, _ = model.get_theta(normalized_data_batch)\n",
    "            thetaAvg += theta.sum(0).unsqueeze(0) / num_docs_train\n",
    "            weighed_theta = sums * theta\n",
    "            thetaWeightedAvg += weighed_theta.sum(0).unsqueeze(0)\n",
    "            if idx % 100 == 0 and idx > 0:\n",
    "                print('batch: {}/{}'.format(idx, len(indices)))\n",
    "        thetaWeightedAvg = thetaWeightedAvg.squeeze().cpu().numpy() / cnt\n",
    "        print('\\nThe 10 most used topics are {}'.format(thetaWeightedAvg.argsort()[::-1][:10]))\n",
    "\n",
    "        ## show topics\n",
    "        beta = model.get_beta()\n",
    "        topic_indices = list(np.random.choice(num_topics, 10)) # 10 random topics\n",
    "        print('\\n')\n",
    "        for k in range(num_topics):#topic_indices:\n",
    "            gamma = beta[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if train_embeddings:\n",
    "            ## show etm embeddings \n",
    "            try:\n",
    "                rho_etm = model.rho.weight.cpu()\n",
    "            except:\n",
    "                rho_etm = model.rho.cpu()\n",
    "            queries = ['andrew', 'woman', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                            'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "            print('\\n')\n",
    "            print('ETM embeddings...')\n",
    "            for word in queries:\n",
    "                print('word: {} .. etm neighbors: {}'.format(word, nearest_neighbors(word, rho_etm, vocab)))\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --mode train --dataset 20ng --data_path data/20ng --num_topics 50 --train_embeddings 1 --epochs 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AZ Social Media Analytics)",
   "language": "python",
   "name": "atsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
