{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Required Embeddings\n",
    "\n",
    "Note: This section can be skipped if embeddings are already prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "# sys.setdefaultencoding() does not exist, here!\n",
    "# reload(sys)  # Reload does the trick!\n",
    "# sys.setdefaultencoding('UTF8')\n",
    "\n",
    "# sys.setdefaultencoding('UTF8')\n",
    "\n",
    "data_file= \"data/drug_review/drugsComTrain_raw.tsv\"        # default='', help='a .txt file containing the corpus'\n",
    "emb_file= \"embeddings/embeddings.txt\"                      #default='embeddings.txt', help='file to save the word embeddings'\n",
    "dim_rho= 300                                               #default=300, help='dimensionality of the word embeddings'\n",
    "min_count= 2                                               #default=2, help='minimum term frequency (to define the vocabulary)'\n",
    "sg= 1                                                      # default=1, help='whether to use skip-gram'\n",
    "workers= 6                                                 #default=25, help='number of CPU cores'\n",
    "negative_samples= 10                                       # default=10, help='number of negative samples'\n",
    "window_size= 4                                             # default=4, help='window size to determine context'\n",
    "iters= 50                                                  #default=50, help='number of iterationst'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class for a memory-friendly iterator over the dataset\n",
    "class MySentences(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.file_type = 'text'\n",
    " \n",
    "    def __iter__(self):\n",
    "        if self.file_type == 'text':\n",
    "            for line in open(self.filename,encoding=\"utf8\"):\n",
    "                yield line.split()\n",
    "        elif self.file_type == 'csv':\n",
    "            for line in self.reviews.values:\n",
    "                yield line.split()\n",
    "                \n",
    "    def __init__(self, filename,col,delimiter = \"\\t\"):\n",
    "        self.filename = filename\n",
    "        data = pd.read_csv(filename,delimiter=delimiter)\n",
    "        self.reviews = data[col][:1000]\n",
    "        self.file_type = 'csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MySentences(data_file,\"review\") # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(sentences, min_count=min_count, sg=sg, size=dim_rho, \n",
    "    iter=iters, workers=workers, negative=negative_samples, window=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Write the embeddings to a file\n",
    "with open(emb_file, 'w') as f:\n",
    "    for v in list(model.wv.vocab):\n",
    "        vec = list(model.wv.__getitem__(v))\n",
    "        f.write(v + ' ')\n",
    "        vec_str = ['%.9f' % val for val in vec]\n",
    "        vec_str = \" \".join(vec_str)\n",
    "        f.write(vec_str + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# abc = pickle.load(\"data/20ng/vocab.pkl\")\n",
    "# abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import data\n",
    "import scipy.io\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from etm import ETM\n",
    "from utils import nearest_neighbors, get_topic_coherence, get_topic_diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/drug_review/drugsComTrain_raw.tsv\",delimiter=\"\\t\")[:1000]\n",
    "# df.to_csv(\"data/drug_review/drugs_train_1000.csv\",index=None)\n",
    "# reviews = df.review\n",
    "# with open(\"train_file.txt\", 'w',encoding='utf8') as f:\n",
    "#     for review in reviews.values:\n",
    "#         f.write(review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"20ng\"\n",
    "\n",
    "dataset =   \"train_file.txt\"                      #default='20ng', help='name of corpus'\n",
    "data_path = 'data/drug_review/'#default='data/20ng', help='directory containing data'\n",
    "emb_path = 'embeddings/embeddings.txt'#default='data/20ng_embeddings.txt', help='directory containing word embeddings'\n",
    "save_path = './results'#default='./results', help='path to save results'\n",
    "batch_size = 100 #default=1000, help='input batch size for training'\n",
    "\n",
    "### model-related arguments\n",
    "num_topics = 15   #default=50, help='number of topics'\n",
    "rho_size = 300    #default=300, help='dimension of rho'\n",
    "emb_size = 300    #default=300, help='dimension of embeddings'\n",
    "t_hidden_size = 800 #default=800, help='dimension of hidden space of q(theta)'\n",
    "theta_act = 'relu' #default='relu', help='tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu)'\n",
    "train_embeddings = 0 #default=0, help='whether to fix rho or train it'\n",
    "\n",
    "### optimization-related arguments\n",
    "lr = 0.005 # default=0.005, help='learning rate'\n",
    "lr_factor =4.0  #default=4.0, help='divide learning rate by this...'\n",
    "epochs = 20 # default=20, help='number of epochs to train...150 for 20ng 100 for others'\n",
    "mode = 'train'# default='train', help='train or eval model'\n",
    "optimizer = 'adam'# default='adam', help='choice of optimizer'\n",
    "seed = 2019# default=2019, help='random seed (default: 1)\n",
    "enc_drop = 0.0# default=0.0, help='dropout rate on encoder'\n",
    "clip = 0.0# default=0.0, help='gradient clipping'\n",
    "nonmono = 10# default=10, help='number of bad hits allowed'\n",
    "wdecay = 1.2e-6# default=1.2e-6, help='some l2 regularization'\n",
    "anneal_lr = 0#  default=0, help='whether to anneal the learning rate or not'\n",
    "bow_norm = 1# default=1, help='normalize the bows or not'\n",
    "\n",
    "### evaluation, visualization, and logging-related arguments\n",
    "num_words = 10  # default=10, help='number of words for topic viz' \n",
    "log_interval = 2 # default=2, help='when to log training'\n",
    "visualize_every = 1 # default=10, help='when to visualize results'\n",
    "eval_batch_size = 1000 # default=1000, help='input batch size for evaluation'\n",
    "load_from = 'results/etm_20ng_K_50_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_1'# default='', help='the name of the ckpt to eval from'\n",
    "tc = 0# default=0, help='whether to compute topic coherence or not'\n",
    "td = 0# default=0, help='whether to compute topic diversity or not'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1684e33ef30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")#(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('\\n')\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, train, valid, test,test_1,test_2 = data.get_data(os.path.join(data_path))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 1. training data\n",
    "train_tokens = train['tokens']\n",
    "train_counts = train['counts']\n",
    "num_docs_train = len(train_tokens)\n",
    "\n",
    "# 2. dev set\n",
    "valid_tokens = valid['tokens']\n",
    "valid_counts = valid['counts']\n",
    "num_docs_valid = len(valid_tokens)\n",
    "\n",
    "# 3. test data\n",
    "test_tokens = test['tokens']\n",
    "test_counts = test['counts']\n",
    "num_docs_test = len(test_tokens)\n",
    "test_1_tokens = test_1['tokens']\n",
    "test_1_counts = test_1['counts']\n",
    "num_docs_test_1 = len(test_1_tokens)\n",
    "test_2_tokens = test_2['tokens']\n",
    "test_2_counts = test_2['counts']\n",
    "num_docs_test_2 = len(test_2_tokens)\n",
    "\n",
    "embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n"
     ]
    }
   ],
   "source": [
    "#/usr/bin/python\n",
    "## get data\n",
    "# 1. vocabulary\n",
    "\n",
    "if not train_embeddings:\n",
    "    emb_path = emb_path\n",
    "    vect_path = os.path.join(data_path.split('/')[0], 'vocab.pkl')   \n",
    "    vectors = {}\n",
    "    with open(emb_path, 'rb') as f:\n",
    "        for l in f:\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "            if word in vocab:\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                vectors[word] = vect\n",
    "    embeddings = np.zeros((vocab_size, emb_size))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(vocab):\n",
    "        try: \n",
    "            embeddings[i] = vectors[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            embeddings[i] = np.random.normal(scale=0.6, size=(emb_size, ))\n",
    "    embeddings = torch.tensor(embeddings).to(device)\n",
    "    embeddings_dim = embeddings.size()\n",
    "\n",
    "print('=*'*100)\n",
    "# print('Training an Embedded Topic Model on {} with the following settings: {}'.format(dataset.upper()))\n",
    "print('=*'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embeddings/embeddings.txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.0)\n",
      "  (theta_act): ReLU()\n",
      "  (alphas): Linear(in_features=300, out_features=15, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=78182, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=15, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## define checkpoint\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "if mode == 'eval':\n",
    "    ckpt = load_from\n",
    "else:\n",
    "    ckpt = os.path.join(save_path, \n",
    "        'etm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_trainEmbeddings_{}'.format(\n",
    "        dataset, num_topics, t_hidden_size, optimizer, clip, theta_act, \n",
    "            lr, batch_size, rho_size, train_embeddings))\n",
    "\n",
    "## define model and optimizer\n",
    "model = ETM(num_topics, vocab_size, t_hidden_size, rho_size, emb_size, \n",
    "                theta_act, embeddings, train_embeddings, enc_drop).to(device)\n",
    "\n",
    "print('model: {}'.format(model))\n",
    "\n",
    "if optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'asgd':\n",
    "    optimizer = optim.ASGD(model.parameters(), lr=lr, t0=0, lambd=0., weight_decay=wdecay)\n",
    "else:\n",
    "    print('Defaulting to vanilla SGD')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    acc_loss = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    cnt = 0\n",
    "    indices = torch.randperm(num_docs_train)\n",
    "    indices = torch.split(indices, batch_size)\n",
    "    for idx, ind in enumerate(indices):\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        data_batch = data.get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "        sums = data_batch.sum(1).unsqueeze(1)\n",
    "        if bow_norm:\n",
    "            normalized_data_batch = data_batch / sums\n",
    "        else:\n",
    "            normalized_data_batch = data_batch\n",
    "        recon_loss, kld_theta = model(data_batch, normalized_data_batch)\n",
    "        total_loss = recon_loss + kld_theta\n",
    "        total_loss.backward()\n",
    "\n",
    "        if clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_loss += torch.sum(recon_loss).item()\n",
    "        acc_kl_theta_loss += torch.sum(kld_theta).item()\n",
    "        cnt += 1\n",
    "\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            cur_loss = round(acc_loss / cnt, 2) \n",
    "            cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "            cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "\n",
    "            print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                epoch, idx, len(indices), optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    \n",
    "    cur_loss = round(acc_loss / cnt, 2) \n",
    "    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "    cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "    print('*'*100)\n",
    "    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "            epoch, optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(m, show_emb=True):\n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "\n",
    "    m.eval()\n",
    "\n",
    "    queries = ['skin','cycle','effects','price','worst','best','efficacy','performance','cancer']\n",
    "\n",
    "    ## visualize topics using monte carlo\n",
    "    with torch.no_grad():\n",
    "        print('#'*100)\n",
    "        print('Visualize topics...')\n",
    "        topics_words = []\n",
    "        gammas = m.get_beta()\n",
    "        for k in range(num_topics):\n",
    "            gamma = gammas[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if show_emb:\n",
    "            ## visualize word embeddings by using V to get nearest neighbors\n",
    "            print('#'*100)\n",
    "            print('Visualize word embeddings by using output embedding matrix')\n",
    "            try:\n",
    "                embeddings = m.rho.weight  # Vocab_size x E\n",
    "            except:\n",
    "                embeddings = m.rho         # Vocab_size x E\n",
    "            neighbors = []\n",
    "            for word in queries:\n",
    "                print('word: {} .. neighbors: {}'.format(\n",
    "                    word, nearest_neighbors(word, embeddings, vocab)))\n",
    "            print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(m, source, tc=False, td=False):\n",
    "    \"\"\"Compute perplexity on document completion.\n",
    "    \"\"\"\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        if source == 'val':\n",
    "            indices = torch.split(torch.tensor(range(num_docs_valid)), eval_batch_size)\n",
    "            tokens = valid_tokens\n",
    "            counts = valid_counts\n",
    "        else: \n",
    "            indices = torch.split(torch.tensor(range(num_docs_test)), eval_batch_size)\n",
    "            tokens = test_tokens\n",
    "            counts = test_counts\n",
    "\n",
    "        ## get \\beta here\n",
    "        beta = m.get_beta()\n",
    "\n",
    "        ### do dc and tc here\n",
    "        acc_loss = 0\n",
    "        cnt = 0\n",
    "        indices_1 = torch.split(torch.tensor(range(num_docs_test_1)), eval_batch_size)\n",
    "        for idx, ind in enumerate(indices_1):\n",
    "            ## get theta from first half of docs\n",
    "            data_batch_1 = data.get_batch(test_1_tokens, test_1_counts, ind, vocab_size, device)\n",
    "            sums_1 = data_batch_1.sum(1).unsqueeze(1)\n",
    "            if bow_norm:\n",
    "                normalized_data_batch_1 = data_batch_1 / sums_1\n",
    "            else:\n",
    "                normalized_data_batch_1 = data_batch_1\n",
    "            theta, _ = m.get_theta(normalized_data_batch_1)\n",
    "\n",
    "            ## get prediction loss using second half\n",
    "            data_batch_2 = data.get_batch(test_2_tokens, test_2_counts, ind, vocab_size, device)\n",
    "            sums_2 = data_batch_2.sum(1).unsqueeze(1)\n",
    "            res = torch.mm(theta, beta)\n",
    "            preds = torch.log(res)\n",
    "            recon_loss = -(preds * data_batch_2).sum(1)\n",
    "            \n",
    "            loss = recon_loss / sums_2.squeeze()\n",
    "            loss = loss.mean().item()\n",
    "            acc_loss += loss\n",
    "            cnt += 1\n",
    "        cur_loss = acc_loss / cnt\n",
    "        ppl_dc = round(math.exp(cur_loss), 1)\n",
    "        print('*'*100)\n",
    "        print('{} Doc Completion PPL: {}'.format(source.upper(), ppl_dc))\n",
    "        print('*'*100)\n",
    "        if tc or td:\n",
    "            beta = beta.data.cpu().numpy()\n",
    "            if tc:\n",
    "                print('Computing topic coherence...')\n",
    "                get_topic_coherence(beta, train_tokens, vocab)\n",
    "            if td:\n",
    "                print('Computing topic diversity...')\n",
    "                get_topic_diversity(beta, 25)\n",
    "        return ppl_dc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Visualizing model quality before training...\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0: ['pimtrea', 'aerations', 'desensitizing', 'popcoffee', 'claritin', 'geologist', 'promotion', 'neuropsychiatric', 'smps']\n",
      "Topic 1: ['strokechestshoulderarms', 'excellentnow', 'phenob', 'surprisingtried', 'waswhile', 'shepherds', 'beforegetting', 'bloadinggas', 'gelastic']\n",
      "Topic 2: ['wheezingcoughing', 'symptomsconvenient', 'meannoying', 'modafinil', 'bladderthe', 'viagraincreased', 'weightlbs', 'entrepreneur', 'quotlearn']\n",
      "Topic 3: ['surgeryquot', 'ldquomagicrdquo', 'quotunscheduled', 'propofal', 'portrayed', 'hesistant', 'dalfampridine', 'thirstdry', 'ductal']\n",
      "Topic 4: ['earsskin', 'balled', 'infomed', 'watermelons', 'precsribed', 'pigged', 'subsideanyone', 'benzaclinmiracle', 'plasticfeeling']\n",
      "Topic 5: ['labseptember', 'erosion', 'payback', 'unformation', 'sutured', 'spotty', 'caringreparil', 'preliletta', 'staggered']\n",
      "Topic 6: ['graced', 'eatersnacker', 'stressfull', 'vitalux', 'headachehate', 'zaart', 'ribaviran', 'golfers', 'sinkwith']\n",
      "Topic 7: ['appetitethis', 'sua', 'blacking', 'workwater', 'watershed', 'passedit', 'exhausting', 'strengthside', 'naltrexoneon']\n",
      "Topic 8: ['pronounce', 'selfinjectiions', 'mutch', 'bipolaritydepression', 'frames', 'spelled', 'superficially', 'webpage', 'ant']\n",
      "Topic 9: ['twangs', 'asp', 'matastised', 'psoriasispros', 'badgered', 'unbearableunable', 'brainampbody', 'benztropine', 'succession']\n",
      "Topic 10: ['erythromicynbp', 'mayjune', 'changesnothing', 'sensibility', 'quotonychomycosisquoti', 'pcs', 'pitty', 'minstrel', 'tirofiban']\n",
      "Topic 11: ['byaz', 'vagnitis', 'fogconfusion', 'rarest', 'liverwas', 'lysinei', 'bcbs', 'minsday', 'legarm']\n",
      "Topic 12: ['picks', 'contrave', 'meaningless', 'medicationin', 'violentdisturbing', 'costjust', 'altering', 'personfrustrating', 'spills']\n",
      "Topic 13: ['shortlived', 'runny', 'pantyliner', 'everythingincluding', 'nowaldara', 'quotfloppyheadquot', 'effervescent', 'infectionthese', 'abandonment']\n",
      "Topic 14: ['vk', 'antiinflammation', 'vfib', 'pumps', 'thatbut', 'ownfor', 'lamctii', 'shortensends', 'quotanger']\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: skin .. neighbors: ['skin', 'red', 'forehead', 'ankles', 'oily', 'broken', 'blotches', 'looks', 'clearer', 'grown', 'rod', 'itchy', 'clear', 'kid', 'rashes', 'uterus', 'breakouts', 'peel', 'irritation', 'turns']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: cycle .. neighbors: ['cycle', 'breakthrough', 'implanon', 'cheek', 'thrush', 'teens', 'occur', 'milk', 'boobs', 'bloody', 'cystic', 'infusion', 'stages', 'foot', 'pocket', 'chin', 'grown', 'growth', 'appointment', 'cleanser']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: effects .. neighbors: ['effects', 'side', 'sedative', 'adverse', 'besides', 'affects', 'hangover', 'lexapro', 'sexual', 'whatsoever', 'ridiculously', 'bothered', 'debilitating', 'benefit', 'reasons', 'opioid', 'wishes', 'hairs', 'thus', 'surprised']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: price .. neighbors: ['price', 'expensive', 'palpitations', 'orgasm', 'wash', 'paste', 'uric', 'performance', 'paying', 'scary', 'stressing', 'costs', 'promise', 'lately', 'burns', 'admit', 'bullet', 'heals', 'voices', 'eliminate']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: worst .. neighbors: ['worst', 'part', 'orsythia', 'incredibly', 'beating', 'best', 'anyways', 'decision', 'ring', 'chest', 'w', 'killing', 'stronger', 'heartburn', 'purchased', 'quality', 'cycle', 'drastically', 'government', 'form']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: best .. neighbors: ['best', 'favorite', 'solution', 'purchased', 'chill', 'worst', 'phenomenal', 'fewer', 'business', 'heard', 'linked', 'excellent', 'idea', 'father', 'admit', 'approved', 'key', 'bullet', 'ability', 'overall']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: efficacy .. neighbors: ['efficacy', 'sexapril', 'helplessnessdepression', 'dischargesomething', 'awaking', 'spiderlike', 'anomalies', 'axe', 'spasmsit', 'arthritisquot', 'halffinished', 'dryheaving', 'retardant', 'antiinflammatoryyou', 'monthsonly', 'dontthe', 'thirsty', 'trinza', 'birththe', 'apomodafinil']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: performance .. neighbors: ['performance', 'rapidly', 'concentration', 'wishes', 'horribly', 'moved', 'shaky', 'bones', 'flashes', 'feelings', 'appealing', 'intestinal', 'voices', 'fade', 'disorientation', 'muscles', 'us', 'somewhat', 'price', 'backing']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: cancer .. neighbors: ['cancer', 'fentanyl', 'behind', 'degenerative', 'disk', 'bloody', 'endometriosis', 'spasms', 'colon', 'tumor', 'intermittent', 'student', 'rheumatologist', 'cheek', 'treating', 'disc', 'landed', 'cord', 'generalized', 'racing']\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Epoch: 1 .. batch: 2/900 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 892.07 .. NELBO: 892.18\n",
      "Epoch: 1 .. batch: 4/900 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 890.64 .. NELBO: 890.78\n",
      "Epoch: 1 .. batch: 6/900 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 885.1 .. NELBO: 885.24\n",
      "Epoch: 1 .. batch: 8/900 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 877.81 .. NELBO: 877.96\n",
      "Epoch: 1 .. batch: 10/900 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 868.02 .. NELBO: 868.19\n",
      "Epoch: 1 .. batch: 12/900 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 877.61 .. NELBO: 877.79\n",
      "Epoch: 1 .. batch: 14/900 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 868.97 .. NELBO: 869.15\n",
      "Epoch: 1 .. batch: 16/900 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 864.4 .. NELBO: 864.59\n",
      "Epoch: 1 .. batch: 18/900 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 859.09 .. NELBO: 859.3\n",
      "Epoch: 1 .. batch: 20/900 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 852.51 .. NELBO: 852.73\n",
      "Epoch: 1 .. batch: 22/900 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 849.82 .. NELBO: 850.06\n",
      "Epoch: 1 .. batch: 24/900 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 840.17 .. NELBO: 840.43\n",
      "Epoch: 1 .. batch: 26/900 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 836.95 .. NELBO: 837.23\n",
      "Epoch: 1 .. batch: 28/900 .. LR: 0.005 .. KL_theta: 0.35 .. Rec_loss: 832.8 .. NELBO: 833.15\n",
      "Epoch: 1 .. batch: 30/900 .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 828.87 .. NELBO: 829.28\n",
      "Epoch: 1 .. batch: 32/900 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 823.38 .. NELBO: 823.9\n",
      "Epoch: 1 .. batch: 34/900 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 818.02 .. NELBO: 818.61\n",
      "Epoch: 1 .. batch: 36/900 .. LR: 0.005 .. KL_theta: 0.74 .. Rec_loss: 815.95 .. NELBO: 816.69\n",
      "Epoch: 1 .. batch: 38/900 .. LR: 0.005 .. KL_theta: 0.88 .. Rec_loss: 812.92 .. NELBO: 813.8\n",
      "Epoch: 1 .. batch: 40/900 .. LR: 0.005 .. KL_theta: 0.95 .. Rec_loss: 808.99 .. NELBO: 809.94\n",
      "Epoch: 1 .. batch: 42/900 .. LR: 0.005 .. KL_theta: 1.08 .. Rec_loss: 800.95 .. NELBO: 802.03\n",
      "Epoch: 1 .. batch: 44/900 .. LR: 0.005 .. KL_theta: 1.15 .. Rec_loss: 796.61 .. NELBO: 797.76\n",
      "Epoch: 1 .. batch: 46/900 .. LR: 0.005 .. KL_theta: 1.19 .. Rec_loss: 791.41 .. NELBO: 792.6\n",
      "Epoch: 1 .. batch: 48/900 .. LR: 0.005 .. KL_theta: 1.24 .. Rec_loss: 786.99 .. NELBO: 788.23\n",
      "Epoch: 1 .. batch: 50/900 .. LR: 0.005 .. KL_theta: 1.3 .. Rec_loss: 782.37 .. NELBO: 783.67\n",
      "Epoch: 1 .. batch: 52/900 .. LR: 0.005 .. KL_theta: 1.39 .. Rec_loss: 779.56 .. NELBO: 780.95\n",
      "Epoch: 1 .. batch: 54/900 .. LR: 0.005 .. KL_theta: 1.52 .. Rec_loss: 777.52 .. NELBO: 779.04\n",
      "Epoch: 1 .. batch: 56/900 .. LR: 0.005 .. KL_theta: 1.72 .. Rec_loss: 775.06 .. NELBO: 776.78\n",
      "Epoch: 1 .. batch: 58/900 .. LR: 0.005 .. KL_theta: 1.82 .. Rec_loss: 770.76 .. NELBO: 772.58\n",
      "Epoch: 1 .. batch: 60/900 .. LR: 0.005 .. KL_theta: 1.92 .. Rec_loss: 767.09 .. NELBO: 769.01\n",
      "Epoch: 1 .. batch: 62/900 .. LR: 0.005 .. KL_theta: 2.15 .. Rec_loss: 764.6 .. NELBO: 766.75\n",
      "Epoch: 1 .. batch: 64/900 .. LR: 0.005 .. KL_theta: 2.33 .. Rec_loss: 761.81 .. NELBO: 764.14\n",
      "Epoch: 1 .. batch: 66/900 .. LR: 0.005 .. KL_theta: 2.47 .. Rec_loss: 758.16 .. NELBO: 760.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 .. batch: 68/900 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 755.86 .. NELBO: 758.52\n",
      "Epoch: 1 .. batch: 70/900 .. LR: 0.005 .. KL_theta: 2.81 .. Rec_loss: 753.12 .. NELBO: 755.93\n",
      "Epoch: 1 .. batch: 72/900 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 752.64 .. NELBO: 755.65\n",
      "Epoch: 1 .. batch: 74/900 .. LR: 0.005 .. KL_theta: 3.16 .. Rec_loss: 751.61 .. NELBO: 754.77\n",
      "Epoch: 1 .. batch: 76/900 .. LR: 0.005 .. KL_theta: 3.3 .. Rec_loss: 748.43 .. NELBO: 751.73\n",
      "Epoch: 1 .. batch: 78/900 .. LR: 0.005 .. KL_theta: 3.45 .. Rec_loss: 746.83 .. NELBO: 750.28\n",
      "Epoch: 1 .. batch: 80/900 .. LR: 0.005 .. KL_theta: 3.57 .. Rec_loss: 744.44 .. NELBO: 748.01\n",
      "Epoch: 1 .. batch: 82/900 .. LR: 0.005 .. KL_theta: 3.68 .. Rec_loss: 742.54 .. NELBO: 746.22\n",
      "Epoch: 1 .. batch: 84/900 .. LR: 0.005 .. KL_theta: 3.82 .. Rec_loss: 740.36 .. NELBO: 744.18\n",
      "Epoch: 1 .. batch: 86/900 .. LR: 0.005 .. KL_theta: 3.94 .. Rec_loss: 738.83 .. NELBO: 742.77\n",
      "Epoch: 1 .. batch: 88/900 .. LR: 0.005 .. KL_theta: 4.06 .. Rec_loss: 739.02 .. NELBO: 743.08\n",
      "Epoch: 1 .. batch: 90/900 .. LR: 0.005 .. KL_theta: 4.15 .. Rec_loss: 737.4 .. NELBO: 741.55\n",
      "Epoch: 1 .. batch: 92/900 .. LR: 0.005 .. KL_theta: 4.27 .. Rec_loss: 736.49 .. NELBO: 740.76\n",
      "Epoch: 1 .. batch: 94/900 .. LR: 0.005 .. KL_theta: 4.38 .. Rec_loss: 735.59 .. NELBO: 739.97\n",
      "Epoch: 1 .. batch: 96/900 .. LR: 0.005 .. KL_theta: 4.45 .. Rec_loss: 733.85 .. NELBO: 738.3\n",
      "Epoch: 1 .. batch: 98/900 .. LR: 0.005 .. KL_theta: 4.55 .. Rec_loss: 734.36 .. NELBO: 738.91\n",
      "Epoch: 1 .. batch: 100/900 .. LR: 0.005 .. KL_theta: 4.62 .. Rec_loss: 732.11 .. NELBO: 736.73\n",
      "Epoch: 1 .. batch: 102/900 .. LR: 0.005 .. KL_theta: 4.7 .. Rec_loss: 731.37 .. NELBO: 736.07\n",
      "Epoch: 1 .. batch: 104/900 .. LR: 0.005 .. KL_theta: 4.78 .. Rec_loss: 730.03 .. NELBO: 734.81\n",
      "Epoch: 1 .. batch: 106/900 .. LR: 0.005 .. KL_theta: 4.83 .. Rec_loss: 728.62 .. NELBO: 733.45\n",
      "Epoch: 1 .. batch: 108/900 .. LR: 0.005 .. KL_theta: 4.91 .. Rec_loss: 727.73 .. NELBO: 732.64\n",
      "Epoch: 1 .. batch: 110/900 .. LR: 0.005 .. KL_theta: 4.96 .. Rec_loss: 726.43 .. NELBO: 731.39\n",
      "Epoch: 1 .. batch: 112/900 .. LR: 0.005 .. KL_theta: 5.03 .. Rec_loss: 725.39 .. NELBO: 730.42\n",
      "Epoch: 1 .. batch: 114/900 .. LR: 0.005 .. KL_theta: 5.08 .. Rec_loss: 724.96 .. NELBO: 730.04\n",
      "Epoch: 1 .. batch: 116/900 .. LR: 0.005 .. KL_theta: 5.14 .. Rec_loss: 723.94 .. NELBO: 729.08\n",
      "Epoch: 1 .. batch: 118/900 .. LR: 0.005 .. KL_theta: 5.2 .. Rec_loss: 723.27 .. NELBO: 728.47\n",
      "Epoch: 1 .. batch: 120/900 .. LR: 0.005 .. KL_theta: 5.24 .. Rec_loss: 722.01 .. NELBO: 727.25\n",
      "Epoch: 1 .. batch: 122/900 .. LR: 0.005 .. KL_theta: 5.29 .. Rec_loss: 721.42 .. NELBO: 726.71\n",
      "Epoch: 1 .. batch: 124/900 .. LR: 0.005 .. KL_theta: 5.34 .. Rec_loss: 720.49 .. NELBO: 725.83\n",
      "Epoch: 1 .. batch: 126/900 .. LR: 0.005 .. KL_theta: 5.39 .. Rec_loss: 719.69 .. NELBO: 725.08\n",
      "Epoch: 1 .. batch: 128/900 .. LR: 0.005 .. KL_theta: 5.43 .. Rec_loss: 718.7 .. NELBO: 724.13\n",
      "Epoch: 1 .. batch: 130/900 .. LR: 0.005 .. KL_theta: 5.46 .. Rec_loss: 717.99 .. NELBO: 723.45\n",
      "Epoch: 1 .. batch: 132/900 .. LR: 0.005 .. KL_theta: 5.5 .. Rec_loss: 716.92 .. NELBO: 722.42\n",
      "Epoch: 1 .. batch: 134/900 .. LR: 0.005 .. KL_theta: 5.53 .. Rec_loss: 716.22 .. NELBO: 721.75\n",
      "Epoch: 1 .. batch: 136/900 .. LR: 0.005 .. KL_theta: 5.57 .. Rec_loss: 715.2 .. NELBO: 720.77\n",
      "Epoch: 1 .. batch: 138/900 .. LR: 0.005 .. KL_theta: 5.59 .. Rec_loss: 714.27 .. NELBO: 719.86\n",
      "Epoch: 1 .. batch: 140/900 .. LR: 0.005 .. KL_theta: 5.63 .. Rec_loss: 714.19 .. NELBO: 719.82\n",
      "Epoch: 1 .. batch: 142/900 .. LR: 0.005 .. KL_theta: 5.67 .. Rec_loss: 713.27 .. NELBO: 718.94\n",
      "Epoch: 1 .. batch: 144/900 .. LR: 0.005 .. KL_theta: 5.68 .. Rec_loss: 712.78 .. NELBO: 718.46\n",
      "Epoch: 1 .. batch: 146/900 .. LR: 0.005 .. KL_theta: 5.71 .. Rec_loss: 712.25 .. NELBO: 717.96\n",
      "Epoch: 1 .. batch: 148/900 .. LR: 0.005 .. KL_theta: 5.74 .. Rec_loss: 712.35 .. NELBO: 718.09\n",
      "Epoch: 1 .. batch: 150/900 .. LR: 0.005 .. KL_theta: 5.76 .. Rec_loss: 711.79 .. NELBO: 717.55\n",
      "Epoch: 1 .. batch: 152/900 .. LR: 0.005 .. KL_theta: 5.79 .. Rec_loss: 711.42 .. NELBO: 717.21\n",
      "Epoch: 1 .. batch: 154/900 .. LR: 0.005 .. KL_theta: 5.81 .. Rec_loss: 710.32 .. NELBO: 716.13\n",
      "Epoch: 1 .. batch: 156/900 .. LR: 0.005 .. KL_theta: 5.82 .. Rec_loss: 709.42 .. NELBO: 715.24\n",
      "Epoch: 1 .. batch: 158/900 .. LR: 0.005 .. KL_theta: 5.83 .. Rec_loss: 708.62 .. NELBO: 714.45\n",
      "Epoch: 1 .. batch: 160/900 .. LR: 0.005 .. KL_theta: 5.86 .. Rec_loss: 708.09 .. NELBO: 713.95\n",
      "Epoch: 1 .. batch: 162/900 .. LR: 0.005 .. KL_theta: 5.88 .. Rec_loss: 707.19 .. NELBO: 713.07\n",
      "Epoch: 1 .. batch: 164/900 .. LR: 0.005 .. KL_theta: 5.88 .. Rec_loss: 706.88 .. NELBO: 712.76\n",
      "Epoch: 1 .. batch: 166/900 .. LR: 0.005 .. KL_theta: 5.9 .. Rec_loss: 706.37 .. NELBO: 712.27\n",
      "Epoch: 1 .. batch: 168/900 .. LR: 0.005 .. KL_theta: 5.92 .. Rec_loss: 705.75 .. NELBO: 711.67\n",
      "Epoch: 1 .. batch: 170/900 .. LR: 0.005 .. KL_theta: 5.93 .. Rec_loss: 704.89 .. NELBO: 710.82\n",
      "Epoch: 1 .. batch: 172/900 .. LR: 0.005 .. KL_theta: 5.94 .. Rec_loss: 704.03 .. NELBO: 709.97\n",
      "Epoch: 1 .. batch: 174/900 .. LR: 0.005 .. KL_theta: 5.95 .. Rec_loss: 703.19 .. NELBO: 709.14\n",
      "Epoch: 1 .. batch: 176/900 .. LR: 0.005 .. KL_theta: 5.97 .. Rec_loss: 702.33 .. NELBO: 708.3\n",
      "Epoch: 1 .. batch: 178/900 .. LR: 0.005 .. KL_theta: 5.98 .. Rec_loss: 702.02 .. NELBO: 708.0\n",
      "Epoch: 1 .. batch: 180/900 .. LR: 0.005 .. KL_theta: 5.98 .. Rec_loss: 701.37 .. NELBO: 707.35\n",
      "Epoch: 1 .. batch: 182/900 .. LR: 0.005 .. KL_theta: 6.0 .. Rec_loss: 700.41 .. NELBO: 706.41\n",
      "Epoch: 1 .. batch: 184/900 .. LR: 0.005 .. KL_theta: 6.0 .. Rec_loss: 700.27 .. NELBO: 706.27\n",
      "Epoch: 1 .. batch: 186/900 .. LR: 0.005 .. KL_theta: 6.01 .. Rec_loss: 699.83 .. NELBO: 705.84\n",
      "Epoch: 1 .. batch: 188/900 .. LR: 0.005 .. KL_theta: 6.03 .. Rec_loss: 699.21 .. NELBO: 705.24\n",
      "Epoch: 1 .. batch: 190/900 .. LR: 0.005 .. KL_theta: 6.04 .. Rec_loss: 698.9 .. NELBO: 704.94\n",
      "Epoch: 1 .. batch: 192/900 .. LR: 0.005 .. KL_theta: 6.04 .. Rec_loss: 698.11 .. NELBO: 704.15\n",
      "Epoch: 1 .. batch: 194/900 .. LR: 0.005 .. KL_theta: 6.05 .. Rec_loss: 697.45 .. NELBO: 703.5\n",
      "Epoch: 1 .. batch: 196/900 .. LR: 0.005 .. KL_theta: 6.06 .. Rec_loss: 697.37 .. NELBO: 703.43\n",
      "Epoch: 1 .. batch: 198/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 696.67 .. NELBO: 702.74\n",
      "Epoch: 1 .. batch: 200/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 696.37 .. NELBO: 702.44\n",
      "Epoch: 1 .. batch: 202/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 696.36 .. NELBO: 702.44\n",
      "Epoch: 1 .. batch: 204/900 .. LR: 0.005 .. KL_theta: 6.09 .. Rec_loss: 695.9 .. NELBO: 701.99\n",
      "Epoch: 1 .. batch: 206/900 .. LR: 0.005 .. KL_theta: 6.09 .. Rec_loss: 695.16 .. NELBO: 701.25\n",
      "Epoch: 1 .. batch: 208/900 .. LR: 0.005 .. KL_theta: 6.1 .. Rec_loss: 694.36 .. NELBO: 700.46\n",
      "Epoch: 1 .. batch: 210/900 .. LR: 0.005 .. KL_theta: 6.1 .. Rec_loss: 694.21 .. NELBO: 700.31\n",
      "Epoch: 1 .. batch: 212/900 .. LR: 0.005 .. KL_theta: 6.1 .. Rec_loss: 693.65 .. NELBO: 699.75\n",
      "Epoch: 1 .. batch: 214/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 693.19 .. NELBO: 699.3\n",
      "Epoch: 1 .. batch: 216/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 692.75 .. NELBO: 698.86\n",
      "Epoch: 1 .. batch: 218/900 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 692.13 .. NELBO: 698.25\n",
      "Epoch: 1 .. batch: 220/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 691.43 .. NELBO: 697.54\n",
      "Epoch: 1 .. batch: 222/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 690.8 .. NELBO: 696.91\n",
      "Epoch: 1 .. batch: 224/900 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 690.49 .. NELBO: 696.61\n",
      "Epoch: 1 .. batch: 226/900 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 690.27 .. NELBO: 696.39\n",
      "Epoch: 1 .. batch: 228/900 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 689.73 .. NELBO: 695.85\n",
      "Epoch: 1 .. batch: 230/900 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 689.46 .. NELBO: 695.59\n",
      "Epoch: 1 .. batch: 232/900 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 688.8 .. NELBO: 694.93\n",
      "Epoch: 1 .. batch: 234/900 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 688.79 .. NELBO: 694.92\n",
      "Epoch: 1 .. batch: 236/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 688.48 .. NELBO: 694.62\n",
      "Epoch: 1 .. batch: 238/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 688.21 .. NELBO: 694.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 .. batch: 240/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 687.54 .. NELBO: 693.68\n",
      "Epoch: 1 .. batch: 242/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 687.14 .. NELBO: 693.28\n",
      "Epoch: 1 .. batch: 244/900 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 686.84 .. NELBO: 692.99\n",
      "Epoch: 1 .. batch: 246/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 686.29 .. NELBO: 692.43\n",
      "Epoch: 1 .. batch: 248/900 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 685.74 .. NELBO: 691.89\n",
      "Epoch: 1 .. batch: 250/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 685.53 .. NELBO: 691.69\n",
      "Epoch: 1 .. batch: 252/900 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 685.26 .. NELBO: 691.41\n",
      "Epoch: 1 .. batch: 254/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 684.91 .. NELBO: 691.07\n",
      "Epoch: 1 .. batch: 256/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 684.52 .. NELBO: 690.68\n",
      "Epoch: 1 .. batch: 258/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 684.3 .. NELBO: 690.46\n",
      "Epoch: 1 .. batch: 260/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 683.72 .. NELBO: 689.88\n",
      "Epoch: 1 .. batch: 262/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 683.27 .. NELBO: 689.43\n",
      "Epoch: 1 .. batch: 264/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 682.91 .. NELBO: 689.07\n",
      "Epoch: 1 .. batch: 266/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 682.6 .. NELBO: 688.76\n",
      "Epoch: 1 .. batch: 268/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 682.22 .. NELBO: 688.38\n",
      "Epoch: 1 .. batch: 270/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 681.74 .. NELBO: 687.9\n",
      "Epoch: 1 .. batch: 272/900 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 681.37 .. NELBO: 687.52\n",
      "Epoch: 1 .. batch: 274/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 681.09 .. NELBO: 687.25\n",
      "Epoch: 1 .. batch: 276/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 680.84 .. NELBO: 687.0\n",
      "Epoch: 1 .. batch: 278/900 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 680.49 .. NELBO: 686.64\n",
      "Epoch: 1 .. batch: 280/900 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 679.89 .. NELBO: 686.04\n",
      "Epoch: 1 .. batch: 282/900 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 679.75 .. NELBO: 685.9\n",
      "Epoch: 1 .. batch: 284/900 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 679.28 .. NELBO: 685.43\n",
      "Epoch: 1 .. batch: 286/900 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 679.02 .. NELBO: 685.17\n",
      "Epoch: 1 .. batch: 288/900 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 678.83 .. NELBO: 684.98\n",
      "Epoch: 1 .. batch: 290/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 678.63 .. NELBO: 684.77\n",
      "Epoch: 1 .. batch: 292/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 678.27 .. NELBO: 684.41\n",
      "Epoch: 1 .. batch: 294/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 678.1 .. NELBO: 684.24\n",
      "Epoch: 1 .. batch: 296/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 677.74 .. NELBO: 683.88\n",
      "Epoch: 1 .. batch: 298/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 677.2 .. NELBO: 683.34\n",
      "Epoch: 1 .. batch: 300/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 677.04 .. NELBO: 683.18\n",
      "Epoch: 1 .. batch: 302/900 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 676.62 .. NELBO: 682.75\n",
      "Epoch: 1 .. batch: 304/900 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 676.33 .. NELBO: 682.46\n",
      "Epoch: 1 .. batch: 306/900 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 676.18 .. NELBO: 682.31\n",
      "Epoch: 1 .. batch: 308/900 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 675.68 .. NELBO: 681.81\n",
      "Epoch: 1 .. batch: 310/900 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 675.14 .. NELBO: 681.26\n",
      "Epoch: 1 .. batch: 312/900 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 674.9 .. NELBO: 681.02\n",
      "Epoch: 1 .. batch: 314/900 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 674.74 .. NELBO: 680.86\n",
      "Epoch: 1 .. batch: 316/900 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 674.55 .. NELBO: 680.67\n",
      "Epoch: 1 .. batch: 318/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 674.17 .. NELBO: 680.28\n",
      "Epoch: 1 .. batch: 320/900 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 674.14 .. NELBO: 680.26\n",
      "Epoch: 1 .. batch: 322/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 674.16 .. NELBO: 680.27\n",
      "Epoch: 1 .. batch: 324/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 673.93 .. NELBO: 680.04\n",
      "Epoch: 1 .. batch: 326/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 673.77 .. NELBO: 679.88\n",
      "Epoch: 1 .. batch: 328/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 673.7 .. NELBO: 679.81\n",
      "Epoch: 1 .. batch: 330/900 .. LR: 0.005 .. KL_theta: 6.1 .. Rec_loss: 673.41 .. NELBO: 679.51\n",
      "Epoch: 1 .. batch: 332/900 .. LR: 0.005 .. KL_theta: 6.1 .. Rec_loss: 673.33 .. NELBO: 679.43\n",
      "Epoch: 1 .. batch: 334/900 .. LR: 0.005 .. KL_theta: 6.1 .. Rec_loss: 673.06 .. NELBO: 679.16\n",
      "Epoch: 1 .. batch: 336/900 .. LR: 0.005 .. KL_theta: 6.1 .. Rec_loss: 672.73 .. NELBO: 678.83\n",
      "Epoch: 1 .. batch: 338/900 .. LR: 0.005 .. KL_theta: 6.09 .. Rec_loss: 672.53 .. NELBO: 678.62\n",
      "Epoch: 1 .. batch: 340/900 .. LR: 0.005 .. KL_theta: 6.09 .. Rec_loss: 672.17 .. NELBO: 678.26\n",
      "Epoch: 1 .. batch: 342/900 .. LR: 0.005 .. KL_theta: 6.09 .. Rec_loss: 672.05 .. NELBO: 678.14\n",
      "Epoch: 1 .. batch: 344/900 .. LR: 0.005 .. KL_theta: 6.09 .. Rec_loss: 671.96 .. NELBO: 678.05\n",
      "Epoch: 1 .. batch: 346/900 .. LR: 0.005 .. KL_theta: 6.09 .. Rec_loss: 671.54 .. NELBO: 677.63\n",
      "Epoch: 1 .. batch: 348/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 671.16 .. NELBO: 677.24\n",
      "Epoch: 1 .. batch: 350/900 .. LR: 0.005 .. KL_theta: 6.09 .. Rec_loss: 670.9 .. NELBO: 676.99\n",
      "Epoch: 1 .. batch: 352/900 .. LR: 0.005 .. KL_theta: 6.09 .. Rec_loss: 670.8 .. NELBO: 676.89\n",
      "Epoch: 1 .. batch: 354/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 670.42 .. NELBO: 676.5\n",
      "Epoch: 1 .. batch: 356/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 670.29 .. NELBO: 676.37\n",
      "Epoch: 1 .. batch: 358/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 670.04 .. NELBO: 676.12\n",
      "Epoch: 1 .. batch: 360/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 669.61 .. NELBO: 675.69\n",
      "Epoch: 1 .. batch: 362/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 669.5 .. NELBO: 675.58\n",
      "Epoch: 1 .. batch: 364/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 669.22 .. NELBO: 675.3\n",
      "Epoch: 1 .. batch: 366/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 668.92 .. NELBO: 675.0\n",
      "Epoch: 1 .. batch: 368/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 668.68 .. NELBO: 674.76\n",
      "Epoch: 1 .. batch: 370/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 668.43 .. NELBO: 674.51\n",
      "Epoch: 1 .. batch: 372/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 668.34 .. NELBO: 674.42\n",
      "Epoch: 1 .. batch: 374/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 668.28 .. NELBO: 674.36\n",
      "Epoch: 1 .. batch: 376/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 667.91 .. NELBO: 673.99\n",
      "Epoch: 1 .. batch: 378/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 667.65 .. NELBO: 673.73\n",
      "Epoch: 1 .. batch: 380/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 667.37 .. NELBO: 673.45\n",
      "Epoch: 1 .. batch: 382/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 667.28 .. NELBO: 673.36\n",
      "Epoch: 1 .. batch: 384/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 666.86 .. NELBO: 672.93\n",
      "Epoch: 1 .. batch: 386/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 666.59 .. NELBO: 672.66\n",
      "Epoch: 1 .. batch: 388/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 666.44 .. NELBO: 672.51\n",
      "Epoch: 1 .. batch: 390/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 666.23 .. NELBO: 672.3\n",
      "Epoch: 1 .. batch: 392/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 665.92 .. NELBO: 671.99\n",
      "Epoch: 1 .. batch: 394/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 665.55 .. NELBO: 671.62\n",
      "Epoch: 1 .. batch: 396/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 665.31 .. NELBO: 671.38\n",
      "Epoch: 1 .. batch: 398/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 665.16 .. NELBO: 671.23\n",
      "Epoch: 1 .. batch: 400/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 664.95 .. NELBO: 671.02\n",
      "Epoch: 1 .. batch: 402/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 664.65 .. NELBO: 670.72\n",
      "Epoch: 1 .. batch: 404/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 664.43 .. NELBO: 670.5\n",
      "Epoch: 1 .. batch: 406/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 664.11 .. NELBO: 670.19\n",
      "Epoch: 1 .. batch: 408/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 663.83 .. NELBO: 669.9\n",
      "Epoch: 1 .. batch: 410/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 663.81 .. NELBO: 669.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 .. batch: 412/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 663.63 .. NELBO: 669.7\n",
      "Epoch: 1 .. batch: 414/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 663.37 .. NELBO: 669.44\n",
      "Epoch: 1 .. batch: 416/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 663.14 .. NELBO: 669.21\n",
      "Epoch: 1 .. batch: 418/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 662.79 .. NELBO: 668.86\n",
      "Epoch: 1 .. batch: 420/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 662.42 .. NELBO: 668.49\n",
      "Epoch: 1 .. batch: 422/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 661.99 .. NELBO: 668.06\n",
      "Epoch: 1 .. batch: 424/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 661.79 .. NELBO: 667.86\n",
      "Epoch: 1 .. batch: 426/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 661.53 .. NELBO: 667.6\n",
      "Epoch: 1 .. batch: 428/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 661.3 .. NELBO: 667.37\n",
      "Epoch: 1 .. batch: 430/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 661.16 .. NELBO: 667.23\n",
      "Epoch: 1 .. batch: 432/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 661.02 .. NELBO: 667.09\n",
      "Epoch: 1 .. batch: 434/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 660.73 .. NELBO: 666.8\n",
      "Epoch: 1 .. batch: 436/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 660.53 .. NELBO: 666.6\n",
      "Epoch: 1 .. batch: 438/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 660.31 .. NELBO: 666.38\n",
      "Epoch: 1 .. batch: 440/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 660.22 .. NELBO: 666.29\n",
      "Epoch: 1 .. batch: 442/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 660.02 .. NELBO: 666.09\n",
      "Epoch: 1 .. batch: 444/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 659.92 .. NELBO: 665.99\n",
      "Epoch: 1 .. batch: 446/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 659.59 .. NELBO: 665.66\n",
      "Epoch: 1 .. batch: 448/900 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 659.44 .. NELBO: 665.51\n",
      "Epoch: 1 .. batch: 450/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 659.09 .. NELBO: 665.17\n",
      "Epoch: 1 .. batch: 452/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 658.88 .. NELBO: 664.96\n",
      "Epoch: 1 .. batch: 454/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 658.58 .. NELBO: 664.66\n",
      "Epoch: 1 .. batch: 456/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 658.27 .. NELBO: 664.35\n",
      "Epoch: 1 .. batch: 458/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 658.16 .. NELBO: 664.24\n",
      "Epoch: 1 .. batch: 460/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 657.92 .. NELBO: 664.0\n",
      "Epoch: 1 .. batch: 462/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 657.63 .. NELBO: 663.71\n",
      "Epoch: 1 .. batch: 464/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 657.41 .. NELBO: 663.49\n",
      "Epoch: 1 .. batch: 466/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 657.23 .. NELBO: 663.31\n",
      "Epoch: 1 .. batch: 468/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 656.91 .. NELBO: 662.99\n",
      "Epoch: 1 .. batch: 470/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 656.67 .. NELBO: 662.75\n",
      "Epoch: 1 .. batch: 472/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 656.62 .. NELBO: 662.7\n",
      "Epoch: 1 .. batch: 474/900 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 656.37 .. NELBO: 662.45\n",
      "Epoch: 1 .. batch: 476/900 .. LR: 0.005 .. KL_theta: 6.09 .. Rec_loss: 656.11 .. NELBO: 662.2\n",
      "Epoch: 1 .. batch: 478/900 .. LR: 0.005 .. KL_theta: 6.09 .. Rec_loss: 656.02 .. NELBO: 662.11\n",
      "Epoch: 1 .. batch: 480/900 .. LR: 0.005 .. KL_theta: 6.09 .. Rec_loss: 655.78 .. NELBO: 661.87\n",
      "Epoch: 1 .. batch: 482/900 .. LR: 0.005 .. KL_theta: 6.09 .. Rec_loss: 655.6 .. NELBO: 661.69\n",
      "Epoch: 1 .. batch: 484/900 .. LR: 0.005 .. KL_theta: 6.1 .. Rec_loss: 655.47 .. NELBO: 661.57\n",
      "Epoch: 1 .. batch: 486/900 .. LR: 0.005 .. KL_theta: 6.1 .. Rec_loss: 655.29 .. NELBO: 661.39\n",
      "Epoch: 1 .. batch: 488/900 .. LR: 0.005 .. KL_theta: 6.1 .. Rec_loss: 655.11 .. NELBO: 661.21\n",
      "Epoch: 1 .. batch: 490/900 .. LR: 0.005 .. KL_theta: 6.1 .. Rec_loss: 655.09 .. NELBO: 661.19\n",
      "Epoch: 1 .. batch: 492/900 .. LR: 0.005 .. KL_theta: 6.1 .. Rec_loss: 654.97 .. NELBO: 661.07\n",
      "Epoch: 1 .. batch: 494/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 654.82 .. NELBO: 660.93\n",
      "Epoch: 1 .. batch: 496/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 654.55 .. NELBO: 660.66\n",
      "Epoch: 1 .. batch: 498/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 654.33 .. NELBO: 660.44\n",
      "Epoch: 1 .. batch: 500/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 654.09 .. NELBO: 660.2\n",
      "Epoch: 1 .. batch: 502/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 653.81 .. NELBO: 659.92\n",
      "Epoch: 1 .. batch: 504/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 653.44 .. NELBO: 659.55\n",
      "Epoch: 1 .. batch: 506/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 653.22 .. NELBO: 659.33\n",
      "Epoch: 1 .. batch: 508/900 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 652.99 .. NELBO: 659.1\n",
      "Epoch: 1 .. batch: 510/900 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 652.82 .. NELBO: 658.94\n",
      "Epoch: 1 .. batch: 512/900 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 652.54 .. NELBO: 658.66\n",
      "Epoch: 1 .. batch: 514/900 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 652.43 .. NELBO: 658.55\n",
      "Epoch: 1 .. batch: 516/900 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 652.13 .. NELBO: 658.25\n",
      "Epoch: 1 .. batch: 518/900 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 651.97 .. NELBO: 658.09\n",
      "Epoch: 1 .. batch: 520/900 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 651.84 .. NELBO: 657.97\n",
      "Epoch: 1 .. batch: 522/900 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 651.64 .. NELBO: 657.77\n",
      "Epoch: 1 .. batch: 524/900 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 651.4 .. NELBO: 657.53\n",
      "Epoch: 1 .. batch: 526/900 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 651.23 .. NELBO: 657.36\n",
      "Epoch: 1 .. batch: 528/900 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 651.02 .. NELBO: 657.15\n",
      "Epoch: 1 .. batch: 530/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 650.98 .. NELBO: 657.12\n",
      "Epoch: 1 .. batch: 532/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 650.6 .. NELBO: 656.74\n",
      "Epoch: 1 .. batch: 534/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 650.3 .. NELBO: 656.44\n",
      "Epoch: 1 .. batch: 536/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 650.07 .. NELBO: 656.21\n",
      "Epoch: 1 .. batch: 538/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 649.96 .. NELBO: 656.1\n",
      "Epoch: 1 .. batch: 540/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 649.68 .. NELBO: 655.82\n",
      "Epoch: 1 .. batch: 542/900 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 649.52 .. NELBO: 655.66\n",
      "Epoch: 1 .. batch: 544/900 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 649.45 .. NELBO: 655.6\n",
      "Epoch: 1 .. batch: 546/900 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 649.25 .. NELBO: 655.4\n",
      "Epoch: 1 .. batch: 548/900 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 649.1 .. NELBO: 655.25\n",
      "Epoch: 1 .. batch: 550/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 648.96 .. NELBO: 655.12\n",
      "Epoch: 1 .. batch: 552/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 648.81 .. NELBO: 654.97\n",
      "Epoch: 1 .. batch: 554/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 648.64 .. NELBO: 654.8\n",
      "Epoch: 1 .. batch: 556/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 648.4 .. NELBO: 654.56\n",
      "Epoch: 1 .. batch: 558/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 648.23 .. NELBO: 654.39\n",
      "Epoch: 1 .. batch: 560/900 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 648.09 .. NELBO: 654.25\n",
      "Epoch: 1 .. batch: 562/900 .. LR: 0.005 .. KL_theta: 6.17 .. Rec_loss: 647.95 .. NELBO: 654.12\n",
      "Epoch: 1 .. batch: 564/900 .. LR: 0.005 .. KL_theta: 6.17 .. Rec_loss: 647.87 .. NELBO: 654.04\n",
      "Epoch: 1 .. batch: 566/900 .. LR: 0.005 .. KL_theta: 6.17 .. Rec_loss: 647.78 .. NELBO: 653.95\n",
      "Epoch: 1 .. batch: 568/900 .. LR: 0.005 .. KL_theta: 6.18 .. Rec_loss: 647.68 .. NELBO: 653.86\n",
      "Epoch: 1 .. batch: 570/900 .. LR: 0.005 .. KL_theta: 6.18 .. Rec_loss: 647.41 .. NELBO: 653.59\n",
      "Epoch: 1 .. batch: 572/900 .. LR: 0.005 .. KL_theta: 6.18 .. Rec_loss: 647.36 .. NELBO: 653.54\n",
      "Epoch: 1 .. batch: 574/900 .. LR: 0.005 .. KL_theta: 6.18 .. Rec_loss: 647.18 .. NELBO: 653.36\n",
      "Epoch: 1 .. batch: 576/900 .. LR: 0.005 .. KL_theta: 6.19 .. Rec_loss: 647.04 .. NELBO: 653.23\n",
      "Epoch: 1 .. batch: 578/900 .. LR: 0.005 .. KL_theta: 6.19 .. Rec_loss: 646.86 .. NELBO: 653.05\n",
      "Epoch: 1 .. batch: 580/900 .. LR: 0.005 .. KL_theta: 6.19 .. Rec_loss: 646.83 .. NELBO: 653.02\n",
      "Epoch: 1 .. batch: 582/900 .. LR: 0.005 .. KL_theta: 6.19 .. Rec_loss: 646.63 .. NELBO: 652.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 .. batch: 584/900 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 646.41 .. NELBO: 652.61\n",
      "Epoch: 1 .. batch: 586/900 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 646.2 .. NELBO: 652.4\n",
      "Epoch: 1 .. batch: 588/900 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 646.14 .. NELBO: 652.34\n",
      "Epoch: 1 .. batch: 590/900 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 646.02 .. NELBO: 652.22\n",
      "Epoch: 1 .. batch: 592/900 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 645.83 .. NELBO: 652.03\n",
      "Epoch: 1 .. batch: 594/900 .. LR: 0.005 .. KL_theta: 6.21 .. Rec_loss: 645.76 .. NELBO: 651.97\n",
      "Epoch: 1 .. batch: 596/900 .. LR: 0.005 .. KL_theta: 6.21 .. Rec_loss: 645.57 .. NELBO: 651.78\n",
      "Epoch: 1 .. batch: 598/900 .. LR: 0.005 .. KL_theta: 6.21 .. Rec_loss: 645.52 .. NELBO: 651.73\n",
      "Epoch: 1 .. batch: 600/900 .. LR: 0.005 .. KL_theta: 6.21 .. Rec_loss: 645.31 .. NELBO: 651.52\n",
      "Epoch: 1 .. batch: 602/900 .. LR: 0.005 .. KL_theta: 6.22 .. Rec_loss: 645.21 .. NELBO: 651.43\n",
      "Epoch: 1 .. batch: 604/900 .. LR: 0.005 .. KL_theta: 6.22 .. Rec_loss: 645.02 .. NELBO: 651.24\n",
      "Epoch: 1 .. batch: 606/900 .. LR: 0.005 .. KL_theta: 6.22 .. Rec_loss: 644.91 .. NELBO: 651.13\n",
      "Epoch: 1 .. batch: 608/900 .. LR: 0.005 .. KL_theta: 6.22 .. Rec_loss: 644.84 .. NELBO: 651.06\n",
      "Epoch: 1 .. batch: 610/900 .. LR: 0.005 .. KL_theta: 6.22 .. Rec_loss: 644.66 .. NELBO: 650.88\n",
      "Epoch: 1 .. batch: 612/900 .. LR: 0.005 .. KL_theta: 6.23 .. Rec_loss: 644.42 .. NELBO: 650.65\n",
      "Epoch: 1 .. batch: 614/900 .. LR: 0.005 .. KL_theta: 6.23 .. Rec_loss: 644.18 .. NELBO: 650.41\n",
      "Epoch: 1 .. batch: 616/900 .. LR: 0.005 .. KL_theta: 6.23 .. Rec_loss: 644.0 .. NELBO: 650.23\n",
      "Epoch: 1 .. batch: 618/900 .. LR: 0.005 .. KL_theta: 6.23 .. Rec_loss: 643.81 .. NELBO: 650.04\n",
      "Epoch: 1 .. batch: 620/900 .. LR: 0.005 .. KL_theta: 6.23 .. Rec_loss: 643.64 .. NELBO: 649.87\n",
      "Epoch: 1 .. batch: 622/900 .. LR: 0.005 .. KL_theta: 6.24 .. Rec_loss: 643.48 .. NELBO: 649.72\n",
      "Epoch: 1 .. batch: 624/900 .. LR: 0.005 .. KL_theta: 6.24 .. Rec_loss: 643.43 .. NELBO: 649.67\n",
      "Epoch: 1 .. batch: 626/900 .. LR: 0.005 .. KL_theta: 6.24 .. Rec_loss: 643.4 .. NELBO: 649.64\n",
      "Epoch: 1 .. batch: 628/900 .. LR: 0.005 .. KL_theta: 6.25 .. Rec_loss: 643.3 .. NELBO: 649.55\n",
      "Epoch: 1 .. batch: 630/900 .. LR: 0.005 .. KL_theta: 6.25 .. Rec_loss: 643.08 .. NELBO: 649.33\n",
      "Epoch: 1 .. batch: 632/900 .. LR: 0.005 .. KL_theta: 6.25 .. Rec_loss: 643.0 .. NELBO: 649.25\n",
      "Epoch: 1 .. batch: 634/900 .. LR: 0.005 .. KL_theta: 6.25 .. Rec_loss: 642.77 .. NELBO: 649.02\n",
      "Epoch: 1 .. batch: 636/900 .. LR: 0.005 .. KL_theta: 6.26 .. Rec_loss: 642.51 .. NELBO: 648.77\n",
      "Epoch: 1 .. batch: 638/900 .. LR: 0.005 .. KL_theta: 6.26 .. Rec_loss: 642.27 .. NELBO: 648.53\n",
      "Epoch: 1 .. batch: 640/900 .. LR: 0.005 .. KL_theta: 6.26 .. Rec_loss: 642.1 .. NELBO: 648.36\n",
      "Epoch: 1 .. batch: 642/900 .. LR: 0.005 .. KL_theta: 6.26 .. Rec_loss: 642.13 .. NELBO: 648.39\n",
      "Epoch: 1 .. batch: 644/900 .. LR: 0.005 .. KL_theta: 6.26 .. Rec_loss: 642.02 .. NELBO: 648.28\n",
      "Epoch: 1 .. batch: 646/900 .. LR: 0.005 .. KL_theta: 6.27 .. Rec_loss: 641.98 .. NELBO: 648.25\n",
      "Epoch: 1 .. batch: 648/900 .. LR: 0.005 .. KL_theta: 6.27 .. Rec_loss: 641.78 .. NELBO: 648.05\n",
      "Epoch: 1 .. batch: 650/900 .. LR: 0.005 .. KL_theta: 6.27 .. Rec_loss: 641.56 .. NELBO: 647.83\n",
      "Epoch: 1 .. batch: 652/900 .. LR: 0.005 .. KL_theta: 6.27 .. Rec_loss: 641.29 .. NELBO: 647.56\n",
      "Epoch: 1 .. batch: 654/900 .. LR: 0.005 .. KL_theta: 6.28 .. Rec_loss: 641.2 .. NELBO: 647.48\n",
      "Epoch: 1 .. batch: 656/900 .. LR: 0.005 .. KL_theta: 6.28 .. Rec_loss: 641.02 .. NELBO: 647.3\n",
      "Epoch: 1 .. batch: 658/900 .. LR: 0.005 .. KL_theta: 6.28 .. Rec_loss: 640.83 .. NELBO: 647.11\n",
      "Epoch: 1 .. batch: 660/900 .. LR: 0.005 .. KL_theta: 6.28 .. Rec_loss: 640.86 .. NELBO: 647.14\n",
      "Epoch: 1 .. batch: 662/900 .. LR: 0.005 .. KL_theta: 6.29 .. Rec_loss: 640.77 .. NELBO: 647.06\n",
      "Epoch: 1 .. batch: 664/900 .. LR: 0.005 .. KL_theta: 6.29 .. Rec_loss: 640.57 .. NELBO: 646.86\n",
      "Epoch: 1 .. batch: 666/900 .. LR: 0.005 .. KL_theta: 6.29 .. Rec_loss: 640.46 .. NELBO: 646.75\n",
      "Epoch: 1 .. batch: 668/900 .. LR: 0.005 .. KL_theta: 6.3 .. Rec_loss: 640.45 .. NELBO: 646.75\n",
      "Epoch: 1 .. batch: 670/900 .. LR: 0.005 .. KL_theta: 6.3 .. Rec_loss: 640.27 .. NELBO: 646.57\n",
      "Epoch: 1 .. batch: 672/900 .. LR: 0.005 .. KL_theta: 6.3 .. Rec_loss: 640.08 .. NELBO: 646.38\n",
      "Epoch: 1 .. batch: 674/900 .. LR: 0.005 .. KL_theta: 6.3 .. Rec_loss: 639.85 .. NELBO: 646.15\n",
      "Epoch: 1 .. batch: 676/900 .. LR: 0.005 .. KL_theta: 6.31 .. Rec_loss: 639.75 .. NELBO: 646.06\n",
      "Epoch: 1 .. batch: 678/900 .. LR: 0.005 .. KL_theta: 6.31 .. Rec_loss: 639.68 .. NELBO: 645.99\n",
      "Epoch: 1 .. batch: 680/900 .. LR: 0.005 .. KL_theta: 6.31 .. Rec_loss: 639.64 .. NELBO: 645.95\n",
      "Epoch: 1 .. batch: 682/900 .. LR: 0.005 .. KL_theta: 6.31 .. Rec_loss: 639.57 .. NELBO: 645.88\n",
      "Epoch: 1 .. batch: 684/900 .. LR: 0.005 .. KL_theta: 6.32 .. Rec_loss: 639.46 .. NELBO: 645.78\n",
      "Epoch: 1 .. batch: 686/900 .. LR: 0.005 .. KL_theta: 6.32 .. Rec_loss: 639.35 .. NELBO: 645.67\n",
      "Epoch: 1 .. batch: 688/900 .. LR: 0.005 .. KL_theta: 6.32 .. Rec_loss: 639.18 .. NELBO: 645.5\n",
      "Epoch: 1 .. batch: 690/900 .. LR: 0.005 .. KL_theta: 6.33 .. Rec_loss: 639.07 .. NELBO: 645.4\n",
      "Epoch: 1 .. batch: 692/900 .. LR: 0.005 .. KL_theta: 6.33 .. Rec_loss: 638.82 .. NELBO: 645.15\n",
      "Epoch: 1 .. batch: 694/900 .. LR: 0.005 .. KL_theta: 6.33 .. Rec_loss: 638.75 .. NELBO: 645.08\n",
      "Epoch: 1 .. batch: 696/900 .. LR: 0.005 .. KL_theta: 6.33 .. Rec_loss: 638.61 .. NELBO: 644.94\n",
      "Epoch: 1 .. batch: 698/900 .. LR: 0.005 .. KL_theta: 6.34 .. Rec_loss: 638.4 .. NELBO: 644.74\n",
      "Epoch: 1 .. batch: 700/900 .. LR: 0.005 .. KL_theta: 6.34 .. Rec_loss: 638.24 .. NELBO: 644.58\n",
      "Epoch: 1 .. batch: 702/900 .. LR: 0.005 .. KL_theta: 6.34 .. Rec_loss: 638.09 .. NELBO: 644.43\n",
      "Epoch: 1 .. batch: 704/900 .. LR: 0.005 .. KL_theta: 6.34 .. Rec_loss: 637.96 .. NELBO: 644.3\n",
      "Epoch: 1 .. batch: 706/900 .. LR: 0.005 .. KL_theta: 6.35 .. Rec_loss: 637.85 .. NELBO: 644.2\n",
      "Epoch: 1 .. batch: 708/900 .. LR: 0.005 .. KL_theta: 6.35 .. Rec_loss: 637.69 .. NELBO: 644.04\n",
      "Epoch: 1 .. batch: 710/900 .. LR: 0.005 .. KL_theta: 6.35 .. Rec_loss: 637.51 .. NELBO: 643.86\n",
      "Epoch: 1 .. batch: 712/900 .. LR: 0.005 .. KL_theta: 6.36 .. Rec_loss: 637.48 .. NELBO: 643.84\n",
      "Epoch: 1 .. batch: 714/900 .. LR: 0.005 .. KL_theta: 6.36 .. Rec_loss: 637.38 .. NELBO: 643.74\n",
      "Epoch: 1 .. batch: 716/900 .. LR: 0.005 .. KL_theta: 6.36 .. Rec_loss: 637.29 .. NELBO: 643.65\n",
      "Epoch: 1 .. batch: 718/900 .. LR: 0.005 .. KL_theta: 6.36 .. Rec_loss: 637.2 .. NELBO: 643.56\n",
      "Epoch: 1 .. batch: 720/900 .. LR: 0.005 .. KL_theta: 6.37 .. Rec_loss: 637.02 .. NELBO: 643.39\n",
      "Epoch: 1 .. batch: 722/900 .. LR: 0.005 .. KL_theta: 6.37 .. Rec_loss: 636.87 .. NELBO: 643.24\n",
      "Epoch: 1 .. batch: 724/900 .. LR: 0.005 .. KL_theta: 6.37 .. Rec_loss: 636.77 .. NELBO: 643.14\n",
      "Epoch: 1 .. batch: 726/900 .. LR: 0.005 .. KL_theta: 6.37 .. Rec_loss: 636.6 .. NELBO: 642.97\n",
      "Epoch: 1 .. batch: 728/900 .. LR: 0.005 .. KL_theta: 6.37 .. Rec_loss: 636.45 .. NELBO: 642.82\n",
      "Epoch: 1 .. batch: 730/900 .. LR: 0.005 .. KL_theta: 6.38 .. Rec_loss: 636.28 .. NELBO: 642.66\n",
      "Epoch: 1 .. batch: 732/900 .. LR: 0.005 .. KL_theta: 6.38 .. Rec_loss: 636.19 .. NELBO: 642.57\n",
      "Epoch: 1 .. batch: 734/900 .. LR: 0.005 .. KL_theta: 6.38 .. Rec_loss: 635.95 .. NELBO: 642.33\n",
      "Epoch: 1 .. batch: 736/900 .. LR: 0.005 .. KL_theta: 6.38 .. Rec_loss: 635.69 .. NELBO: 642.07\n",
      "Epoch: 1 .. batch: 738/900 .. LR: 0.005 .. KL_theta: 6.39 .. Rec_loss: 635.61 .. NELBO: 642.0\n",
      "Epoch: 1 .. batch: 740/900 .. LR: 0.005 .. KL_theta: 6.39 .. Rec_loss: 635.54 .. NELBO: 641.93\n",
      "Epoch: 1 .. batch: 742/900 .. LR: 0.005 .. KL_theta: 6.39 .. Rec_loss: 635.45 .. NELBO: 641.84\n",
      "Epoch: 1 .. batch: 744/900 .. LR: 0.005 .. KL_theta: 6.39 .. Rec_loss: 635.34 .. NELBO: 641.73\n",
      "Epoch: 1 .. batch: 746/900 .. LR: 0.005 .. KL_theta: 6.4 .. Rec_loss: 635.19 .. NELBO: 641.59\n",
      "Epoch: 1 .. batch: 748/900 .. LR: 0.005 .. KL_theta: 6.4 .. Rec_loss: 635.08 .. NELBO: 641.48\n",
      "Epoch: 1 .. batch: 750/900 .. LR: 0.005 .. KL_theta: 6.4 .. Rec_loss: 634.8 .. NELBO: 641.2\n",
      "Epoch: 1 .. batch: 752/900 .. LR: 0.005 .. KL_theta: 6.4 .. Rec_loss: 634.68 .. NELBO: 641.08\n",
      "Epoch: 1 .. batch: 754/900 .. LR: 0.005 .. KL_theta: 6.41 .. Rec_loss: 634.59 .. NELBO: 641.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 .. batch: 756/900 .. LR: 0.005 .. KL_theta: 6.41 .. Rec_loss: 634.38 .. NELBO: 640.79\n",
      "Epoch: 1 .. batch: 758/900 .. LR: 0.005 .. KL_theta: 6.41 .. Rec_loss: 634.21 .. NELBO: 640.62\n",
      "Epoch: 1 .. batch: 760/900 .. LR: 0.005 .. KL_theta: 6.41 .. Rec_loss: 634.13 .. NELBO: 640.54\n",
      "Epoch: 1 .. batch: 762/900 .. LR: 0.005 .. KL_theta: 6.41 .. Rec_loss: 634.03 .. NELBO: 640.44\n",
      "Epoch: 1 .. batch: 764/900 .. LR: 0.005 .. KL_theta: 6.42 .. Rec_loss: 633.95 .. NELBO: 640.37\n",
      "Epoch: 1 .. batch: 766/900 .. LR: 0.005 .. KL_theta: 6.42 .. Rec_loss: 633.84 .. NELBO: 640.26\n",
      "Epoch: 1 .. batch: 768/900 .. LR: 0.005 .. KL_theta: 6.42 .. Rec_loss: 633.73 .. NELBO: 640.15\n",
      "Epoch: 1 .. batch: 770/900 .. LR: 0.005 .. KL_theta: 6.42 .. Rec_loss: 633.66 .. NELBO: 640.08\n",
      "Epoch: 1 .. batch: 772/900 .. LR: 0.005 .. KL_theta: 6.43 .. Rec_loss: 633.56 .. NELBO: 639.99\n",
      "Epoch: 1 .. batch: 774/900 .. LR: 0.005 .. KL_theta: 6.43 .. Rec_loss: 633.44 .. NELBO: 639.87\n",
      "Epoch: 1 .. batch: 776/900 .. LR: 0.005 .. KL_theta: 6.43 .. Rec_loss: 633.27 .. NELBO: 639.7\n",
      "Epoch: 1 .. batch: 778/900 .. LR: 0.005 .. KL_theta: 6.43 .. Rec_loss: 633.09 .. NELBO: 639.52\n",
      "Epoch: 1 .. batch: 780/900 .. LR: 0.005 .. KL_theta: 6.44 .. Rec_loss: 632.94 .. NELBO: 639.38\n",
      "Epoch: 1 .. batch: 782/900 .. LR: 0.005 .. KL_theta: 6.44 .. Rec_loss: 632.93 .. NELBO: 639.37\n",
      "Epoch: 1 .. batch: 784/900 .. LR: 0.005 .. KL_theta: 6.44 .. Rec_loss: 632.81 .. NELBO: 639.25\n",
      "Epoch: 1 .. batch: 786/900 .. LR: 0.005 .. KL_theta: 6.45 .. Rec_loss: 632.66 .. NELBO: 639.11\n",
      "Epoch: 1 .. batch: 788/900 .. LR: 0.005 .. KL_theta: 6.45 .. Rec_loss: 632.53 .. NELBO: 638.98\n",
      "Epoch: 1 .. batch: 790/900 .. LR: 0.005 .. KL_theta: 6.45 .. Rec_loss: 632.44 .. NELBO: 638.89\n",
      "Epoch: 1 .. batch: 792/900 .. LR: 0.005 .. KL_theta: 6.45 .. Rec_loss: 632.32 .. NELBO: 638.77\n",
      "Epoch: 1 .. batch: 794/900 .. LR: 0.005 .. KL_theta: 6.46 .. Rec_loss: 632.11 .. NELBO: 638.57\n",
      "Epoch: 1 .. batch: 796/900 .. LR: 0.005 .. KL_theta: 6.46 .. Rec_loss: 631.92 .. NELBO: 638.38\n",
      "Epoch: 1 .. batch: 798/900 .. LR: 0.005 .. KL_theta: 6.46 .. Rec_loss: 631.71 .. NELBO: 638.17\n",
      "Epoch: 1 .. batch: 800/900 .. LR: 0.005 .. KL_theta: 6.46 .. Rec_loss: 631.63 .. NELBO: 638.09\n",
      "Epoch: 1 .. batch: 802/900 .. LR: 0.005 .. KL_theta: 6.47 .. Rec_loss: 631.5 .. NELBO: 637.97\n",
      "Epoch: 1 .. batch: 804/900 .. LR: 0.005 .. KL_theta: 6.47 .. Rec_loss: 631.45 .. NELBO: 637.92\n",
      "Epoch: 1 .. batch: 806/900 .. LR: 0.005 .. KL_theta: 6.47 .. Rec_loss: 631.31 .. NELBO: 637.78\n",
      "Epoch: 1 .. batch: 808/900 .. LR: 0.005 .. KL_theta: 6.47 .. Rec_loss: 631.2 .. NELBO: 637.67\n",
      "Epoch: 1 .. batch: 810/900 .. LR: 0.005 .. KL_theta: 6.48 .. Rec_loss: 631.14 .. NELBO: 637.62\n",
      "Epoch: 1 .. batch: 812/900 .. LR: 0.005 .. KL_theta: 6.48 .. Rec_loss: 631.03 .. NELBO: 637.51\n",
      "Epoch: 1 .. batch: 814/900 .. LR: 0.005 .. KL_theta: 6.48 .. Rec_loss: 630.98 .. NELBO: 637.46\n",
      "Epoch: 1 .. batch: 816/900 .. LR: 0.005 .. KL_theta: 6.48 .. Rec_loss: 630.87 .. NELBO: 637.35\n",
      "Epoch: 1 .. batch: 818/900 .. LR: 0.005 .. KL_theta: 6.49 .. Rec_loss: 630.75 .. NELBO: 637.24\n",
      "Epoch: 1 .. batch: 820/900 .. LR: 0.005 .. KL_theta: 6.49 .. Rec_loss: 630.65 .. NELBO: 637.14\n",
      "Epoch: 1 .. batch: 822/900 .. LR: 0.005 .. KL_theta: 6.49 .. Rec_loss: 630.6 .. NELBO: 637.09\n",
      "Epoch: 1 .. batch: 824/900 .. LR: 0.005 .. KL_theta: 6.49 .. Rec_loss: 630.51 .. NELBO: 637.0\n",
      "Epoch: 1 .. batch: 826/900 .. LR: 0.005 .. KL_theta: 6.5 .. Rec_loss: 630.3 .. NELBO: 636.8\n",
      "Epoch: 1 .. batch: 828/900 .. LR: 0.005 .. KL_theta: 6.5 .. Rec_loss: 630.19 .. NELBO: 636.69\n",
      "Epoch: 1 .. batch: 830/900 .. LR: 0.005 .. KL_theta: 6.5 .. Rec_loss: 630.06 .. NELBO: 636.56\n",
      "Epoch: 1 .. batch: 832/900 .. LR: 0.005 .. KL_theta: 6.5 .. Rec_loss: 629.98 .. NELBO: 636.48\n",
      "Epoch: 1 .. batch: 834/900 .. LR: 0.005 .. KL_theta: 6.5 .. Rec_loss: 629.89 .. NELBO: 636.39\n",
      "Epoch: 1 .. batch: 836/900 .. LR: 0.005 .. KL_theta: 6.51 .. Rec_loss: 629.72 .. NELBO: 636.23\n",
      "Epoch: 1 .. batch: 838/900 .. LR: 0.005 .. KL_theta: 6.51 .. Rec_loss: 629.68 .. NELBO: 636.19\n",
      "Epoch: 1 .. batch: 840/900 .. LR: 0.005 .. KL_theta: 6.51 .. Rec_loss: 629.59 .. NELBO: 636.1\n",
      "Epoch: 1 .. batch: 842/900 .. LR: 0.005 .. KL_theta: 6.52 .. Rec_loss: 629.47 .. NELBO: 635.99\n",
      "Epoch: 1 .. batch: 844/900 .. LR: 0.005 .. KL_theta: 6.52 .. Rec_loss: 629.44 .. NELBO: 635.96\n",
      "Epoch: 1 .. batch: 846/900 .. LR: 0.005 .. KL_theta: 6.52 .. Rec_loss: 629.37 .. NELBO: 635.89\n",
      "Epoch: 1 .. batch: 848/900 .. LR: 0.005 .. KL_theta: 6.53 .. Rec_loss: 629.23 .. NELBO: 635.76\n",
      "Epoch: 1 .. batch: 850/900 .. LR: 0.005 .. KL_theta: 6.53 .. Rec_loss: 629.1 .. NELBO: 635.63\n",
      "Epoch: 1 .. batch: 852/900 .. LR: 0.005 .. KL_theta: 6.53 .. Rec_loss: 629.02 .. NELBO: 635.55\n",
      "Epoch: 1 .. batch: 854/900 .. LR: 0.005 .. KL_theta: 6.53 .. Rec_loss: 628.94 .. NELBO: 635.47\n",
      "Epoch: 1 .. batch: 856/900 .. LR: 0.005 .. KL_theta: 6.54 .. Rec_loss: 628.8 .. NELBO: 635.34\n",
      "Epoch: 1 .. batch: 858/900 .. LR: 0.005 .. KL_theta: 6.54 .. Rec_loss: 628.73 .. NELBO: 635.27\n",
      "Epoch: 1 .. batch: 860/900 .. LR: 0.005 .. KL_theta: 6.54 .. Rec_loss: 628.61 .. NELBO: 635.15\n",
      "Epoch: 1 .. batch: 862/900 .. LR: 0.005 .. KL_theta: 6.54 .. Rec_loss: 628.45 .. NELBO: 634.99\n",
      "Epoch: 1 .. batch: 864/900 .. LR: 0.005 .. KL_theta: 6.54 .. Rec_loss: 628.34 .. NELBO: 634.88\n",
      "Epoch: 1 .. batch: 866/900 .. LR: 0.005 .. KL_theta: 6.55 .. Rec_loss: 628.2 .. NELBO: 634.75\n",
      "Epoch: 1 .. batch: 868/900 .. LR: 0.005 .. KL_theta: 6.55 .. Rec_loss: 628.14 .. NELBO: 634.69\n",
      "Epoch: 1 .. batch: 870/900 .. LR: 0.005 .. KL_theta: 6.55 .. Rec_loss: 628.16 .. NELBO: 634.71\n",
      "Epoch: 1 .. batch: 872/900 .. LR: 0.005 .. KL_theta: 6.56 .. Rec_loss: 628.01 .. NELBO: 634.57\n",
      "Epoch: 1 .. batch: 874/900 .. LR: 0.005 .. KL_theta: 6.56 .. Rec_loss: 627.91 .. NELBO: 634.47\n",
      "Epoch: 1 .. batch: 876/900 .. LR: 0.005 .. KL_theta: 6.56 .. Rec_loss: 627.76 .. NELBO: 634.32\n",
      "Epoch: 1 .. batch: 878/900 .. LR: 0.005 .. KL_theta: 6.56 .. Rec_loss: 627.63 .. NELBO: 634.19\n",
      "Epoch: 1 .. batch: 880/900 .. LR: 0.005 .. KL_theta: 6.57 .. Rec_loss: 627.55 .. NELBO: 634.12\n",
      "Epoch: 1 .. batch: 882/900 .. LR: 0.005 .. KL_theta: 6.57 .. Rec_loss: 627.33 .. NELBO: 633.9\n",
      "Epoch: 1 .. batch: 884/900 .. LR: 0.005 .. KL_theta: 6.57 .. Rec_loss: 627.19 .. NELBO: 633.76\n",
      "Epoch: 1 .. batch: 886/900 .. LR: 0.005 .. KL_theta: 6.57 .. Rec_loss: 626.98 .. NELBO: 633.55\n",
      "Epoch: 1 .. batch: 888/900 .. LR: 0.005 .. KL_theta: 6.57 .. Rec_loss: 626.9 .. NELBO: 633.47\n",
      "Epoch: 1 .. batch: 890/900 .. LR: 0.005 .. KL_theta: 6.58 .. Rec_loss: 626.72 .. NELBO: 633.3\n",
      "Epoch: 1 .. batch: 892/900 .. LR: 0.005 .. KL_theta: 6.58 .. Rec_loss: 626.57 .. NELBO: 633.15\n",
      "Epoch: 1 .. batch: 894/900 .. LR: 0.005 .. KL_theta: 6.58 .. Rec_loss: 626.47 .. NELBO: 633.05\n",
      "Epoch: 1 .. batch: 896/900 .. LR: 0.005 .. KL_theta: 6.58 .. Rec_loss: 626.34 .. NELBO: 632.92\n",
      "Epoch: 1 .. batch: 898/900 .. LR: 0.005 .. KL_theta: 6.59 .. Rec_loss: 626.31 .. NELBO: 632.9\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 6.59 .. Rec_loss: 626.26 .. NELBO: 632.85\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 2078.4\n",
      "****************************************************************************************************\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0: ['had', 'day', 'days', 'before', 'weeks', 'two', 'since', 'medicine', 'year']\n",
      "Topic 1: ['an', 'water', 'blood', 'second', 'mg', 'drink', 'mouth', 'b', 'pressure']\n",
      "Topic 2: ['on', 'take', 'mg', 'over', 'works', 'every', 'work', 'again', 'prescribed']\n",
      "Topic 3: ['been', 'after', 'im', 'about', 'first', 'months', 'years', 'out', 'get']\n",
      "Topic 4: ['i', 'necessary', 'ergotomine', 'swingangeraggressiondepression', 'bouncing', 'worsetoday', 'limbohave', 'halfgram', 'dizzywhats']\n",
      "Topic 5: ['all', 'or', 'pain', 'very', 'really', 'also', 'little', 'symptoms', 'bad']\n",
      "Topic 6: ['not', 'as', 'if', 'you', 'would', 'will', 'because', 'can', 'do']\n",
      "Topic 7: ['so', 'side', 'no', 'effects', 'like', 'up', 'which', 'an', 'weight']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 8: ['the', 'your', 'and', 'me', 'ago', 'in', 'figure', 'vancomycin', 'at']\n",
      "Topic 9: ['now', 'ive', 'has', 'only', 'from', 'also', 'ever', 'skin', 'thing']\n",
      "Topic 10: ['with', 'having', 'lot', 'without', 'through', 'diagnosed', 'issues', 'problems', 'caused']\n",
      "Topic 11: ['just', 'one', 'are', 'doctor', 'went', 'they', 'control', 'tried', 'going']\n",
      "Topic 12: ['started', 'when', 'more', 'anxiety', 'never', 'much', 'off', 'feel', 'better']\n",
      "Topic 13: ['this', 'is', 'am', 'medication', 'drug', 'cant', 'know', 'how', 'product']\n",
      "Topic 14: ['was', 'ativan', 'viibryd', 'vyvanse', 'lamictal', 'badly', 'killed', 'regimen', 'fire']\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: skin .. neighbors: ['skin', 'red', 'forehead', 'ankles', 'oily', 'broken', 'blotches', 'looks', 'clearer', 'grown', 'rod', 'itchy', 'clear', 'kid', 'rashes', 'uterus', 'breakouts', 'peel', 'irritation', 'turns']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: cycle .. neighbors: ['cycle', 'breakthrough', 'implanon', 'cheek', 'thrush', 'teens', 'occur', 'milk', 'boobs', 'bloody', 'cystic', 'infusion', 'stages', 'foot', 'pocket', 'chin', 'grown', 'growth', 'appointment', 'cleanser']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: effects .. neighbors: ['effects', 'side', 'sedative', 'adverse', 'besides', 'affects', 'hangover', 'lexapro', 'sexual', 'whatsoever', 'ridiculously', 'bothered', 'debilitating', 'benefit', 'reasons', 'opioid', 'wishes', 'hairs', 'thus', 'surprised']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: price .. neighbors: ['price', 'expensive', 'palpitations', 'orgasm', 'wash', 'paste', 'uric', 'performance', 'paying', 'scary', 'stressing', 'costs', 'promise', 'lately', 'burns', 'admit', 'bullet', 'heals', 'voices', 'eliminate']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: worst .. neighbors: ['worst', 'part', 'orsythia', 'incredibly', 'beating', 'best', 'anyways', 'decision', 'ring', 'chest', 'w', 'killing', 'stronger', 'heartburn', 'purchased', 'quality', 'cycle', 'drastically', 'government', 'form']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: best .. neighbors: ['best', 'favorite', 'solution', 'purchased', 'chill', 'worst', 'phenomenal', 'fewer', 'business', 'heard', 'linked', 'excellent', 'idea', 'father', 'admit', 'approved', 'key', 'bullet', 'ability', 'overall']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: efficacy .. neighbors: ['efficacy', 'sexapril', 'helplessnessdepression', 'dischargesomething', 'awaking', 'spiderlike', 'anomalies', 'axe', 'spasmsit', 'arthritisquot', 'halffinished', 'dryheaving', 'retardant', 'antiinflammatoryyou', 'monthsonly', 'dontthe', 'thirsty', 'trinza', 'birththe', 'apomodafinil']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: performance .. neighbors: ['performance', 'rapidly', 'concentration', 'wishes', 'horribly', 'moved', 'shaky', 'bones', 'flashes', 'feelings', 'appealing', 'intestinal', 'voices', 'fade', 'disorientation', 'muscles', 'us', 'somewhat', 'price', 'backing']\n",
      "vectors:  (78182, 300)\n",
      "query:  (300,)\n",
      "word: cancer .. neighbors: ['cancer', 'fentanyl', 'behind', 'degenerative', 'disk', 'bloody', 'endometriosis', 'spasms', 'colon', 'tumor', 'intermittent', 'student', 'rheumatologist', 'cheek', 'treating', 'disc', 'landed', 'cord', 'generalized', 'racing']\n",
      "####################################################################################################\n",
      "Epoch: 2 .. batch: 2/900 .. LR: 0.005 .. KL_theta: 8.15 .. Rec_loss: 626.77 .. NELBO: 634.92\n",
      "Epoch: 2 .. batch: 4/900 .. LR: 0.005 .. KL_theta: 7.98 .. Rec_loss: 615.0 .. NELBO: 622.98\n",
      "Epoch: 2 .. batch: 6/900 .. LR: 0.005 .. KL_theta: 7.91 .. Rec_loss: 603.25 .. NELBO: 611.16\n",
      "Epoch: 2 .. batch: 8/900 .. LR: 0.005 .. KL_theta: 7.85 .. Rec_loss: 606.36 .. NELBO: 614.21\n",
      "Epoch: 2 .. batch: 10/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 602.97 .. NELBO: 610.83\n",
      "Epoch: 2 .. batch: 12/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 602.51 .. NELBO: 610.39\n",
      "Epoch: 2 .. batch: 14/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 601.61 .. NELBO: 609.48\n",
      "Epoch: 2 .. batch: 16/900 .. LR: 0.005 .. KL_theta: 7.83 .. Rec_loss: 600.9 .. NELBO: 608.73\n",
      "Epoch: 2 .. batch: 18/900 .. LR: 0.005 .. KL_theta: 7.83 .. Rec_loss: 598.58 .. NELBO: 606.41\n",
      "Epoch: 2 .. batch: 20/900 .. LR: 0.005 .. KL_theta: 7.8 .. Rec_loss: 596.51 .. NELBO: 604.31\n",
      "Epoch: 2 .. batch: 22/900 .. LR: 0.005 .. KL_theta: 7.82 .. Rec_loss: 597.75 .. NELBO: 605.57\n",
      "Epoch: 2 .. batch: 24/900 .. LR: 0.005 .. KL_theta: 7.83 .. Rec_loss: 598.74 .. NELBO: 606.57\n",
      "Epoch: 2 .. batch: 26/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 600.44 .. NELBO: 608.28\n",
      "Epoch: 2 .. batch: 28/900 .. LR: 0.005 .. KL_theta: 7.83 .. Rec_loss: 599.66 .. NELBO: 607.49\n",
      "Epoch: 2 .. batch: 30/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 598.47 .. NELBO: 606.33\n",
      "Epoch: 2 .. batch: 32/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 596.99 .. NELBO: 604.86\n",
      "Epoch: 2 .. batch: 34/900 .. LR: 0.005 .. KL_theta: 7.82 .. Rec_loss: 597.99 .. NELBO: 605.81\n",
      "Epoch: 2 .. batch: 36/900 .. LR: 0.005 .. KL_theta: 7.81 .. Rec_loss: 597.6 .. NELBO: 605.41\n",
      "Epoch: 2 .. batch: 38/900 .. LR: 0.005 .. KL_theta: 7.82 .. Rec_loss: 596.3 .. NELBO: 604.12\n",
      "Epoch: 2 .. batch: 40/900 .. LR: 0.005 .. KL_theta: 7.82 .. Rec_loss: 594.86 .. NELBO: 602.68\n",
      "Epoch: 2 .. batch: 42/900 .. LR: 0.005 .. KL_theta: 7.8 .. Rec_loss: 594.88 .. NELBO: 602.68\n",
      "Epoch: 2 .. batch: 44/900 .. LR: 0.005 .. KL_theta: 7.8 .. Rec_loss: 593.33 .. NELBO: 601.13\n",
      "Epoch: 2 .. batch: 46/900 .. LR: 0.005 .. KL_theta: 7.8 .. Rec_loss: 591.67 .. NELBO: 599.47\n",
      "Epoch: 2 .. batch: 48/900 .. LR: 0.005 .. KL_theta: 7.81 .. Rec_loss: 591.49 .. NELBO: 599.3\n",
      "Epoch: 2 .. batch: 50/900 .. LR: 0.005 .. KL_theta: 7.8 .. Rec_loss: 590.97 .. NELBO: 598.77\n",
      "Epoch: 2 .. batch: 52/900 .. LR: 0.005 .. KL_theta: 7.79 .. Rec_loss: 591.52 .. NELBO: 599.31\n",
      "Epoch: 2 .. batch: 54/900 .. LR: 0.005 .. KL_theta: 7.79 .. Rec_loss: 591.31 .. NELBO: 599.1\n",
      "Epoch: 2 .. batch: 56/900 .. LR: 0.005 .. KL_theta: 7.79 .. Rec_loss: 589.37 .. NELBO: 597.16\n",
      "Epoch: 2 .. batch: 58/900 .. LR: 0.005 .. KL_theta: 7.82 .. Rec_loss: 589.42 .. NELBO: 597.24\n",
      "Epoch: 2 .. batch: 60/900 .. LR: 0.005 .. KL_theta: 7.81 .. Rec_loss: 589.48 .. NELBO: 597.29\n",
      "Epoch: 2 .. batch: 62/900 .. LR: 0.005 .. KL_theta: 7.82 .. Rec_loss: 590.52 .. NELBO: 598.34\n",
      "Epoch: 2 .. batch: 64/900 .. LR: 0.005 .. KL_theta: 7.81 .. Rec_loss: 590.1 .. NELBO: 597.91\n",
      "Epoch: 2 .. batch: 66/900 .. LR: 0.005 .. KL_theta: 7.79 .. Rec_loss: 590.29 .. NELBO: 598.08\n",
      "Epoch: 2 .. batch: 68/900 .. LR: 0.005 .. KL_theta: 7.8 .. Rec_loss: 591.19 .. NELBO: 598.99\n",
      "Epoch: 2 .. batch: 70/900 .. LR: 0.005 .. KL_theta: 7.82 .. Rec_loss: 590.9 .. NELBO: 598.72\n",
      "Epoch: 2 .. batch: 72/900 .. LR: 0.005 .. KL_theta: 7.82 .. Rec_loss: 589.62 .. NELBO: 597.44\n",
      "Epoch: 2 .. batch: 74/900 .. LR: 0.005 .. KL_theta: 7.81 .. Rec_loss: 589.74 .. NELBO: 597.55\n",
      "Epoch: 2 .. batch: 76/900 .. LR: 0.005 .. KL_theta: 7.8 .. Rec_loss: 589.17 .. NELBO: 596.97\n",
      "Epoch: 2 .. batch: 78/900 .. LR: 0.005 .. KL_theta: 7.8 .. Rec_loss: 588.74 .. NELBO: 596.54\n",
      "Epoch: 2 .. batch: 80/900 .. LR: 0.005 .. KL_theta: 7.81 .. Rec_loss: 589.15 .. NELBO: 596.96\n",
      "Epoch: 2 .. batch: 82/900 .. LR: 0.005 .. KL_theta: 7.82 .. Rec_loss: 589.65 .. NELBO: 597.47\n",
      "Epoch: 2 .. batch: 84/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 589.43 .. NELBO: 597.27\n",
      "Epoch: 2 .. batch: 86/900 .. LR: 0.005 .. KL_theta: 7.83 .. Rec_loss: 588.76 .. NELBO: 596.59\n",
      "Epoch: 2 .. batch: 88/900 .. LR: 0.005 .. KL_theta: 7.83 .. Rec_loss: 589.86 .. NELBO: 597.69\n",
      "Epoch: 2 .. batch: 90/900 .. LR: 0.005 .. KL_theta: 7.82 .. Rec_loss: 589.65 .. NELBO: 597.47\n",
      "Epoch: 2 .. batch: 92/900 .. LR: 0.005 .. KL_theta: 7.83 .. Rec_loss: 590.34 .. NELBO: 598.17\n",
      "Epoch: 2 .. batch: 94/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 589.48 .. NELBO: 597.32\n",
      "Epoch: 2 .. batch: 96/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 589.42 .. NELBO: 597.26\n",
      "Epoch: 2 .. batch: 98/900 .. LR: 0.005 .. KL_theta: 7.83 .. Rec_loss: 589.18 .. NELBO: 597.01\n",
      "Epoch: 2 .. batch: 100/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 589.36 .. NELBO: 597.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 .. batch: 102/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 589.8 .. NELBO: 597.64\n",
      "Epoch: 2 .. batch: 104/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 588.68 .. NELBO: 596.52\n",
      "Epoch: 2 .. batch: 106/900 .. LR: 0.005 .. KL_theta: 7.82 .. Rec_loss: 588.47 .. NELBO: 596.29\n",
      "Epoch: 2 .. batch: 108/900 .. LR: 0.005 .. KL_theta: 7.83 .. Rec_loss: 587.74 .. NELBO: 595.57\n",
      "Epoch: 2 .. batch: 110/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 587.87 .. NELBO: 595.71\n",
      "Epoch: 2 .. batch: 112/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 588.0 .. NELBO: 595.84\n",
      "Epoch: 2 .. batch: 114/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 587.62 .. NELBO: 595.46\n",
      "Epoch: 2 .. batch: 116/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 588.08 .. NELBO: 595.92\n",
      "Epoch: 2 .. batch: 118/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 587.6 .. NELBO: 595.44\n",
      "Epoch: 2 .. batch: 120/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 587.75 .. NELBO: 595.59\n",
      "Epoch: 2 .. batch: 122/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 587.76 .. NELBO: 595.6\n",
      "Epoch: 2 .. batch: 124/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 587.5 .. NELBO: 595.34\n",
      "Epoch: 2 .. batch: 126/900 .. LR: 0.005 .. KL_theta: 7.85 .. Rec_loss: 587.74 .. NELBO: 595.59\n",
      "Epoch: 2 .. batch: 128/900 .. LR: 0.005 .. KL_theta: 7.85 .. Rec_loss: 587.44 .. NELBO: 595.29\n",
      "Epoch: 2 .. batch: 130/900 .. LR: 0.005 .. KL_theta: 7.85 .. Rec_loss: 587.82 .. NELBO: 595.67\n",
      "Epoch: 2 .. batch: 132/900 .. LR: 0.005 .. KL_theta: 7.85 .. Rec_loss: 587.4 .. NELBO: 595.25\n",
      "Epoch: 2 .. batch: 134/900 .. LR: 0.005 .. KL_theta: 7.85 .. Rec_loss: 586.99 .. NELBO: 594.84\n",
      "Epoch: 2 .. batch: 136/900 .. LR: 0.005 .. KL_theta: 7.85 .. Rec_loss: 586.93 .. NELBO: 594.78\n",
      "Epoch: 2 .. batch: 138/900 .. LR: 0.005 .. KL_theta: 7.85 .. Rec_loss: 586.38 .. NELBO: 594.23\n",
      "Epoch: 2 .. batch: 140/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 585.75 .. NELBO: 593.59\n",
      "Epoch: 2 .. batch: 142/900 .. LR: 0.005 .. KL_theta: 7.83 .. Rec_loss: 585.19 .. NELBO: 593.02\n",
      "Epoch: 2 .. batch: 144/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 584.99 .. NELBO: 592.83\n",
      "Epoch: 2 .. batch: 146/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 585.19 .. NELBO: 593.03\n",
      "Epoch: 2 .. batch: 148/900 .. LR: 0.005 .. KL_theta: 7.83 .. Rec_loss: 584.7 .. NELBO: 592.53\n",
      "Epoch: 2 .. batch: 150/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 584.87 .. NELBO: 592.71\n",
      "Epoch: 2 .. batch: 152/900 .. LR: 0.005 .. KL_theta: 7.85 .. Rec_loss: 585.19 .. NELBO: 593.04\n",
      "Epoch: 2 .. batch: 154/900 .. LR: 0.005 .. KL_theta: 7.84 .. Rec_loss: 585.07 .. NELBO: 592.91\n",
      "Epoch: 2 .. batch: 156/900 .. LR: 0.005 .. KL_theta: 7.85 .. Rec_loss: 585.12 .. NELBO: 592.97\n",
      "Epoch: 2 .. batch: 158/900 .. LR: 0.005 .. KL_theta: 7.85 .. Rec_loss: 584.95 .. NELBO: 592.8\n",
      "Epoch: 2 .. batch: 160/900 .. LR: 0.005 .. KL_theta: 7.85 .. Rec_loss: 584.62 .. NELBO: 592.47\n",
      "Epoch: 2 .. batch: 162/900 .. LR: 0.005 .. KL_theta: 7.85 .. Rec_loss: 584.78 .. NELBO: 592.63\n",
      "Epoch: 2 .. batch: 164/900 .. LR: 0.005 .. KL_theta: 7.85 .. Rec_loss: 584.88 .. NELBO: 592.73\n",
      "Epoch: 2 .. batch: 166/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 584.98 .. NELBO: 592.84\n",
      "Epoch: 2 .. batch: 168/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 584.84 .. NELBO: 592.7\n",
      "Epoch: 2 .. batch: 170/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 584.87 .. NELBO: 592.73\n",
      "Epoch: 2 .. batch: 172/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 584.97 .. NELBO: 592.83\n",
      "Epoch: 2 .. batch: 174/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 585.22 .. NELBO: 593.08\n",
      "Epoch: 2 .. batch: 176/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 584.77 .. NELBO: 592.63\n",
      "Epoch: 2 .. batch: 178/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 584.64 .. NELBO: 592.51\n",
      "Epoch: 2 .. batch: 180/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 584.52 .. NELBO: 592.39\n",
      "Epoch: 2 .. batch: 182/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 584.41 .. NELBO: 592.28\n",
      "Epoch: 2 .. batch: 184/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 584.57 .. NELBO: 592.45\n",
      "Epoch: 2 .. batch: 186/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 584.3 .. NELBO: 592.17\n",
      "Epoch: 2 .. batch: 188/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 583.65 .. NELBO: 591.52\n",
      "Epoch: 2 .. batch: 190/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 583.54 .. NELBO: 591.41\n",
      "Epoch: 2 .. batch: 192/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 583.42 .. NELBO: 591.29\n",
      "Epoch: 2 .. batch: 194/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 583.25 .. NELBO: 591.12\n",
      "Epoch: 2 .. batch: 196/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 583.1 .. NELBO: 590.97\n",
      "Epoch: 2 .. batch: 198/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 582.98 .. NELBO: 590.85\n",
      "Epoch: 2 .. batch: 200/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 582.69 .. NELBO: 590.56\n",
      "Epoch: 2 .. batch: 202/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 582.22 .. NELBO: 590.08\n",
      "Epoch: 2 .. batch: 204/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 581.9 .. NELBO: 589.76\n",
      "Epoch: 2 .. batch: 206/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 582.15 .. NELBO: 590.02\n",
      "Epoch: 2 .. batch: 208/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 582.1 .. NELBO: 589.97\n",
      "Epoch: 2 .. batch: 210/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 582.17 .. NELBO: 590.04\n",
      "Epoch: 2 .. batch: 212/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 582.05 .. NELBO: 589.92\n",
      "Epoch: 2 .. batch: 214/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 581.71 .. NELBO: 589.58\n",
      "Epoch: 2 .. batch: 216/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 581.52 .. NELBO: 589.38\n",
      "Epoch: 2 .. batch: 218/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 581.53 .. NELBO: 589.39\n",
      "Epoch: 2 .. batch: 220/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 581.36 .. NELBO: 589.23\n",
      "Epoch: 2 .. batch: 222/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 581.21 .. NELBO: 589.08\n",
      "Epoch: 2 .. batch: 224/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 581.12 .. NELBO: 588.99\n",
      "Epoch: 2 .. batch: 226/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 580.99 .. NELBO: 588.86\n",
      "Epoch: 2 .. batch: 228/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 580.97 .. NELBO: 588.85\n",
      "Epoch: 2 .. batch: 230/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 580.56 .. NELBO: 588.43\n",
      "Epoch: 2 .. batch: 232/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 580.67 .. NELBO: 588.53\n",
      "Epoch: 2 .. batch: 234/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 580.56 .. NELBO: 588.43\n",
      "Epoch: 2 .. batch: 236/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 580.64 .. NELBO: 588.51\n",
      "Epoch: 2 .. batch: 238/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 580.36 .. NELBO: 588.23\n",
      "Epoch: 2 .. batch: 240/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 580.44 .. NELBO: 588.3\n",
      "Epoch: 2 .. batch: 242/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 580.53 .. NELBO: 588.39\n",
      "Epoch: 2 .. batch: 244/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 580.27 .. NELBO: 588.13\n",
      "Epoch: 2 .. batch: 246/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 580.5 .. NELBO: 588.36\n",
      "Epoch: 2 .. batch: 248/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 580.47 .. NELBO: 588.34\n",
      "Epoch: 2 .. batch: 250/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 580.54 .. NELBO: 588.41\n",
      "Epoch: 2 .. batch: 252/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 580.51 .. NELBO: 588.38\n",
      "Epoch: 2 .. batch: 254/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 580.42 .. NELBO: 588.29\n",
      "Epoch: 2 .. batch: 256/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 580.53 .. NELBO: 588.39\n",
      "Epoch: 2 .. batch: 258/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 580.23 .. NELBO: 588.09\n",
      "Epoch: 2 .. batch: 260/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 579.79 .. NELBO: 587.65\n",
      "Epoch: 2 .. batch: 262/900 .. LR: 0.005 .. KL_theta: 7.86 .. Rec_loss: 579.99 .. NELBO: 587.85\n",
      "Epoch: 2 .. batch: 264/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 580.07 .. NELBO: 587.94\n",
      "Epoch: 2 .. batch: 266/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 579.87 .. NELBO: 587.74\n",
      "Epoch: 2 .. batch: 268/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 579.83 .. NELBO: 587.7\n",
      "Epoch: 2 .. batch: 270/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 579.92 .. NELBO: 587.79\n",
      "Epoch: 2 .. batch: 272/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 579.92 .. NELBO: 587.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 .. batch: 274/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 580.09 .. NELBO: 587.96\n",
      "Epoch: 2 .. batch: 276/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 579.94 .. NELBO: 587.81\n",
      "Epoch: 2 .. batch: 278/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 580.07 .. NELBO: 587.94\n",
      "Epoch: 2 .. batch: 280/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 580.32 .. NELBO: 588.2\n",
      "Epoch: 2 .. batch: 282/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 580.06 .. NELBO: 587.94\n",
      "Epoch: 2 .. batch: 284/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 580.15 .. NELBO: 588.03\n",
      "Epoch: 2 .. batch: 286/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 580.21 .. NELBO: 588.09\n",
      "Epoch: 2 .. batch: 288/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 580.23 .. NELBO: 588.11\n",
      "Epoch: 2 .. batch: 290/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 580.21 .. NELBO: 588.09\n",
      "Epoch: 2 .. batch: 292/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 580.15 .. NELBO: 588.03\n",
      "Epoch: 2 .. batch: 294/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 579.95 .. NELBO: 587.83\n",
      "Epoch: 2 .. batch: 296/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 579.72 .. NELBO: 587.6\n",
      "Epoch: 2 .. batch: 298/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 579.46 .. NELBO: 587.33\n",
      "Epoch: 2 .. batch: 300/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 579.48 .. NELBO: 587.35\n",
      "Epoch: 2 .. batch: 302/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 579.29 .. NELBO: 587.16\n",
      "Epoch: 2 .. batch: 304/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 579.21 .. NELBO: 587.08\n",
      "Epoch: 2 .. batch: 306/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 579.12 .. NELBO: 587.0\n",
      "Epoch: 2 .. batch: 308/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 579.49 .. NELBO: 587.37\n",
      "Epoch: 2 .. batch: 310/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 579.3 .. NELBO: 587.18\n",
      "Epoch: 2 .. batch: 312/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 579.27 .. NELBO: 587.15\n",
      "Epoch: 2 .. batch: 314/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 579.29 .. NELBO: 587.16\n",
      "Epoch: 2 .. batch: 316/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 579.27 .. NELBO: 587.15\n",
      "Epoch: 2 .. batch: 318/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 579.0 .. NELBO: 586.88\n",
      "Epoch: 2 .. batch: 320/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 578.87 .. NELBO: 586.75\n",
      "Epoch: 2 .. batch: 322/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 578.82 .. NELBO: 586.69\n",
      "Epoch: 2 .. batch: 324/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 578.76 .. NELBO: 586.63\n",
      "Epoch: 2 .. batch: 326/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 578.69 .. NELBO: 586.56\n",
      "Epoch: 2 .. batch: 328/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 578.42 .. NELBO: 586.3\n",
      "Epoch: 2 .. batch: 330/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 578.28 .. NELBO: 586.16\n",
      "Epoch: 2 .. batch: 332/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 578.35 .. NELBO: 586.22\n",
      "Epoch: 2 .. batch: 334/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 578.42 .. NELBO: 586.29\n",
      "Epoch: 2 .. batch: 336/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 578.36 .. NELBO: 586.24\n",
      "Epoch: 2 .. batch: 338/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 578.04 .. NELBO: 585.92\n",
      "Epoch: 2 .. batch: 340/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 577.99 .. NELBO: 585.86\n",
      "Epoch: 2 .. batch: 342/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.93 .. NELBO: 585.81\n",
      "Epoch: 2 .. batch: 344/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.96 .. NELBO: 585.84\n",
      "Epoch: 2 .. batch: 346/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.74 .. NELBO: 585.62\n",
      "Epoch: 2 .. batch: 348/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.69 .. NELBO: 585.57\n",
      "Epoch: 2 .. batch: 350/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.63 .. NELBO: 585.51\n",
      "Epoch: 2 .. batch: 352/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.68 .. NELBO: 585.56\n",
      "Epoch: 2 .. batch: 354/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.62 .. NELBO: 585.5\n",
      "Epoch: 2 .. batch: 356/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.43 .. NELBO: 585.31\n",
      "Epoch: 2 .. batch: 358/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.19 .. NELBO: 585.07\n",
      "Epoch: 2 .. batch: 360/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.34 .. NELBO: 585.22\n",
      "Epoch: 2 .. batch: 362/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.46 .. NELBO: 585.34\n",
      "Epoch: 2 .. batch: 364/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.46 .. NELBO: 585.34\n",
      "Epoch: 2 .. batch: 366/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.46 .. NELBO: 585.34\n",
      "Epoch: 2 .. batch: 368/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.36 .. NELBO: 585.24\n",
      "Epoch: 2 .. batch: 370/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.21 .. NELBO: 585.09\n",
      "Epoch: 2 .. batch: 372/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.25 .. NELBO: 585.13\n",
      "Epoch: 2 .. batch: 374/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.34 .. NELBO: 585.22\n",
      "Epoch: 2 .. batch: 376/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.52 .. NELBO: 585.4\n",
      "Epoch: 2 .. batch: 378/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.42 .. NELBO: 585.3\n",
      "Epoch: 2 .. batch: 380/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.28 .. NELBO: 585.16\n",
      "Epoch: 2 .. batch: 382/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.3 .. NELBO: 585.18\n",
      "Epoch: 2 .. batch: 384/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 577.03 .. NELBO: 584.91\n",
      "Epoch: 2 .. batch: 386/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 576.98 .. NELBO: 584.86\n",
      "Epoch: 2 .. batch: 388/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 576.95 .. NELBO: 584.83\n",
      "Epoch: 2 .. batch: 390/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 576.8 .. NELBO: 584.68\n",
      "Epoch: 2 .. batch: 392/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 576.78 .. NELBO: 584.66\n",
      "Epoch: 2 .. batch: 394/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 576.63 .. NELBO: 584.51\n",
      "Epoch: 2 .. batch: 396/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 576.64 .. NELBO: 584.51\n",
      "Epoch: 2 .. batch: 398/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 576.64 .. NELBO: 584.51\n",
      "Epoch: 2 .. batch: 400/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 576.57 .. NELBO: 584.45\n",
      "Epoch: 2 .. batch: 402/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 576.36 .. NELBO: 584.24\n",
      "Epoch: 2 .. batch: 404/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 576.18 .. NELBO: 584.05\n",
      "Epoch: 2 .. batch: 406/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 576.07 .. NELBO: 583.94\n",
      "Epoch: 2 .. batch: 408/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 576.08 .. NELBO: 583.95\n",
      "Epoch: 2 .. batch: 410/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.98 .. NELBO: 583.85\n",
      "Epoch: 2 .. batch: 412/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.84 .. NELBO: 583.71\n",
      "Epoch: 2 .. batch: 414/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.81 .. NELBO: 583.68\n",
      "Epoch: 2 .. batch: 416/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.73 .. NELBO: 583.6\n",
      "Epoch: 2 .. batch: 418/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.8 .. NELBO: 583.67\n",
      "Epoch: 2 .. batch: 420/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.87 .. NELBO: 583.74\n",
      "Epoch: 2 .. batch: 422/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.8 .. NELBO: 583.67\n",
      "Epoch: 2 .. batch: 424/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.79 .. NELBO: 583.66\n",
      "Epoch: 2 .. batch: 426/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.78 .. NELBO: 583.66\n",
      "Epoch: 2 .. batch: 428/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.63 .. NELBO: 583.5\n",
      "Epoch: 2 .. batch: 430/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.54 .. NELBO: 583.41\n",
      "Epoch: 2 .. batch: 432/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.51 .. NELBO: 583.38\n",
      "Epoch: 2 .. batch: 434/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.62 .. NELBO: 583.5\n",
      "Epoch: 2 .. batch: 436/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.69 .. NELBO: 583.57\n",
      "Epoch: 2 .. batch: 438/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.83 .. NELBO: 583.71\n",
      "Epoch: 2 .. batch: 440/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.76 .. NELBO: 583.64\n",
      "Epoch: 2 .. batch: 442/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.9 .. NELBO: 583.78\n",
      "Epoch: 2 .. batch: 444/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.95 .. NELBO: 583.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 .. batch: 446/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.77 .. NELBO: 583.65\n",
      "Epoch: 2 .. batch: 448/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.83 .. NELBO: 583.71\n",
      "Epoch: 2 .. batch: 450/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.67 .. NELBO: 583.55\n",
      "Epoch: 2 .. batch: 452/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.65 .. NELBO: 583.53\n",
      "Epoch: 2 .. batch: 454/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.65 .. NELBO: 583.53\n",
      "Epoch: 2 .. batch: 456/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.52 .. NELBO: 583.4\n",
      "Epoch: 2 .. batch: 458/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.33 .. NELBO: 583.21\n",
      "Epoch: 2 .. batch: 460/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.39 .. NELBO: 583.26\n",
      "Epoch: 2 .. batch: 462/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.38 .. NELBO: 583.25\n",
      "Epoch: 2 .. batch: 464/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.29 .. NELBO: 583.16\n",
      "Epoch: 2 .. batch: 466/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.45 .. NELBO: 583.33\n",
      "Epoch: 2 .. batch: 468/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.38 .. NELBO: 583.26\n",
      "Epoch: 2 .. batch: 470/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.19 .. NELBO: 583.06\n",
      "Epoch: 2 .. batch: 472/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 575.27 .. NELBO: 583.14\n",
      "Epoch: 2 .. batch: 474/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.16 .. NELBO: 583.04\n",
      "Epoch: 2 .. batch: 476/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.05 .. NELBO: 582.93\n",
      "Epoch: 2 .. batch: 478/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.09 .. NELBO: 582.97\n",
      "Epoch: 2 .. batch: 480/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 575.12 .. NELBO: 583.0\n",
      "Epoch: 2 .. batch: 482/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.99 .. NELBO: 582.87\n",
      "Epoch: 2 .. batch: 484/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 574.91 .. NELBO: 582.78\n",
      "Epoch: 2 .. batch: 486/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 574.93 .. NELBO: 582.8\n",
      "Epoch: 2 .. batch: 488/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 574.8 .. NELBO: 582.67\n",
      "Epoch: 2 .. batch: 490/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 574.76 .. NELBO: 582.63\n",
      "Epoch: 2 .. batch: 492/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.7 .. NELBO: 582.58\n",
      "Epoch: 2 .. batch: 494/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.74 .. NELBO: 582.62\n",
      "Epoch: 2 .. batch: 496/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 574.71 .. NELBO: 582.58\n",
      "Epoch: 2 .. batch: 498/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 574.65 .. NELBO: 582.52\n",
      "Epoch: 2 .. batch: 500/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 574.59 .. NELBO: 582.46\n",
      "Epoch: 2 .. batch: 502/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 574.64 .. NELBO: 582.51\n",
      "Epoch: 2 .. batch: 504/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.73 .. NELBO: 582.61\n",
      "Epoch: 2 .. batch: 506/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.75 .. NELBO: 582.63\n",
      "Epoch: 2 .. batch: 508/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 574.75 .. NELBO: 582.62\n",
      "Epoch: 2 .. batch: 510/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 574.7 .. NELBO: 582.57\n",
      "Epoch: 2 .. batch: 512/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.57 .. NELBO: 582.45\n",
      "Epoch: 2 .. batch: 514/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.61 .. NELBO: 582.49\n",
      "Epoch: 2 .. batch: 516/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.52 .. NELBO: 582.4\n",
      "Epoch: 2 .. batch: 518/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.42 .. NELBO: 582.3\n",
      "Epoch: 2 .. batch: 520/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.34 .. NELBO: 582.22\n",
      "Epoch: 2 .. batch: 522/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.26 .. NELBO: 582.14\n",
      "Epoch: 2 .. batch: 524/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 574.26 .. NELBO: 582.13\n",
      "Epoch: 2 .. batch: 526/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 574.1 .. NELBO: 581.97\n",
      "Epoch: 2 .. batch: 528/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 574.04 .. NELBO: 581.91\n",
      "Epoch: 2 .. batch: 530/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.16 .. NELBO: 582.04\n",
      "Epoch: 2 .. batch: 532/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.16 .. NELBO: 582.04\n",
      "Epoch: 2 .. batch: 534/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.06 .. NELBO: 581.94\n",
      "Epoch: 2 .. batch: 536/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.1 .. NELBO: 581.98\n",
      "Epoch: 2 .. batch: 538/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.01 .. NELBO: 581.89\n",
      "Epoch: 2 .. batch: 540/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 574.03 .. NELBO: 581.91\n",
      "Epoch: 2 .. batch: 542/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 573.9 .. NELBO: 581.78\n",
      "Epoch: 2 .. batch: 544/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 573.77 .. NELBO: 581.65\n",
      "Epoch: 2 .. batch: 546/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 573.68 .. NELBO: 581.56\n",
      "Epoch: 2 .. batch: 548/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 573.76 .. NELBO: 581.64\n",
      "Epoch: 2 .. batch: 550/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 573.52 .. NELBO: 581.4\n",
      "Epoch: 2 .. batch: 552/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 573.35 .. NELBO: 581.22\n",
      "Epoch: 2 .. batch: 554/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 573.24 .. NELBO: 581.11\n",
      "Epoch: 2 .. batch: 556/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 573.13 .. NELBO: 581.0\n",
      "Epoch: 2 .. batch: 558/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 573.06 .. NELBO: 580.93\n",
      "Epoch: 2 .. batch: 560/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 573.15 .. NELBO: 581.03\n",
      "Epoch: 2 .. batch: 562/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 573.15 .. NELBO: 581.02\n",
      "Epoch: 2 .. batch: 564/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 573.17 .. NELBO: 581.04\n",
      "Epoch: 2 .. batch: 566/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 573.14 .. NELBO: 581.01\n",
      "Epoch: 2 .. batch: 568/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 573.14 .. NELBO: 581.02\n",
      "Epoch: 2 .. batch: 570/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 573.1 .. NELBO: 580.98\n",
      "Epoch: 2 .. batch: 572/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 573.02 .. NELBO: 580.9\n",
      "Epoch: 2 .. batch: 574/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.88 .. NELBO: 580.75\n",
      "Epoch: 2 .. batch: 576/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.9 .. NELBO: 580.77\n",
      "Epoch: 2 .. batch: 578/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.93 .. NELBO: 580.8\n",
      "Epoch: 2 .. batch: 580/900 .. LR: 0.005 .. KL_theta: 7.88 .. Rec_loss: 573.0 .. NELBO: 580.88\n",
      "Epoch: 2 .. batch: 582/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.92 .. NELBO: 580.79\n",
      "Epoch: 2 .. batch: 584/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.95 .. NELBO: 580.82\n",
      "Epoch: 2 .. batch: 586/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.94 .. NELBO: 580.81\n",
      "Epoch: 2 .. batch: 588/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.81 .. NELBO: 580.68\n",
      "Epoch: 2 .. batch: 590/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.75 .. NELBO: 580.62\n",
      "Epoch: 2 .. batch: 592/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.72 .. NELBO: 580.59\n",
      "Epoch: 2 .. batch: 594/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.76 .. NELBO: 580.63\n",
      "Epoch: 2 .. batch: 596/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.75 .. NELBO: 580.62\n",
      "Epoch: 2 .. batch: 598/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.76 .. NELBO: 580.63\n",
      "Epoch: 2 .. batch: 600/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.73 .. NELBO: 580.6\n",
      "Epoch: 2 .. batch: 602/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.72 .. NELBO: 580.59\n",
      "Epoch: 2 .. batch: 604/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.72 .. NELBO: 580.59\n",
      "Epoch: 2 .. batch: 606/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.76 .. NELBO: 580.63\n",
      "Epoch: 2 .. batch: 608/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.71 .. NELBO: 580.58\n",
      "Epoch: 2 .. batch: 610/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.71 .. NELBO: 580.58\n",
      "Epoch: 2 .. batch: 612/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.65 .. NELBO: 580.52\n",
      "Epoch: 2 .. batch: 614/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.64 .. NELBO: 580.51\n",
      "Epoch: 2 .. batch: 616/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.55 .. NELBO: 580.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 .. batch: 618/900 .. LR: 0.005 .. KL_theta: 7.87 .. Rec_loss: 572.41 .. NELBO: 580.28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if mode == 'train':\n",
    "    ## train model on data \n",
    "    best_epoch = 0\n",
    "    best_val_ppl = 1e9\n",
    "    all_val_ppls = []\n",
    "    print('\\n')\n",
    "    print('Visualizing model quality before training...')\n",
    "    visualize(model)\n",
    "    print('\\n')\n",
    "    for epoch in range(1, epochs):\n",
    "        train(epoch)\n",
    "        val_ppl = evaluate(model, 'val')\n",
    "        if val_ppl < best_val_ppl:\n",
    "            with open(ckpt, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_epoch = epoch\n",
    "            best_val_ppl = val_ppl\n",
    "        else:\n",
    "            ## check whether to anneal lr\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            if anneal_lr and (len(all_val_ppls) > nonmono and val_ppl > min(all_val_ppls[:-nonmono]) and lr > 1e-5):\n",
    "                optimizer.param_groups[0]['lr'] /= lr_factor\n",
    "        if epoch % visualize_every == 0:\n",
    "            visualize(model)\n",
    "        all_val_ppls.append(val_ppl)\n",
    "    with open(ckpt, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    model = model.to(device)\n",
    "    val_ppl = evaluate(model, 'val')\n",
    "else:   \n",
    "    with open(ckpt, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ## get document completion perplexities\n",
    "        test_ppl = evaluate(model, 'test', tc=tc, td=td)\n",
    "\n",
    "        ## get most used topics\n",
    "        indices = torch.tensor(range(num_docs_train))\n",
    "        indices = torch.split(indices, batch_size)\n",
    "        thetaAvg = torch.zeros(1, num_topics).to(device)\n",
    "        thetaWeightedAvg = torch.zeros(1, num_topics).to(device)\n",
    "        cnt = 0\n",
    "        for idx, ind in enumerate(indices):\n",
    "            try:\n",
    "                data_batch = data.get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "                sums = data_batch.sum(1).unsqueeze(1)\n",
    "                cnt += sums.sum(0).squeeze().cpu().numpy()\n",
    "                if bow_norm:\n",
    "                    normalized_data_batch = data_batch / sums\n",
    "                else:\n",
    "                    normalized_data_batch = data_batch\n",
    "                theta, _ = model.get_theta(normalized_data_batch)\n",
    "                thetaAvg += theta.sum(0).unsqueeze(0) / num_docs_train\n",
    "                weighed_theta = sums * theta\n",
    "                thetaWeightedAvg += weighed_theta.sum(0).unsqueeze(0)\n",
    "                if idx % 100 == 0 and idx > 0:\n",
    "                    print('batch: {}/{}'.format(idx, len(indices)))\n",
    "            except IndexError:\n",
    "                continue\n",
    "        thetaWeightedAvg = thetaWeightedAvg.squeeze().cpu().numpy() / cnt\n",
    "        print('\\nThe 10 most used topics are {}'.format(thetaWeightedAvg.argsort()[::-1][:10]))\n",
    "\n",
    "        ## show topics\n",
    "        beta = model.get_beta()\n",
    "        topic_indices = list(np.random.choice(num_topics, 10)) # 10 random topics\n",
    "        print('\\n')\n",
    "        for k in range(num_topics):#topic_indices:\n",
    "            gamma = beta[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if train_embeddings:\n",
    "            ## show etm embeddings \n",
    "            try:\n",
    "                rho_etm = model.rho.weight.cpu()\n",
    "            except:\n",
    "                rho_etm = model.rho.cpu()\n",
    "            queries = ['andrew', 'woman', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                            'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "            print('\\n')\n",
    "            print('ETM embeddings...')\n",
    "            for word in queries:\n",
    "                print('word: {} .. etm neighbors: {}'.format(word, nearest_neighbors(word, rho_etm, vocab)))\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python main.py --mode train --dataset 20ng --data_path data/20ng --num_topics 50 --train_embeddings 1 --epochs 1000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AZ Social Media Analytics)",
   "language": "python",
   "name": "atsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
