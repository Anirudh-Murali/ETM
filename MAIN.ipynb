{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Required Embeddings\n",
    "\n",
    "Note: This section can be skipped if embeddings are already prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "# sys.setdefaultencoding() does not exist, here!\n",
    "# reload(sys)  # Reload does the trick!\n",
    "# sys.setdefaultencoding('UTF8')\n",
    "\n",
    "# sys.setdefaultencoding('UTF8')\n",
    "\n",
    "data_file= \"data/drug_review/preprocessed_reviews.csv\"        # default='', help='a .txt file containing the corpus'\n",
    "\n",
    "dim_rho= 200                                               #default=300, help='dimensionality of the word embeddings'\n",
    "min_count= 4                                               #default=2, help='minimum term frequency (to define the vocabulary)'\n",
    "sg= 1                                                      # default=1, help='whether to use skip-gram'\n",
    "# workers= 6                                                #default=25, help='number of CPU cores'\n",
    "negative_samples= 10                                       # default=10, help='number of negative samples'\n",
    "window_size= 8                                             # default=4, help='window size to determine context'\n",
    "iters= 50                                                  #default=50, help='number of iterationst'\n",
    "\n",
    "emb_file= \"embeddings/embeddings\"+\"_dim_\"+str(dim_rho)+\"_min_count_\"+str(min_count)+\"_sg_\"+str(sg)+\"_negative_samples_\"+str(negative_samples)+\"_window_size_\"+str(window_size)+\"_iters_\"+str(iters)+\".txt\"                      #default='embeddings.txt', help='file to save the word embeddings'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class for a memory-friendly iterator over the dataset\n",
    "class MySentences(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.file_type = 'text'\n",
    " \n",
    "    def __iter__(self):\n",
    "        if self.file_type == 'text':\n",
    "            for line in open(self.filename,encoding=\"utf8\"):\n",
    "                yield line.split()\n",
    "        elif self.file_type == 'csv':\n",
    "            for line in tqdm(self.reviews.values):\n",
    "                try:\n",
    "                    yield line.split()\n",
    "#                     print(line)\n",
    "                except AttributeError:\n",
    "#                     print(line)\n",
    "                    continue\n",
    "                \n",
    "    def __init__(self, filename,col):\n",
    "        self.filename = filename\n",
    "        data = pd.read_csv(filename)\n",
    "        self.reviews = data[col]#[:10000]\n",
    "        self.file_type = 'csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 15000/15000 [00:00<00:00, 111225.44it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6091.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6544.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6862.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6743.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6848.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 7137.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6816.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6792.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6604.78it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6919.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6723.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6663.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6684.60it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6583.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6507.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6932.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6928.90it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6895.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6807.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6772.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6827.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6841.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6876.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6620.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6181.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6930.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6961.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6941.75it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6972.83it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6743.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6527.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 5920.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6347.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6851.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6940.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6584.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6438.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6613.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6784.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6835.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6355.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6610.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6637.00it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6145.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6295.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6346.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 5837.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6279.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6047.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15000/15000 [00:02<00:00, 6103.48it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = MySentences(data_file,'review') # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(sentences, min_count=min_count, sg=sg, size=dim_rho, \n",
    "    iter=(iters), negative=negative_samples, window=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Write the embeddings to a file\n",
    "with open(emb_file, 'w',encoding='utf8') as f:\n",
    "    for v in list(model.wv.vocab):\n",
    "        vec = list(model.wv.__getitem__(v))\n",
    "        f.write(v + ' ')\n",
    "        vec_str = ['%.9f' % val for val in vec]\n",
    "        vec_str = \" \".join(vec_str)\n",
    "        f.write(vec_str + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# abc = pickle.load(\"data/20ng/vocab.pkl\")\n",
    "# abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import data\n",
    "import scipy.io\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from etm import ETM\n",
    "from utils import nearest_neighbors, get_topic_coherence, get_topic_diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/drug_review/drugsComTrain_raw.tsv\",delimiter=\"\\t\")[:1000]\n",
    "# df.to_csv(\"data/drug_review/drugs_train_1000.csv\",index=None)\n",
    "# reviews = df.review\n",
    "# with open(\"train_file.txt\", 'w',encoding='utf8') as f:\n",
    "#     for review in reviews.values:\n",
    "#         f.write(review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"20ng\"\n",
    "\n",
    "dataset =   \"train_file.txt\"                      #default='20ng', help='name of corpus'\n",
    "data_path = 'data/drug_review/'#default='data/20ng', help='directory containing data'\n",
    "emb_path = \"embeddings/embeddings\"+\"_dim_\"+str(dim_rho)+\"_min_count_\"+str(min_count)+\"_sg_\"+str(sg)+\"_negative_samples_\"+str(negative_samples)+\"_window_size_\"+str(window_size)+\"_iters_\"+str(iters)+\".txt\"#default='data/20ng_embeddings.txt', help='directory containing word embeddings'\n",
    "save_path = './results'#default='./results', help='path to save results'\n",
    "batch_size = 100 #default=1000, help='input batch size for training'\n",
    "\n",
    "### model-related arguments\n",
    "num_topics = 15   #default=50, help='number of topics'\n",
    "rho_size = 200    #default=300, help='dimension of rho'\n",
    "emb_size = 200    #default=300, help='dimension of embeddings'\n",
    "t_hidden_size = 800 #default=800, help='dimension of hidden space of q(theta)'\n",
    "theta_act = 'relu' #default='relu', help='tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu)'\n",
    "train_embeddings = 0 #default=0, help='whether to fix rho or train it'\n",
    "\n",
    "### optimization-related arguments\n",
    "lr = 0.005 # default=0.005, help='learning rate'\n",
    "lr_factor =4.0  #default=4.0, help='divide learning rate by this...'\n",
    "epochs = 10 # default=20, help='number of epochs to train...150 for 20ng 100 for others'\n",
    "mode = 'train'# default='train', help='train or eval model'\n",
    "optimizer = 'adam'# default='adam', help='choice of optimizer'\n",
    "seed = 2019# default=2019, help='random seed (default: 1)\n",
    "enc_drop = 0.0# default=0.0, help='dropout rate on encoder'\n",
    "clip = 0.0# default=0.0, help='gradient clipping'\n",
    "nonmono = 10# default=10, help='number of bad hits allowed'\n",
    "wdecay = 1.2e-6# default=1.2e-6, help='some l2 regularization'\n",
    "anneal_lr = 0#  default=0, help='whether to anneal the learning rate or not'\n",
    "bow_norm = 1# default=1, help='normalize the bows or not'\n",
    "\n",
    "### evaluation, visualization, and logging-related arguments\n",
    "num_words = 10  # default=10, help='number of words for topic viz' \n",
    "log_interval = 2 # default=2, help='when to log training'\n",
    "visualize_every = 1 # default=10, help='when to visualize results'\n",
    "eval_batch_size = 1000 # default=1000, help='input batch size for evaluation'\n",
    "load_from = 'results/etm_train_file.txt_K_15_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_100_RhoSize_200_trainEmbeddings_0'\n",
    "tc = 0# default=0, help='whether to compute topic coherence or not'\n",
    "td = 0# default=0, help='whether to compute topic diversity or not'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x283ac112670>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")#(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('\\n')\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, train, valid, test,test_1,test_2 = data.get_data(os.path.join(data_path))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 1. training data\n",
    "train_tokens = train['tokens']\n",
    "train_counts = train['counts']\n",
    "num_docs_train = len(train_tokens)\n",
    "\n",
    "# 2. dev set\n",
    "valid_tokens = valid['tokens']\n",
    "valid_counts = valid['counts']\n",
    "num_docs_valid = len(valid_tokens)\n",
    "\n",
    "# 3. test data\n",
    "test_tokens = test['tokens']\n",
    "test_counts = test['counts']\n",
    "num_docs_test = len(test_tokens)\n",
    "test_1_tokens = test_1['tokens']\n",
    "test_1_counts = test_1['counts']\n",
    "num_docs_test_1 = len(test_1_tokens)\n",
    "test_2_tokens = test_2['tokens']\n",
    "test_2_counts = test_2['counts']\n",
    "num_docs_test_2 = len(test_2_tokens)\n",
    "\n",
    "embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n"
     ]
    }
   ],
   "source": [
    "#/usr/bin/python\n",
    "## get data\n",
    "# 1. vocabulary\n",
    "\n",
    "if not train_embeddings:\n",
    "    emb_path = emb_path\n",
    "    vect_path = os.path.join(data_path.split('/')[0], 'vocab.pkl')   \n",
    "    vectors = {}\n",
    "    with open(emb_path, 'rb') as f:\n",
    "        for l in f:\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "            if word in vocab:\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                vectors[word] = vect\n",
    "    embeddings = np.zeros((vocab_size, emb_size))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(vocab):\n",
    "        try: \n",
    "            embeddings[i] = vectors[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            embeddings[i] = np.random.normal(scale=0.6, size=(emb_size, ))\n",
    "    embeddings = torch.tensor(embeddings).to(device)\n",
    "    embeddings_dim = embeddings.size()\n",
    "\n",
    "print('=*'*100)\n",
    "# print('Training an Embedded Topic Model on {} with the following settings: {}'.format(dataset.upper()))\n",
    "print('=*'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embeddings/embeddings_dim_200_min_count_4_sg_1_negative_samples_10_window_size_8_iters_50.txt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.0)\n",
      "  (theta_act): ReLU()\n",
      "  (alphas): Linear(in_features=200, out_features=15, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=19856, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=15, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## define checkpoint\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "if mode == 'eval':\n",
    "    ckpt = load_from\n",
    "else:\n",
    "    ckpt = os.path.join(save_path, \n",
    "        'etm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_trainEmbeddings_{}'.format(\n",
    "        dataset, num_topics, t_hidden_size, optimizer, clip, theta_act, \n",
    "            lr, batch_size, rho_size, train_embeddings))\n",
    "\n",
    "## define model and optimizer\n",
    "model = ETM(num_topics, vocab_size, t_hidden_size, rho_size, emb_size, \n",
    "                theta_act, embeddings, train_embeddings, enc_drop).to(device)\n",
    "\n",
    "print('model: {}'.format(model))\n",
    "\n",
    "if optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "elif optimizer == 'asgd':\n",
    "    optimizer = optim.ASGD(model.parameters(), lr=lr, t0=0, lambd=0., weight_decay=wdecay)\n",
    "else:\n",
    "    print('Defaulting to vanilla SGD')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    acc_loss = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    cnt = 0\n",
    "    indices = torch.randperm(num_docs_train)\n",
    "    indices = torch.split(indices, batch_size)\n",
    "    for idx, ind in enumerate(indices):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            data_batch = data.get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "            sums = data_batch.sum(1).unsqueeze(1)\n",
    "            if bow_norm:\n",
    "                normalized_data_batch = data_batch / sums\n",
    "            else:\n",
    "                normalized_data_batch = data_batch\n",
    "            recon_loss, kld_theta = model(data_batch, normalized_data_batch)\n",
    "            total_loss = recon_loss + kld_theta\n",
    "            total_loss.backward()\n",
    "\n",
    "            if clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_loss += torch.sum(recon_loss).item()\n",
    "            acc_kl_theta_loss += torch.sum(kld_theta).item()\n",
    "            cnt += 1\n",
    "\n",
    "            if idx % log_interval == 0 and idx > 0:\n",
    "                cur_loss = round(acc_loss / cnt, 2) \n",
    "                cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "                cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "\n",
    "                print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                    epoch, idx, len(indices), optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "        except IndexError:\n",
    "            cnt+=1\n",
    "            continue\n",
    "    cur_loss = round(acc_loss / cnt, 2) \n",
    "    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "    cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "    print('*'*100)\n",
    "    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "            epoch, optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(m, show_emb=True):\n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "\n",
    "    m.eval()\n",
    "\n",
    "    queries = ['skin','cycle','effects','price','worst','best','efficacy','performance','cancer','disease']\n",
    "\n",
    "    ## visualize topics using monte carlo\n",
    "    with torch.no_grad():\n",
    "        print('#'*100)\n",
    "        print('Visualize topics...')\n",
    "        topics_words = []\n",
    "        gammas = m.get_beta()\n",
    "        \n",
    "        for k in range(num_topics):\n",
    "            gamma = gammas[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if show_emb:\n",
    "            ## visualize word embeddings by using V to get nearest neighbors\n",
    "            print('#'*100)\n",
    "            print('Visualize word embeddings by using output embedding matrix')\n",
    "            try:\n",
    "                embeddings = m.rho.weight  # Vocab_size x E\n",
    "            except:\n",
    "                embeddings = m.rho         # Vocab_size x E\n",
    "            neighbors = []\n",
    "            for word in queries:\n",
    "                try:\n",
    "                    print('word: {} .. neighbors: {}'.format(\n",
    "                        word, nearest_neighbors(word, embeddings, vocab)))\n",
    "                except ValueError:\n",
    "                    print(\"querry doesn't exist!!\")\n",
    "            print('#'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(m, source, tc=False, td=False):\n",
    "    \"\"\"Compute perplexity on document completion.\n",
    "    \"\"\"\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        if source == 'val':\n",
    "            indices = torch.split(torch.tensor(range(num_docs_valid)), eval_batch_size)\n",
    "            tokens = valid_tokens\n",
    "            counts = valid_counts\n",
    "        else: \n",
    "            indices = torch.split(torch.tensor(range(num_docs_test)), eval_batch_size)\n",
    "            tokens = test_tokens\n",
    "            counts = test_counts\n",
    "\n",
    "        ## get \\beta here\n",
    "        beta = m.get_beta()\n",
    "\n",
    "        ### do dc and tc here\n",
    "        acc_loss = 0\n",
    "        cnt = 0\n",
    "        indices_1 = torch.split(torch.tensor(range(num_docs_test_1)), eval_batch_size)\n",
    "        for idx, ind in enumerate(indices_1):\n",
    "            try:\n",
    "                ## get theta from first half of docs\n",
    "                data_batch_1 = data.get_batch(test_1_tokens, test_1_counts, ind, vocab_size, device)\n",
    "                sums_1 = data_batch_1.sum(1).unsqueeze(1)\n",
    "                if bow_norm:\n",
    "                    normalized_data_batch_1 = data_batch_1 / sums_1\n",
    "                else:\n",
    "                    normalized_data_batch_1 = data_batch_1\n",
    "                theta, _ = m.get_theta(normalized_data_batch_1)\n",
    "\n",
    "                ## get prediction loss using second half\n",
    "                data_batch_2 = data.get_batch(test_2_tokens, test_2_counts, ind, vocab_size, device)\n",
    "                sums_2 = data_batch_2.sum(1).unsqueeze(1)\n",
    "                res = torch.mm(theta, beta)\n",
    "                preds = torch.log(res)\n",
    "                recon_loss = -(preds * data_batch_2).sum(1)\n",
    "\n",
    "                loss = recon_loss / sums_2.squeeze()\n",
    "                loss = loss.mean().item()\n",
    "                acc_loss += loss\n",
    "                cnt += 1\n",
    "            except IndexError:\n",
    "                cnt+=1\n",
    "                continue\n",
    "        cur_loss = acc_loss / cnt\n",
    "        ppl_dc = round(math.exp(cur_loss), 1)\n",
    "        print('*'*100)\n",
    "        print('{} Doc Completion PPL: {}'.format(source.upper(), ppl_dc))\n",
    "        print('*'*100)\n",
    "        if tc or td:\n",
    "            beta = beta.data.cpu().numpy()\n",
    "            if tc:\n",
    "                print('Computing topic coherence...')\n",
    "                get_topic_coherence(beta, train_tokens, vocab)\n",
    "            if td:\n",
    "                print('Computing topic diversity...')\n",
    "                get_topic_diversity(beta, 25)\n",
    "        return ppl_dc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Visualizing model quality before training...\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0: ['haa', 'spendy', 'anythingi', 'aricept', 'skincellulitespider', 'ticsnot', 'forbid', 'weepyshort', 'wpreservatives']\n",
      "Topic 1: ['depoproverai', 'simcor', 'quotplugquot', 'smother', 'badlyand', 'nondrug', 'attackanxiety', 'deminishing', 'apirl']\n",
      "Topic 2: ['levoxyl', 'dermatogolist', 'driveid', 'bumbs', 'depersonalize', 'freaky', 'meso', 'ego', 'trustworthy']\n",
      "Topic 3: ['omacor', 'nauseouslost', 'attacksfor', 'wrestle', 'celebrix', 'enjoyment', 'fluoxetineprozac', 'reliance', 'papuls']\n",
      "Topic 4: ['awfuldidnt', 'levlen', 'perceptible', 'willprescribe', 'dominance', 'basement', 'clyamdia', 'clenil', 'so']\n",
      "Topic 5: ['preauthoraziton', 'dayseww', 'addedcant', 'phoenix', 'creamits', 'incorporate', 'azurette', 'issuesinsertion', 'triphasiltrivora']\n",
      "Topic 6: ['antibioticresistant', 'disinterest', 'spam', 'bodyfat', 'hammer', 'sleepingbreathing', 'ravenously', 'alluquot', 'balk']\n",
      "Topic 7: ['subcutaneously', 'drugim', 'insecurity', 'diahrreaing', 'ejaculatory', 'achepressure', 'nebulizar', 'dissipates', 'periodtwo']\n",
      "Topic 8: ['sxs', 'wld', 'selfcath', 'amazingyay', 'gob', 'headachenauseous', 'cystsi', 'uncrusted', 'mocked']\n",
      "Topic 9: ['poundsirregular', 'stiffer', 'gadocdptsd', 'insometimes', 'stickiness', 'unimaginable', 'tally', 'golimumab', 'emetophobia']\n",
      "Topic 10: ['realm', 'triplethreat', 'pevention', 'toquot', 'fancy', 'reactionary', 'quotwill', 'peripexole', 'fortesta']\n",
      "Topic 11: ['imight', 'abilifyso', 'stil', 'olopatadine', 'upsetam', 'reckon', 'urology', 'crusade', 'headachesquot']\n",
      "Topic 12: ['counterintuitive', 'discontinuation', 'anaphylaxis', 'bawling', 'oncedaily', 'snappy', 'evoclin', 'leakageincontinence', 'swollowing']\n",
      "Topic 13: ['amparms', 'mesh', 'quotsilentquot', 'irritiation', 'tlevel', 'baggage', 'normalcy', 'clinched', 'footwear']\n",
      "Topic 14: ['speckle', 'scamming', 'amwe', 'ketogenic', 'deviate', 'haaa', 'quotclear', 'mcgh', 'quotuncomfortablequot']\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: skin .. neighbors: ['skin', 'face', 'clear', 'acne', 'sunscreen', 'breakout', 'pimple', 'peel', 'flake', 'redness', 'epiduo', 'retin', 'papule', 'ziana', 'oily', 'neutrogena', 'clinique', 'acanya', 'complexion', 'moisturiser']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cycle .. neighbors: ['cycle', 'period', 'menstrual', 'lighter', 'safyral', 'month', 'bleeding', 'menstruate', 'hormone', 'cramp', 'heavy', 'tracker', 'predictable', 'natazia', 'amethia', 'spot', 'bleed', 'mononessa', 'heavier', 'pill']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: price .. neighbors: ['price', 'pay', 'retail', 'formulary', 'insurance', 'paid', 'fee', 'dollar', 'usa', 'tag', 'pocket', 'citizen', 'drugstore', 'astronomical', 'restasis', 'coupon', 'astonish', 'cheap', 'pollen', 'copays']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: best .. neighbors: ['best', 'try', 'good', 'great', 'work', 'bet', 'take', 'sacroiliac', 'ive', 'methe', 'tricyclic', 'help', 'youth', 'adherence', 'knowledgeable', 'sceptical', 'astelin', 'different', 'psychology', 'pharmacy']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: efficacy .. neighbors: ['efficacy', 'generess', 'buti', 'version', 'maintenance', 'unisom', 'weighti', 'threshold', 'simponi', 'approval', 'reluctantly', 'sufficient', 'psoriatic', 'accustom', 'national', 'intolerable', 'ssrisnris', 'fee', 'attest', 'pdoc']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: performance .. neighbors: ['performance', 'sexual', 'cialis', 'bph', 'semen', 'erection', 'ratio', 'distraction', 'climax', 'avodart', 'stendra', 'viagra', 'benign', 'smarter', 'elate', 'stamen', 'prostate', 'anorgasmia', 'marriage', 'adhdadd']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cancer .. neighbors: ['cancer', 'ibrance', 'radiation', 'chemo', 'lymph', 'node', 'tarceva', 'prostate', 'zoladex', 'femara', 'oncologist', 'chemotherapy', 'leukemia', 'sutent', 'alfuzosin', 'carcinoma', 'arimidex', 'casodex', 'psa', 'avastin']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: disease .. neighbors: ['disease', 'degenerative', 'crohn', 'autoimmune', 'lyme', 'obstructive', 'herniated', 'diagnose', 'fistula', 'classify', 'lumbar', 'hashimotos', 'disc', 'remicade', 'currently', 'inflammatory', 'ankylose', 'kapidex', 'herniation', 'leukemia']\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Epoch: 1 .. batch: 2/135 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 321.27 .. NELBO: 321.29\n",
      "Epoch: 1 .. batch: 4/135 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 323.45 .. NELBO: 323.47\n",
      "Epoch: 1 .. batch: 6/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 323.54 .. NELBO: 323.55\n",
      "Epoch: 1 .. batch: 8/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 324.06 .. NELBO: 324.07\n",
      "Epoch: 1 .. batch: 10/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 324.87 .. NELBO: 324.88\n",
      "Epoch: 1 .. batch: 12/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 328.29 .. NELBO: 328.3\n",
      "Epoch: 1 .. batch: 14/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 324.95 .. NELBO: 324.96\n",
      "Epoch: 1 .. batch: 16/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 325.2 .. NELBO: 325.21\n",
      "Epoch: 1 .. batch: 18/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 326.75 .. NELBO: 326.76\n",
      "Epoch: 1 .. batch: 20/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 325.95 .. NELBO: 325.96\n",
      "Epoch: 1 .. batch: 22/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 325.16 .. NELBO: 325.17\n",
      "Epoch: 1 .. batch: 24/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 325.36 .. NELBO: 325.37\n",
      "Epoch: 1 .. batch: 26/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 324.85 .. NELBO: 324.86\n",
      "Epoch: 1 .. batch: 28/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 323.9 .. NELBO: 323.91\n",
      "Epoch: 1 .. batch: 30/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 322.98 .. NELBO: 322.99\n",
      "Epoch: 1 .. batch: 32/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 323.05 .. NELBO: 323.06\n",
      "Epoch: 1 .. batch: 34/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 322.41 .. NELBO: 322.42\n",
      "Epoch: 1 .. batch: 36/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 322.39 .. NELBO: 322.4\n",
      "Epoch: 1 .. batch: 38/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 322.41 .. NELBO: 322.42\n",
      "Epoch: 1 .. batch: 40/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 322.89 .. NELBO: 322.9\n",
      "Epoch: 1 .. batch: 42/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 322.69 .. NELBO: 322.7\n",
      "Epoch: 1 .. batch: 44/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 322.71 .. NELBO: 322.72\n",
      "Epoch: 1 .. batch: 46/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 322.73 .. NELBO: 322.74\n",
      "Epoch: 1 .. batch: 48/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 321.59 .. NELBO: 321.6\n",
      "Epoch: 1 .. batch: 50/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 322.03 .. NELBO: 322.04\n",
      "Epoch: 1 .. batch: 52/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 321.27 .. NELBO: 321.28\n",
      "Epoch: 1 .. batch: 56/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 314.34 .. NELBO: 314.35\n",
      "Epoch: 1 .. batch: 58/135 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 314.16 .. NELBO: 314.17\n",
      "Epoch: 1 .. batch: 60/135 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 314.96 .. NELBO: 314.98\n",
      "Epoch: 1 .. batch: 62/135 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 313.95 .. NELBO: 313.97\n",
      "Epoch: 1 .. batch: 64/135 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 313.85 .. NELBO: 313.87\n",
      "Epoch: 1 .. batch: 66/135 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 313.94 .. NELBO: 313.96\n",
      "Epoch: 1 .. batch: 68/135 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 313.6 .. NELBO: 313.62\n",
      "Epoch: 1 .. batch: 70/135 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 312.98 .. NELBO: 313.0\n",
      "Epoch: 1 .. batch: 72/135 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 312.9 .. NELBO: 312.92\n",
      "Epoch: 1 .. batch: 74/135 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 312.58 .. NELBO: 312.6\n",
      "Epoch: 1 .. batch: 76/135 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 312.74 .. NELBO: 312.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 .. batch: 78/135 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 312.42 .. NELBO: 312.45\n",
      "Epoch: 1 .. batch: 80/135 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 312.58 .. NELBO: 312.61\n",
      "Epoch: 1 .. batch: 82/135 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 312.37 .. NELBO: 312.41\n",
      "Epoch: 1 .. batch: 84/135 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 312.45 .. NELBO: 312.49\n",
      "Epoch: 1 .. batch: 86/135 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 312.04 .. NELBO: 312.08\n",
      "Epoch: 1 .. batch: 88/135 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 311.94 .. NELBO: 311.99\n",
      "Epoch: 1 .. batch: 90/135 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 311.84 .. NELBO: 311.9\n",
      "Epoch: 1 .. batch: 92/135 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 311.57 .. NELBO: 311.64\n",
      "Epoch: 1 .. batch: 94/135 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 311.8 .. NELBO: 311.88\n",
      "Epoch: 1 .. batch: 96/135 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 311.15 .. NELBO: 311.24\n",
      "Epoch: 1 .. batch: 98/135 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 311.05 .. NELBO: 311.15\n",
      "Epoch: 1 .. batch: 100/135 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 311.28 .. NELBO: 311.39\n",
      "Epoch: 1 .. batch: 102/135 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 311.41 .. NELBO: 311.53\n",
      "Epoch: 1 .. batch: 104/135 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 311.45 .. NELBO: 311.58\n",
      "Epoch: 1 .. batch: 106/135 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 311.35 .. NELBO: 311.48\n",
      "Epoch: 1 .. batch: 108/135 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 311.39 .. NELBO: 311.54\n",
      "Epoch: 1 .. batch: 110/135 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 311.21 .. NELBO: 311.36\n",
      "Epoch: 1 .. batch: 112/135 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 311.5 .. NELBO: 311.66\n",
      "Epoch: 1 .. batch: 114/135 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 311.57 .. NELBO: 311.74\n",
      "Epoch: 1 .. batch: 116/135 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 311.62 .. NELBO: 311.8\n",
      "Epoch: 1 .. batch: 118/135 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 311.3 .. NELBO: 311.49\n",
      "Epoch: 1 .. batch: 120/135 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 310.94 .. NELBO: 311.14\n",
      "Epoch: 1 .. batch: 122/135 .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 310.83 .. NELBO: 311.04\n",
      "Epoch: 1 .. batch: 124/135 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 310.94 .. NELBO: 311.16\n",
      "Epoch: 1 .. batch: 126/135 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 310.99 .. NELBO: 311.22\n",
      "Epoch: 1 .. batch: 128/135 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 311.03 .. NELBO: 311.27\n",
      "Epoch: 1 .. batch: 130/135 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 310.88 .. NELBO: 311.13\n",
      "Epoch: 1 .. batch: 132/135 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 310.86 .. NELBO: 311.11\n",
      "Epoch: 1 .. batch: 134/135 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 310.8 .. NELBO: 311.06\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 310.8 .. NELBO: 311.06\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 937.0\n",
      "****************************************************************************************************\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0: ['chillstripled', 'quotdoctor', 'postmeal', 'fogwas', 'laydown', 'yearstried', 'buckswhich', 'ensures', 'angst']\n",
      "Topic 1: ['chillstripled', 'quotdoctor', 'postmeal', 'yearstried', 'laydown', 'upscale', 'fogwas', 'ensures', 'buckswhich']\n",
      "Topic 2: ['chillstripled', 'postmeal', 'quotdoctor', 'fogwas', 'yearstried', 'laydown', 'ensures', 'resultsplaques', 'wellbutrine']\n",
      "Topic 3: ['chillstripled', 'quotdoctor', 'postmeal', 'yearstried', 'fogwas', 'resultsplaques', 'ensures', 'angst', 'laydown']\n",
      "Topic 4: ['chillstripled', 'postmeal', 'quotdoctor', 'laydown', 'fogwas', 'upscale', 'buckswhich', 'yearstried', 'resultsplaques']\n",
      "Topic 5: ['quotdoctor', 'postmeal', 'chillstripled', 'laydown', 'fogwas', 'angst', 'upscale', 'resultsplaques', 'wellbutrine']\n",
      "Topic 6: ['propaganda', 'boob', 'worstended', 'bleeding', 'period', 'paragard', 'tenderness', 'implanon', 'depo']\n",
      "Topic 7: ['postmeal', 'chillstripled', 'quotdoctor', 'fogwas', 'yearstried', 'angst', 'upscale', 'laydown', 'valiumis']\n",
      "Topic 8: ['chillstripled', 'postmeal', 'quotdoctor', 'yearstried', 'laydown', 'upscale', 'angst', 'ensures', 'fogwas']\n",
      "Topic 9: ['postmeal', 'quotdoctor', 'chillstripled', 'fogwas', 'wellbutrine', 'yearstried', 'resultsplaques', 'angst', 'laydown']\n",
      "Topic 10: ['chillstripled', 'postmeal', 'quotdoctor', 'yearstried', 'fogwas', 'laydown', 'ensures', 'resultsplaques', 'buckswhich']\n",
      "Topic 11: ['postmeal', 'quotdoctor', 'chillstripled', 'laydown', 'fogwas', 'yearstried', 'ensures', 'wellbutrine', 'buckswhich']\n",
      "Topic 12: ['chillstripled', 'quotdoctor', 'postmeal', 'fogwas', 'laydown', 'yearstried', 'angst', 'buckswhich', 'upscale']\n",
      "Topic 13: ['postmeal', 'chillstripled', 'quotdoctor', 'fogwas', 'laydown', 'yearstried', 'wellbutrine', 'ensures', 'buckswhich']\n",
      "Topic 14: ['chillstripled', 'postmeal', 'quotdoctor', 'laydown', 'fogwas', 'wellbutrine', 'yearstried', 'upscale', 'angst']\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: skin .. neighbors: ['skin', 'face', 'clear', 'acne', 'sunscreen', 'breakout', 'pimple', 'peel', 'flake', 'redness', 'epiduo', 'retin', 'papule', 'ziana', 'oily', 'neutrogena', 'clinique', 'acanya', 'complexion', 'moisturiser']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cycle .. neighbors: ['cycle', 'period', 'menstrual', 'lighter', 'safyral', 'month', 'bleeding', 'menstruate', 'hormone', 'cramp', 'heavy', 'tracker', 'predictable', 'natazia', 'amethia', 'spot', 'bleed', 'mononessa', 'heavier', 'pill']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: price .. neighbors: ['price', 'pay', 'retail', 'formulary', 'insurance', 'paid', 'fee', 'dollar', 'usa', 'tag', 'pocket', 'citizen', 'drugstore', 'astronomical', 'restasis', 'coupon', 'astonish', 'cheap', 'pollen', 'copays']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: best .. neighbors: ['best', 'try', 'good', 'great', 'work', 'bet', 'take', 'sacroiliac', 'ive', 'methe', 'tricyclic', 'help', 'youth', 'adherence', 'knowledgeable', 'sceptical', 'astelin', 'different', 'psychology', 'pharmacy']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: efficacy .. neighbors: ['efficacy', 'generess', 'buti', 'version', 'maintenance', 'unisom', 'weighti', 'threshold', 'simponi', 'approval', 'reluctantly', 'sufficient', 'psoriatic', 'accustom', 'national', 'intolerable', 'ssrisnris', 'fee', 'attest', 'pdoc']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: performance .. neighbors: ['performance', 'sexual', 'cialis', 'bph', 'semen', 'erection', 'ratio', 'distraction', 'climax', 'avodart', 'stendra', 'viagra', 'benign', 'smarter', 'elate', 'stamen', 'prostate', 'anorgasmia', 'marriage', 'adhdadd']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cancer .. neighbors: ['cancer', 'ibrance', 'radiation', 'chemo', 'lymph', 'node', 'tarceva', 'prostate', 'zoladex', 'femara', 'oncologist', 'chemotherapy', 'leukemia', 'sutent', 'alfuzosin', 'carcinoma', 'arimidex', 'casodex', 'psa', 'avastin']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: disease .. neighbors: ['disease', 'degenerative', 'crohn', 'autoimmune', 'lyme', 'obstructive', 'herniated', 'diagnose', 'fistula', 'classify', 'lumbar', 'hashimotos', 'disc', 'remicade', 'currently', 'inflammatory', 'ankylose', 'kapidex', 'herniation', 'leukemia']\n",
      "####################################################################################################\n",
      "Epoch: 2 .. batch: 2/135 .. LR: 0.005 .. KL_theta: 0.68 .. Rec_loss: 291.72 .. NELBO: 292.4\n",
      "Epoch: 2 .. batch: 4/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 299.78 .. NELBO: 300.37\n",
      "Epoch: 2 .. batch: 6/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 256.06 .. NELBO: 256.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 .. batch: 8/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 268.13 .. NELBO: 268.67\n",
      "Epoch: 2 .. batch: 10/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 274.14 .. NELBO: 274.74\n",
      "Epoch: 2 .. batch: 12/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 275.93 .. NELBO: 276.52\n",
      "Epoch: 2 .. batch: 14/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 279.64 .. NELBO: 280.24\n",
      "Epoch: 2 .. batch: 16/135 .. LR: 0.005 .. KL_theta: 0.63 .. Rec_loss: 282.04 .. NELBO: 282.67\n",
      "Epoch: 2 .. batch: 18/135 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 285.95 .. NELBO: 286.6\n",
      "Epoch: 2 .. batch: 20/135 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 287.87 .. NELBO: 288.51\n",
      "Epoch: 2 .. batch: 22/135 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 288.49 .. NELBO: 289.14\n",
      "Epoch: 2 .. batch: 24/135 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 288.04 .. NELBO: 288.69\n",
      "Epoch: 2 .. batch: 26/135 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 291.47 .. NELBO: 292.12\n",
      "Epoch: 2 .. batch: 28/135 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 292.3 .. NELBO: 292.94\n",
      "Epoch: 2 .. batch: 30/135 .. LR: 0.005 .. KL_theta: 0.63 .. Rec_loss: 293.16 .. NELBO: 293.79\n",
      "Epoch: 2 .. batch: 32/135 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 294.25 .. NELBO: 294.89\n",
      "Epoch: 2 .. batch: 34/135 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 295.64 .. NELBO: 296.29\n",
      "Epoch: 2 .. batch: 36/135 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 296.43 .. NELBO: 297.08\n",
      "Epoch: 2 .. batch: 38/135 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 297.52 .. NELBO: 298.16\n",
      "Epoch: 2 .. batch: 40/135 .. LR: 0.005 .. KL_theta: 0.63 .. Rec_loss: 297.2 .. NELBO: 297.83\n",
      "Epoch: 2 .. batch: 42/135 .. LR: 0.005 .. KL_theta: 0.63 .. Rec_loss: 298.31 .. NELBO: 298.94\n",
      "Epoch: 2 .. batch: 44/135 .. LR: 0.005 .. KL_theta: 0.62 .. Rec_loss: 298.25 .. NELBO: 298.87\n",
      "Epoch: 2 .. batch: 46/135 .. LR: 0.005 .. KL_theta: 0.62 .. Rec_loss: 298.8 .. NELBO: 299.42\n",
      "Epoch: 2 .. batch: 48/135 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 299.04 .. NELBO: 299.65\n",
      "Epoch: 2 .. batch: 50/135 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 299.1 .. NELBO: 299.71\n",
      "Epoch: 2 .. batch: 52/135 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 299.76 .. NELBO: 300.37\n",
      "Epoch: 2 .. batch: 54/135 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 299.67 .. NELBO: 300.28\n",
      "Epoch: 2 .. batch: 56/135 .. LR: 0.005 .. KL_theta: 0.62 .. Rec_loss: 299.44 .. NELBO: 300.06\n",
      "Epoch: 2 .. batch: 58/135 .. LR: 0.005 .. KL_theta: 0.62 .. Rec_loss: 299.68 .. NELBO: 300.3\n",
      "Epoch: 2 .. batch: 60/135 .. LR: 0.005 .. KL_theta: 0.62 .. Rec_loss: 300.38 .. NELBO: 301.0\n",
      "Epoch: 2 .. batch: 62/135 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 300.64 .. NELBO: 301.25\n",
      "Epoch: 2 .. batch: 64/135 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 300.41 .. NELBO: 301.02\n",
      "Epoch: 2 .. batch: 66/135 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 300.8 .. NELBO: 301.41\n",
      "Epoch: 2 .. batch: 68/135 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 301.27 .. NELBO: 301.88\n",
      "Epoch: 2 .. batch: 70/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 301.18 .. NELBO: 301.78\n",
      "Epoch: 2 .. batch: 72/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 300.56 .. NELBO: 301.16\n",
      "Epoch: 2 .. batch: 74/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 301.0 .. NELBO: 301.6\n",
      "Epoch: 2 .. batch: 76/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 301.16 .. NELBO: 301.76\n",
      "Epoch: 2 .. batch: 78/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 300.99 .. NELBO: 301.59\n",
      "Epoch: 2 .. batch: 80/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 300.76 .. NELBO: 301.36\n",
      "Epoch: 2 .. batch: 82/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 300.9 .. NELBO: 301.5\n",
      "Epoch: 2 .. batch: 84/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 301.33 .. NELBO: 301.93\n",
      "Epoch: 2 .. batch: 86/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 301.36 .. NELBO: 301.96\n",
      "Epoch: 2 .. batch: 88/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 301.72 .. NELBO: 302.32\n",
      "Epoch: 2 .. batch: 90/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 301.97 .. NELBO: 302.57\n",
      "Epoch: 2 .. batch: 92/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 302.18 .. NELBO: 302.78\n",
      "Epoch: 2 .. batch: 94/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 302.05 .. NELBO: 302.64\n",
      "Epoch: 2 .. batch: 96/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 302.25 .. NELBO: 302.84\n",
      "Epoch: 2 .. batch: 98/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 302.15 .. NELBO: 302.74\n",
      "Epoch: 2 .. batch: 100/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 302.0 .. NELBO: 302.59\n",
      "Epoch: 2 .. batch: 102/135 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 302.01 .. NELBO: 302.61\n",
      "Epoch: 2 .. batch: 104/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 302.23 .. NELBO: 302.82\n",
      "Epoch: 2 .. batch: 106/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 302.39 .. NELBO: 302.98\n",
      "Epoch: 2 .. batch: 108/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 302.49 .. NELBO: 303.08\n",
      "Epoch: 2 .. batch: 110/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 302.69 .. NELBO: 303.28\n",
      "Epoch: 2 .. batch: 112/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 302.38 .. NELBO: 302.97\n",
      "Epoch: 2 .. batch: 114/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 302.67 .. NELBO: 303.26\n",
      "Epoch: 2 .. batch: 116/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 302.98 .. NELBO: 303.57\n",
      "Epoch: 2 .. batch: 118/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 303.26 .. NELBO: 303.85\n",
      "Epoch: 2 .. batch: 120/135 .. LR: 0.005 .. KL_theta: 0.58 .. Rec_loss: 303.41 .. NELBO: 303.99\n",
      "Epoch: 2 .. batch: 122/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 303.46 .. NELBO: 304.05\n",
      "Epoch: 2 .. batch: 124/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 303.59 .. NELBO: 304.18\n",
      "Epoch: 2 .. batch: 126/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 303.57 .. NELBO: 304.16\n",
      "Epoch: 2 .. batch: 128/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 303.86 .. NELBO: 304.45\n",
      "Epoch: 2 .. batch: 130/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 303.99 .. NELBO: 304.58\n",
      "Epoch: 2 .. batch: 132/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 304.1 .. NELBO: 304.69\n",
      "Epoch: 2 .. batch: 134/135 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 304.28 .. NELBO: 304.87\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 304.28 .. NELBO: 304.87\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 910.8\n",
      "****************************************************************************************************\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0: ['postmeal', 'quotdoctor', 'laydown', 'fogwas', 'chillstripled', 'wellbutrine', 'yearstried', 'ensures', 'upscale']\n",
      "Topic 1: ['postmeal', 'quotdoctor', 'chillstripled', 'fogwas', 'laydown', 'yearstried', 'wellbutrine', 'upscale', 'buckswhich']\n",
      "Topic 2: ['quotdoctor', 'chillstripled', 'postmeal', 'fogwas', 'laydown', 'yearstried', 'ensures', 'wellbutrine', 'buckswhich']\n",
      "Topic 3: ['postmeal', 'quotdoctor', 'chillstripled', 'fogwas', 'laydown', 'yearstried', 'upscale', 'ensures', 'wellbutrine']\n",
      "Topic 4: ['quotdoctor', 'postmeal', 'fogwas', 'chillstripled', 'wellbutrine', 'laydown', 'ensures', 'buckswhich', 'yearstried']\n",
      "Topic 5: ['postmeal', 'laydown', 'quotdoctor', 'chillstripled', 'wellbutrine', 'yearstried', 'fogwas', 'ensures', 'buckswhich']\n",
      "Topic 6: ['period', 'swing', 'bleeding', 'spot', 'implanon', 'birth', 'nexplanon', 'tenderness', 'depo']\n",
      "Topic 7: ['quotdoctor', 'postmeal', 'chillstripled', 'laydown', 'fogwas', 'ensures', 'yearstried', 'buckswhich', 'wellbutrine']\n",
      "Topic 8: ['postmeal', 'quotdoctor', 'fogwas', 'chillstripled', 'laydown', 'yearstried', 'wellbutrine', 'buckswhich', 'ensures']\n",
      "Topic 9: ['postmeal', 'quotdoctor', 'chillstripled', 'laydown', 'fogwas', 'yearstried', 'wellbutrine', 'buckswhich', 'resultsplaques']\n",
      "Topic 10: ['quotdoctor', 'postmeal', 'laydown', 'chillstripled', 'fogwas', 'wellbutrine', 'yearstried', 'buckswhich', 'ensures']\n",
      "Topic 11: ['quotdoctor', 'postmeal', 'fogwas', 'laydown', 'chillstripled', 'wellbutrine', 'yearstried', 'buckswhich', 'ensures']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 12: ['quotdoctor', 'postmeal', 'chillstripled', 'fogwas', 'laydown', 'yearstried', 'ensures', 'wellbutrine', 'buckswhich']\n",
      "Topic 13: ['quotdoctor', 'fogwas', 'postmeal', 'chillstripled', 'laydown', 'yearstried', 'wellbutrine', 'buckswhich', 'ensures']\n",
      "Topic 14: ['postmeal', 'chillstripled', 'quotdoctor', 'laydown', 'fogwas', 'wellbutrine', 'yearstried', 'upscale', 'gi']\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: skin .. neighbors: ['skin', 'face', 'clear', 'acne', 'sunscreen', 'breakout', 'pimple', 'peel', 'flake', 'redness', 'epiduo', 'retin', 'papule', 'ziana', 'oily', 'neutrogena', 'clinique', 'acanya', 'complexion', 'moisturiser']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cycle .. neighbors: ['cycle', 'period', 'menstrual', 'lighter', 'safyral', 'month', 'bleeding', 'menstruate', 'hormone', 'cramp', 'heavy', 'tracker', 'predictable', 'natazia', 'amethia', 'spot', 'bleed', 'mononessa', 'heavier', 'pill']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: price .. neighbors: ['price', 'pay', 'retail', 'formulary', 'insurance', 'paid', 'fee', 'dollar', 'usa', 'tag', 'pocket', 'citizen', 'drugstore', 'astronomical', 'restasis', 'coupon', 'astonish', 'cheap', 'pollen', 'copays']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: best .. neighbors: ['best', 'try', 'good', 'great', 'work', 'bet', 'take', 'sacroiliac', 'ive', 'methe', 'tricyclic', 'help', 'youth', 'adherence', 'knowledgeable', 'sceptical', 'astelin', 'different', 'psychology', 'pharmacy']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: efficacy .. neighbors: ['efficacy', 'generess', 'buti', 'version', 'maintenance', 'unisom', 'weighti', 'threshold', 'simponi', 'approval', 'reluctantly', 'sufficient', 'psoriatic', 'accustom', 'national', 'intolerable', 'ssrisnris', 'fee', 'attest', 'pdoc']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: performance .. neighbors: ['performance', 'sexual', 'cialis', 'bph', 'semen', 'erection', 'ratio', 'distraction', 'climax', 'avodart', 'stendra', 'viagra', 'benign', 'smarter', 'elate', 'stamen', 'prostate', 'anorgasmia', 'marriage', 'adhdadd']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cancer .. neighbors: ['cancer', 'ibrance', 'radiation', 'chemo', 'lymph', 'node', 'tarceva', 'prostate', 'zoladex', 'femara', 'oncologist', 'chemotherapy', 'leukemia', 'sutent', 'alfuzosin', 'carcinoma', 'arimidex', 'casodex', 'psa', 'avastin']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: disease .. neighbors: ['disease', 'degenerative', 'crohn', 'autoimmune', 'lyme', 'obstructive', 'herniated', 'diagnose', 'fistula', 'classify', 'lumbar', 'hashimotos', 'disc', 'remicade', 'currently', 'inflammatory', 'ankylose', 'kapidex', 'herniation', 'leukemia']\n",
      "####################################################################################################\n",
      "Epoch: 3 .. batch: 2/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 315.53 .. NELBO: 316.07\n",
      "Epoch: 3 .. batch: 4/135 .. LR: 0.005 .. KL_theta: 0.55 .. Rec_loss: 313.18 .. NELBO: 313.73\n",
      "Epoch: 3 .. batch: 6/135 .. LR: 0.005 .. KL_theta: 0.55 .. Rec_loss: 312.66 .. NELBO: 313.21\n",
      "Epoch: 3 .. batch: 8/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 309.7 .. NELBO: 310.24\n",
      "Epoch: 3 .. batch: 10/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 307.73 .. NELBO: 308.27\n",
      "Epoch: 3 .. batch: 12/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 308.13 .. NELBO: 308.67\n",
      "Epoch: 3 .. batch: 14/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 308.01 .. NELBO: 308.54\n",
      "Epoch: 3 .. batch: 16/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 308.62 .. NELBO: 309.15\n",
      "Epoch: 3 .. batch: 18/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 310.58 .. NELBO: 311.12\n",
      "Epoch: 3 .. batch: 20/135 .. LR: 0.005 .. KL_theta: 0.55 .. Rec_loss: 308.77 .. NELBO: 309.32\n",
      "Epoch: 3 .. batch: 24/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 296.05 .. NELBO: 296.56\n",
      "Epoch: 3 .. batch: 26/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 297.32 .. NELBO: 297.83\n",
      "Epoch: 3 .. batch: 28/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 298.19 .. NELBO: 298.7\n",
      "Epoch: 3 .. batch: 30/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 299.34 .. NELBO: 299.86\n",
      "Epoch: 3 .. batch: 32/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 300.12 .. NELBO: 300.64\n",
      "Epoch: 3 .. batch: 34/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 300.56 .. NELBO: 301.09\n",
      "Epoch: 3 .. batch: 36/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 301.43 .. NELBO: 301.96\n",
      "Epoch: 3 .. batch: 38/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 301.21 .. NELBO: 301.75\n",
      "Epoch: 3 .. batch: 40/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 300.95 .. NELBO: 301.49\n",
      "Epoch: 3 .. batch: 42/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 300.93 .. NELBO: 301.47\n",
      "Epoch: 3 .. batch: 44/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.35 .. NELBO: 302.88\n",
      "Epoch: 3 .. batch: 46/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 303.35 .. NELBO: 303.89\n",
      "Epoch: 3 .. batch: 48/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 303.61 .. NELBO: 304.15\n",
      "Epoch: 3 .. batch: 50/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 304.1 .. NELBO: 304.64\n",
      "Epoch: 3 .. batch: 52/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.62 .. NELBO: 304.15\n",
      "Epoch: 3 .. batch: 54/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.93 .. NELBO: 304.46\n",
      "Epoch: 3 .. batch: 56/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.22 .. NELBO: 303.75\n",
      "Epoch: 3 .. batch: 58/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.64 .. NELBO: 303.17\n",
      "Epoch: 3 .. batch: 60/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 302.93 .. NELBO: 303.47\n",
      "Epoch: 3 .. batch: 62/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 303.06 .. NELBO: 303.6\n",
      "Epoch: 3 .. batch: 64/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 303.55 .. NELBO: 304.09\n",
      "Epoch: 3 .. batch: 66/135 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 302.9 .. NELBO: 303.44\n",
      "Epoch: 3 .. batch: 68/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.03 .. NELBO: 303.56\n",
      "Epoch: 3 .. batch: 70/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.7 .. NELBO: 303.23\n",
      "Epoch: 3 .. batch: 72/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.38 .. NELBO: 302.91\n",
      "Epoch: 3 .. batch: 74/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.54 .. NELBO: 303.07\n",
      "Epoch: 3 .. batch: 76/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.1 .. NELBO: 302.63\n",
      "Epoch: 3 .. batch: 78/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.31 .. NELBO: 302.84\n",
      "Epoch: 3 .. batch: 80/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.43 .. NELBO: 302.96\n",
      "Epoch: 3 .. batch: 82/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.66 .. NELBO: 303.19\n",
      "Epoch: 3 .. batch: 84/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.79 .. NELBO: 303.32\n",
      "Epoch: 3 .. batch: 86/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.75 .. NELBO: 303.28\n",
      "Epoch: 3 .. batch: 88/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.57 .. NELBO: 303.1\n",
      "Epoch: 3 .. batch: 90/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.63 .. NELBO: 303.16\n",
      "Epoch: 3 .. batch: 92/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 302.43 .. NELBO: 302.95\n",
      "Epoch: 3 .. batch: 94/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 302.77 .. NELBO: 303.29\n",
      "Epoch: 3 .. batch: 96/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 302.36 .. NELBO: 302.88\n",
      "Epoch: 3 .. batch: 98/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.58 .. NELBO: 303.11\n",
      "Epoch: 3 .. batch: 100/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.49 .. NELBO: 303.02\n",
      "Epoch: 3 .. batch: 102/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 302.77 .. NELBO: 303.3\n",
      "Epoch: 3 .. batch: 104/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.42 .. NELBO: 303.95\n",
      "Epoch: 3 .. batch: 106/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.5 .. NELBO: 304.03\n",
      "Epoch: 3 .. batch: 108/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.35 .. NELBO: 303.88\n",
      "Epoch: 3 .. batch: 110/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.11 .. NELBO: 303.64\n",
      "Epoch: 3 .. batch: 112/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.12 .. NELBO: 303.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 .. batch: 114/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.04 .. NELBO: 303.57\n",
      "Epoch: 3 .. batch: 116/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.38 .. NELBO: 303.91\n",
      "Epoch: 3 .. batch: 118/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.83 .. NELBO: 304.36\n",
      "Epoch: 3 .. batch: 120/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 304.18 .. NELBO: 304.71\n",
      "Epoch: 3 .. batch: 122/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.9 .. NELBO: 304.43\n",
      "Epoch: 3 .. batch: 124/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.76 .. NELBO: 304.29\n",
      "Epoch: 3 .. batch: 126/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 303.86 .. NELBO: 304.38\n",
      "Epoch: 3 .. batch: 128/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 304.11 .. NELBO: 304.63\n",
      "Epoch: 3 .. batch: 130/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 304.18 .. NELBO: 304.7\n",
      "Epoch: 3 .. batch: 132/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 303.92 .. NELBO: 304.44\n",
      "Epoch: 3 .. batch: 134/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 303.72 .. NELBO: 304.24\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 303.72 .. NELBO: 304.24\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 895.7\n",
      "****************************************************************************************************\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0: ['postmeal', 'quotdoctor', 'laydown', 'chillstripled', 'fogwas', 'wellbutrine', 'yearstried', 'upscale', 'buckswhich']\n",
      "Topic 1: ['quotdoctor', 'postmeal', 'laydown', 'yearstried', 'fogwas', 'wellbutrine', 'chillstripled', 'upscale', 'quotmyquot']\n",
      "Topic 2: ['quotdoctor', 'yearstried', 'postmeal', 'laydown', 'fogwas', 'chillstripled', 'wellbutrine', 'quotmyquot', 'buckswhich']\n",
      "Topic 3: ['postmeal', 'quotdoctor', 'fogwas', 'wellbutrine', 'yearstried', 'laydown', 'chillstripled', 'gi', 'upscale']\n",
      "Topic 4: ['postmeal', 'quotdoctor', 'wellbutrine', 'yearstried', 'fogwas', 'laydown', 'chillstripled', 'upscale', 'quotmyquot']\n",
      "Topic 5: ['laydown', 'postmeal', 'fogwas', 'yearstried', 'quotdoctor', 'wellbutrine', 'chillstripled', 'buckswhich', 'upscale']\n",
      "Topic 6: ['period', 'swing', 'bleeding', 'gain', 'spot', 'birth', 'sex', 'implanon', 'acne']\n",
      "Topic 7: ['quotdoctor', 'postmeal', 'fogwas', 'yearstried', 'chillstripled', 'wellbutrine', 'laydown', 'ensures', 'buckswhich']\n",
      "Topic 8: ['postmeal', 'quotdoctor', 'wellbutrine', 'chillstripled', 'fogwas', 'yearstried', 'laydown', 'upscale', 'buckswhich']\n",
      "Topic 9: ['quotdoctor', 'postmeal', 'yearstried', 'wellbutrine', 'chillstripled', 'fogwas', 'laydown', 'upscale', 'buckswhich']\n",
      "Topic 10: ['postmeal', 'quotdoctor', 'yearstried', 'fogwas', 'wellbutrine', 'laydown', 'chillstripled', 'buckswhich', 'upscale']\n",
      "Topic 11: ['quotdoctor', 'postmeal', 'laydown', 'wellbutrine', 'yearstried', 'chillstripled', 'fogwas', 'buckswhich', 'gi']\n",
      "Topic 12: ['postmeal', 'fogwas', 'quotdoctor', 'wellbutrine', 'yearstried', 'laydown', 'chillstripled', 'buckswhich', 'ensures']\n",
      "Topic 13: ['postmeal', 'laydown', 'fogwas', 'wellbutrine', 'quotdoctor', 'yearstried', 'chillstripled', 'upscale', 'gi']\n",
      "Topic 14: ['postmeal', 'quotdoctor', 'fogwas', 'wellbutrine', 'yearstried', 'laydown', 'chillstripled', 'buckswhich', 'gi']\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: skin .. neighbors: ['skin', 'face', 'clear', 'acne', 'sunscreen', 'breakout', 'pimple', 'peel', 'flake', 'redness', 'epiduo', 'retin', 'papule', 'ziana', 'oily', 'neutrogena', 'clinique', 'acanya', 'complexion', 'moisturiser']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cycle .. neighbors: ['cycle', 'period', 'menstrual', 'lighter', 'safyral', 'month', 'bleeding', 'menstruate', 'hormone', 'cramp', 'heavy', 'tracker', 'predictable', 'natazia', 'amethia', 'spot', 'bleed', 'mononessa', 'heavier', 'pill']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: price .. neighbors: ['price', 'pay', 'retail', 'formulary', 'insurance', 'paid', 'fee', 'dollar', 'usa', 'tag', 'pocket', 'citizen', 'drugstore', 'astronomical', 'restasis', 'coupon', 'astonish', 'cheap', 'pollen', 'copays']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: best .. neighbors: ['best', 'try', 'good', 'great', 'work', 'bet', 'take', 'sacroiliac', 'ive', 'methe', 'tricyclic', 'help', 'youth', 'adherence', 'knowledgeable', 'sceptical', 'astelin', 'different', 'psychology', 'pharmacy']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: efficacy .. neighbors: ['efficacy', 'generess', 'buti', 'version', 'maintenance', 'unisom', 'weighti', 'threshold', 'simponi', 'approval', 'reluctantly', 'sufficient', 'psoriatic', 'accustom', 'national', 'intolerable', 'ssrisnris', 'fee', 'attest', 'pdoc']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: performance .. neighbors: ['performance', 'sexual', 'cialis', 'bph', 'semen', 'erection', 'ratio', 'distraction', 'climax', 'avodart', 'stendra', 'viagra', 'benign', 'smarter', 'elate', 'stamen', 'prostate', 'anorgasmia', 'marriage', 'adhdadd']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cancer .. neighbors: ['cancer', 'ibrance', 'radiation', 'chemo', 'lymph', 'node', 'tarceva', 'prostate', 'zoladex', 'femara', 'oncologist', 'chemotherapy', 'leukemia', 'sutent', 'alfuzosin', 'carcinoma', 'arimidex', 'casodex', 'psa', 'avastin']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: disease .. neighbors: ['disease', 'degenerative', 'crohn', 'autoimmune', 'lyme', 'obstructive', 'herniated', 'diagnose', 'fistula', 'classify', 'lumbar', 'hashimotos', 'disc', 'remicade', 'currently', 'inflammatory', 'ankylose', 'kapidex', 'herniation', 'leukemia']\n",
      "####################################################################################################\n",
      "Epoch: 4 .. batch: 2/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 312.97 .. NELBO: 313.45\n",
      "Epoch: 4 .. batch: 4/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 306.52 .. NELBO: 307.0\n",
      "Epoch: 4 .. batch: 6/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 308.7 .. NELBO: 309.22\n",
      "Epoch: 4 .. batch: 8/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 308.81 .. NELBO: 309.34\n",
      "Epoch: 4 .. batch: 10/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 306.57 .. NELBO: 307.09\n",
      "Epoch: 4 .. batch: 12/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 304.23 .. NELBO: 304.74\n",
      "Epoch: 4 .. batch: 14/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 305.45 .. NELBO: 305.96\n",
      "Epoch: 4 .. batch: 16/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.61 .. NELBO: 306.11\n",
      "Epoch: 4 .. batch: 18/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 306.23 .. NELBO: 306.74\n",
      "Epoch: 4 .. batch: 20/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 306.26 .. NELBO: 306.77\n",
      "Epoch: 4 .. batch: 22/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 308.33 .. NELBO: 308.85\n",
      "Epoch: 4 .. batch: 24/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 306.76 .. NELBO: 307.28\n",
      "Epoch: 4 .. batch: 26/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 306.63 .. NELBO: 307.16\n",
      "Epoch: 4 .. batch: 28/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 307.62 .. NELBO: 308.15\n",
      "Epoch: 4 .. batch: 30/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 307.17 .. NELBO: 307.7\n",
      "Epoch: 4 .. batch: 32/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 306.45 .. NELBO: 306.98\n",
      "Epoch: 4 .. batch: 34/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 307.26 .. NELBO: 307.79\n",
      "Epoch: 4 .. batch: 36/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 306.26 .. NELBO: 306.79\n",
      "Epoch: 4 .. batch: 38/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 305.51 .. NELBO: 306.04\n",
      "Epoch: 4 .. batch: 40/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 305.78 .. NELBO: 306.31\n",
      "Epoch: 4 .. batch: 42/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 305.3 .. NELBO: 305.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 .. batch: 44/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 305.52 .. NELBO: 306.05\n",
      "Epoch: 4 .. batch: 46/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 306.11 .. NELBO: 306.64\n",
      "Epoch: 4 .. batch: 48/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 305.97 .. NELBO: 306.5\n",
      "Epoch: 4 .. batch: 50/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 306.12 .. NELBO: 306.64\n",
      "Epoch: 4 .. batch: 52/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 306.18 .. NELBO: 306.7\n",
      "Epoch: 4 .. batch: 54/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 306.31 .. NELBO: 306.83\n",
      "Epoch: 4 .. batch: 56/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 306.12 .. NELBO: 306.65\n",
      "Epoch: 4 .. batch: 58/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 306.39 .. NELBO: 306.92\n",
      "Epoch: 4 .. batch: 60/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 305.91 .. NELBO: 306.44\n",
      "Epoch: 4 .. batch: 62/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 305.35 .. NELBO: 305.88\n",
      "Epoch: 4 .. batch: 64/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 305.37 .. NELBO: 305.9\n",
      "Epoch: 4 .. batch: 66/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 305.85 .. NELBO: 306.38\n",
      "Epoch: 4 .. batch: 68/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 305.59 .. NELBO: 306.11\n",
      "Epoch: 4 .. batch: 70/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 306.08 .. NELBO: 306.6\n",
      "Epoch: 4 .. batch: 72/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 305.87 .. NELBO: 306.39\n",
      "Epoch: 4 .. batch: 74/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 305.6 .. NELBO: 306.12\n",
      "Epoch: 4 .. batch: 76/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 305.48 .. NELBO: 306.01\n",
      "Epoch: 4 .. batch: 78/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 305.31 .. NELBO: 305.84\n",
      "Epoch: 4 .. batch: 82/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 301.94 .. NELBO: 302.46\n",
      "Epoch: 4 .. batch: 84/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 301.69 .. NELBO: 302.21\n",
      "Epoch: 4 .. batch: 86/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 301.77 .. NELBO: 302.28\n",
      "Epoch: 4 .. batch: 88/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 301.45 .. NELBO: 301.96\n",
      "Epoch: 4 .. batch: 90/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 301.65 .. NELBO: 302.16\n",
      "Epoch: 4 .. batch: 92/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 301.64 .. NELBO: 302.15\n",
      "Epoch: 4 .. batch: 94/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 301.83 .. NELBO: 302.34\n",
      "Epoch: 4 .. batch: 96/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.17 .. NELBO: 302.68\n",
      "Epoch: 4 .. batch: 98/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.4 .. NELBO: 302.91\n",
      "Epoch: 4 .. batch: 100/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.29 .. NELBO: 302.8\n",
      "Epoch: 4 .. batch: 102/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.32 .. NELBO: 302.83\n",
      "Epoch: 4 .. batch: 104/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.04 .. NELBO: 302.55\n",
      "Epoch: 4 .. batch: 106/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.06 .. NELBO: 302.57\n",
      "Epoch: 4 .. batch: 108/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.28 .. NELBO: 302.79\n",
      "Epoch: 4 .. batch: 110/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.11 .. NELBO: 302.62\n",
      "Epoch: 4 .. batch: 112/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.08 .. NELBO: 302.59\n",
      "Epoch: 4 .. batch: 114/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 301.94 .. NELBO: 302.46\n",
      "Epoch: 4 .. batch: 116/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 302.09 .. NELBO: 302.61\n",
      "Epoch: 4 .. batch: 118/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 302.37 .. NELBO: 302.89\n",
      "Epoch: 4 .. batch: 120/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 303.05 .. NELBO: 303.57\n",
      "Epoch: 4 .. batch: 122/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 303.12 .. NELBO: 303.64\n",
      "Epoch: 4 .. batch: 124/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 303.26 .. NELBO: 303.78\n",
      "Epoch: 4 .. batch: 126/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 303.17 .. NELBO: 303.69\n",
      "Epoch: 4 .. batch: 128/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 302.95 .. NELBO: 303.47\n",
      "Epoch: 4 .. batch: 130/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 303.19 .. NELBO: 303.71\n",
      "Epoch: 4 .. batch: 132/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 303.33 .. NELBO: 303.84\n",
      "Epoch: 4 .. batch: 134/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 303.57 .. NELBO: 304.08\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 303.57 .. NELBO: 304.08\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 890.5\n",
      "****************************************************************************************************\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0: ['quotdoctor', 'postmeal', 'chillstripled', 'upscale', 'fogwas', 'laydown', 'yearstried', 'wellbutrine', 'resultsplaques']\n",
      "Topic 1: ['postmeal', 'fogwas', 'chillstripled', 'yearstried', 'quotdoctor', 'buckswhich', 'laydown', 'wellbutrine', 'upscale']\n",
      "Topic 2: ['fogwas', 'postmeal', 'quotdoctor', 'chillstripled', 'wellbutrine', 'yearstried', 'buckswhich', 'laydown', 'upscale']\n",
      "Topic 3: ['postmeal', 'chillstripled', 'fogwas', 'quotdoctor', 'upscale', 'laydown', 'yearstried', 'wellbutrine', 'gi']\n",
      "Topic 4: ['postmeal', 'quotdoctor', 'chillstripled', 'upscale', 'wellbutrine', 'yearstried', 'laydown', 'fogwas', 'resultsplaques']\n",
      "Topic 5: ['postmeal', 'quotdoctor', 'upscale', 'chillstripled', 'laydown', 'yearstried', 'fogwas', 'wellbutrine', 'gi']\n",
      "Topic 6: ['period', 'gain', 'swing', 'bleeding', 'birth', 'sex', 'spot', 'acne', 'weight']\n",
      "Topic 7: ['postmeal', 'quotdoctor', 'yearstried', 'upscale', 'wellbutrine', 'laydown', 'chillstripled', 'fogwas', 'valiumis']\n",
      "Topic 8: ['postmeal', 'quotdoctor', 'laydown', 'upscale', 'fogwas', 'chillstripled', 'yearstried', 'wellbutrine', 'gi']\n",
      "Topic 9: ['postmeal', 'fogwas', 'quotdoctor', 'chillstripled', 'laydown', 'yearstried', 'wellbutrine', 'buckswhich', 'upscale']\n",
      "Topic 10: ['postmeal', 'quotdoctor', 'laydown', 'wellbutrine', 'chillstripled', 'yearstried', 'fogwas', 'upscale', 'buckswhich']\n",
      "Topic 11: ['postmeal', 'quotdoctor', 'upscale', 'yearstried', 'chillstripled', 'fogwas', 'laydown', 'buckswhich', 'wellbutrine']\n",
      "Topic 12: ['postmeal', 'quotdoctor', 'upscale', 'chillstripled', 'fogwas', 'buckswhich', 'yearstried', 'laydown', 'gi']\n",
      "Topic 13: ['postmeal', 'quotdoctor', 'chillstripled', 'fogwas', 'yearstried', 'wellbutrine', 'upscale', 'laydown', 'buckswhich']\n",
      "Topic 14: ['postmeal', 'quotdoctor', 'chillstripled', 'upscale', 'wellbutrine', 'fogwas', 'laydown', 'yearstried', 'quotmyquot']\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: skin .. neighbors: ['skin', 'face', 'clear', 'acne', 'sunscreen', 'breakout', 'pimple', 'peel', 'flake', 'redness', 'epiduo', 'retin', 'papule', 'ziana', 'oily', 'neutrogena', 'clinique', 'acanya', 'complexion', 'moisturiser']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cycle .. neighbors: ['cycle', 'period', 'menstrual', 'lighter', 'safyral', 'month', 'bleeding', 'menstruate', 'hormone', 'cramp', 'heavy', 'tracker', 'predictable', 'natazia', 'amethia', 'spot', 'bleed', 'mononessa', 'heavier', 'pill']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: price .. neighbors: ['price', 'pay', 'retail', 'formulary', 'insurance', 'paid', 'fee', 'dollar', 'usa', 'tag', 'pocket', 'citizen', 'drugstore', 'astronomical', 'restasis', 'coupon', 'astonish', 'cheap', 'pollen', 'copays']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: best .. neighbors: ['best', 'try', 'good', 'great', 'work', 'bet', 'take', 'sacroiliac', 'ive', 'methe', 'tricyclic', 'help', 'youth', 'adherence', 'knowledgeable', 'sceptical', 'astelin', 'different', 'psychology', 'pharmacy']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: efficacy .. neighbors: ['efficacy', 'generess', 'buti', 'version', 'maintenance', 'unisom', 'weighti', 'threshold', 'simponi', 'approval', 'reluctantly', 'sufficient', 'psoriatic', 'accustom', 'national', 'intolerable', 'ssrisnris', 'fee', 'attest', 'pdoc']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: performance .. neighbors: ['performance', 'sexual', 'cialis', 'bph', 'semen', 'erection', 'ratio', 'distraction', 'climax', 'avodart', 'stendra', 'viagra', 'benign', 'smarter', 'elate', 'stamen', 'prostate', 'anorgasmia', 'marriage', 'adhdadd']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cancer .. neighbors: ['cancer', 'ibrance', 'radiation', 'chemo', 'lymph', 'node', 'tarceva', 'prostate', 'zoladex', 'femara', 'oncologist', 'chemotherapy', 'leukemia', 'sutent', 'alfuzosin', 'carcinoma', 'arimidex', 'casodex', 'psa', 'avastin']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: disease .. neighbors: ['disease', 'degenerative', 'crohn', 'autoimmune', 'lyme', 'obstructive', 'herniated', 'diagnose', 'fistula', 'classify', 'lumbar', 'hashimotos', 'disc', 'remicade', 'currently', 'inflammatory', 'ankylose', 'kapidex', 'herniation', 'leukemia']\n",
      "####################################################################################################\n",
      "Epoch: 5 .. batch: 2/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 306.61 .. NELBO: 307.13\n",
      "Epoch: 5 .. batch: 4/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 305.79 .. NELBO: 306.3\n",
      "Epoch: 5 .. batch: 6/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 301.49 .. NELBO: 302.0\n",
      "Epoch: 5 .. batch: 8/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.45 .. NELBO: 303.95\n",
      "Epoch: 5 .. batch: 10/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.56 .. NELBO: 304.06\n",
      "Epoch: 5 .. batch: 12/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.7 .. NELBO: 306.2\n",
      "Epoch: 5 .. batch: 14/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 305.89 .. NELBO: 306.41\n",
      "Epoch: 5 .. batch: 16/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 305.91 .. NELBO: 306.44\n",
      "Epoch: 5 .. batch: 18/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 305.22 .. NELBO: 305.75\n",
      "Epoch: 5 .. batch: 20/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 304.82 .. NELBO: 305.34\n",
      "Epoch: 5 .. batch: 22/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 302.04 .. NELBO: 302.56\n",
      "Epoch: 5 .. batch: 24/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 302.67 .. NELBO: 303.19\n",
      "Epoch: 5 .. batch: 26/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 300.31 .. NELBO: 300.83\n",
      "Epoch: 5 .. batch: 28/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 301.24 .. NELBO: 301.76\n",
      "Epoch: 5 .. batch: 30/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 302.45 .. NELBO: 302.97\n",
      "Epoch: 5 .. batch: 32/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 301.73 .. NELBO: 302.24\n",
      "Epoch: 5 .. batch: 34/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.64 .. NELBO: 303.15\n",
      "Epoch: 5 .. batch: 36/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.5 .. NELBO: 303.01\n",
      "Epoch: 5 .. batch: 38/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.78 .. NELBO: 303.29\n",
      "Epoch: 5 .. batch: 40/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 303.55 .. NELBO: 304.06\n",
      "Epoch: 5 .. batch: 42/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 303.96 .. NELBO: 304.48\n",
      "Epoch: 5 .. batch: 44/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 305.26 .. NELBO: 305.77\n",
      "Epoch: 5 .. batch: 46/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 305.27 .. NELBO: 305.78\n",
      "Epoch: 5 .. batch: 48/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 299.02 .. NELBO: 299.52\n",
      "Epoch: 5 .. batch: 50/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 298.9 .. NELBO: 299.4\n",
      "Epoch: 5 .. batch: 52/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 299.29 .. NELBO: 299.79\n",
      "Epoch: 5 .. batch: 54/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 299.66 .. NELBO: 300.16\n",
      "Epoch: 5 .. batch: 56/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 299.63 .. NELBO: 300.13\n",
      "Epoch: 5 .. batch: 58/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 299.14 .. NELBO: 299.65\n",
      "Epoch: 5 .. batch: 60/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 299.76 .. NELBO: 300.26\n",
      "Epoch: 5 .. batch: 62/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 300.84 .. NELBO: 301.34\n",
      "Epoch: 5 .. batch: 64/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 300.98 .. NELBO: 301.49\n",
      "Epoch: 5 .. batch: 66/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 301.33 .. NELBO: 301.84\n",
      "Epoch: 5 .. batch: 68/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 301.23 .. NELBO: 301.74\n",
      "Epoch: 5 .. batch: 70/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 301.1 .. NELBO: 301.61\n",
      "Epoch: 5 .. batch: 72/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 301.35 .. NELBO: 301.86\n",
      "Epoch: 5 .. batch: 74/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 301.57 .. NELBO: 302.08\n",
      "Epoch: 5 .. batch: 76/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 301.85 .. NELBO: 302.36\n",
      "Epoch: 5 .. batch: 78/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 301.7 .. NELBO: 302.21\n",
      "Epoch: 5 .. batch: 80/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.38 .. NELBO: 301.88\n",
      "Epoch: 5 .. batch: 82/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.69 .. NELBO: 302.19\n",
      "Epoch: 5 .. batch: 84/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.32 .. NELBO: 301.82\n",
      "Epoch: 5 .. batch: 86/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.63 .. NELBO: 302.13\n",
      "Epoch: 5 .. batch: 88/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.64 .. NELBO: 302.14\n",
      "Epoch: 5 .. batch: 90/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.25 .. NELBO: 301.75\n",
      "Epoch: 5 .. batch: 92/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.7 .. NELBO: 302.2\n",
      "Epoch: 5 .. batch: 94/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.55 .. NELBO: 302.05\n",
      "Epoch: 5 .. batch: 96/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.54 .. NELBO: 302.04\n",
      "Epoch: 5 .. batch: 98/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.58 .. NELBO: 302.08\n",
      "Epoch: 5 .. batch: 100/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.69 .. NELBO: 302.19\n",
      "Epoch: 5 .. batch: 102/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.82 .. NELBO: 302.32\n",
      "Epoch: 5 .. batch: 104/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.04 .. NELBO: 302.54\n",
      "Epoch: 5 .. batch: 106/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.08 .. NELBO: 302.58\n",
      "Epoch: 5 .. batch: 108/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.91 .. NELBO: 302.41\n",
      "Epoch: 5 .. batch: 110/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.05 .. NELBO: 302.55\n",
      "Epoch: 5 .. batch: 112/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.12 .. NELBO: 302.62\n",
      "Epoch: 5 .. batch: 114/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.47 .. NELBO: 302.97\n",
      "Epoch: 5 .. batch: 116/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.69 .. NELBO: 303.19\n",
      "Epoch: 5 .. batch: 118/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.9 .. NELBO: 303.4\n",
      "Epoch: 5 .. batch: 120/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.97 .. NELBO: 303.47\n",
      "Epoch: 5 .. batch: 122/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.82 .. NELBO: 303.32\n",
      "Epoch: 5 .. batch: 124/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.79 .. NELBO: 303.29\n",
      "Epoch: 5 .. batch: 126/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.96 .. NELBO: 303.46\n",
      "Epoch: 5 .. batch: 128/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.9 .. NELBO: 303.4\n",
      "Epoch: 5 .. batch: 130/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.22 .. NELBO: 303.72\n",
      "Epoch: 5 .. batch: 132/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.22 .. NELBO: 303.72\n",
      "Epoch: 5 .. batch: 134/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.23 .. NELBO: 303.73\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.23 .. NELBO: 303.73\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 887.9\n",
      "****************************************************************************************************\n",
      "####################################################################################################\n",
      "Visualize topics...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['fogwas', 'quotdoctor', 'postmeal', 'chillstripled', 'laydown', 'yearstried', 'upscale', 'buckswhich', 'wellbutrine']\n",
      "Topic 1: ['chillstripled', 'fogwas', 'postmeal', 'quotdoctor', 'upscale', 'laydown', 'yearstried', 'wellbutrine', 'buckswhich']\n",
      "Topic 2: ['fogwas', 'quotdoctor', 'postmeal', 'laydown', 'chillstripled', 'yearstried', 'upscale', 'wellbutrine', 'buckswhich']\n",
      "Topic 3: ['quotdoctor', 'chillstripled', 'yearstried', 'postmeal', 'fogwas', 'laydown', 'upscale', 'buckswhich', 'gi']\n",
      "Topic 4: ['quotdoctor', 'chillstripled', 'postmeal', 'fogwas', 'buckswhich', 'yearstried', 'laydown', 'upscale', 'wellbutrine']\n",
      "Topic 5: ['quotdoctor', 'chillstripled', 'postmeal', 'fogwas', 'laydown', 'yearstried', 'upscale', 'buckswhich', 'gi']\n",
      "Topic 6: ['period', 'gain', 'swing', 'weight', 'acne', 'sex', 'bleeding', 'birth', 'spot']\n",
      "Topic 7: ['quotdoctor', 'postmeal', 'chillstripled', 'fogwas', 'laydown', 'upscale', 'wellbutrine', 'yearstried', 'buckswhich']\n",
      "Topic 8: ['quotdoctor', 'yearstried', 'postmeal', 'upscale', 'chillstripled', 'laydown', 'fogwas', 'buckswhich', 'wellbutrine']\n",
      "Topic 9: ['quotdoctor', 'fogwas', 'yearstried', 'chillstripled', 'postmeal', 'laydown', 'upscale', 'buckswhich', 'wellbutrine']\n",
      "Topic 10: ['quotdoctor', 'postmeal', 'fogwas', 'laydown', 'yearstried', 'chillstripled', 'upscale', 'wellbutrine', 'buckswhich']\n",
      "Topic 11: ['chillstripled', 'quotdoctor', 'postmeal', 'yearstried', 'laydown', 'fogwas', 'buckswhich', 'upscale', 'wellbutrine']\n",
      "Topic 12: ['quotdoctor', 'chillstripled', 'postmeal', 'fogwas', 'yearstried', 'laydown', 'wellbutrine', 'upscale', 'angst']\n",
      "Topic 13: ['chillstripled', 'fogwas', 'quotdoctor', 'postmeal', 'laydown', 'yearstried', 'upscale', 'wellbutrine', 'buckswhich']\n",
      "Topic 14: ['fogwas', 'postmeal', 'quotdoctor', 'chillstripled', 'laydown', 'yearstried', 'upscale', 'wellbutrine', 'buckswhich']\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: skin .. neighbors: ['skin', 'face', 'clear', 'acne', 'sunscreen', 'breakout', 'pimple', 'peel', 'flake', 'redness', 'epiduo', 'retin', 'papule', 'ziana', 'oily', 'neutrogena', 'clinique', 'acanya', 'complexion', 'moisturiser']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cycle .. neighbors: ['cycle', 'period', 'menstrual', 'lighter', 'safyral', 'month', 'bleeding', 'menstruate', 'hormone', 'cramp', 'heavy', 'tracker', 'predictable', 'natazia', 'amethia', 'spot', 'bleed', 'mononessa', 'heavier', 'pill']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: price .. neighbors: ['price', 'pay', 'retail', 'formulary', 'insurance', 'paid', 'fee', 'dollar', 'usa', 'tag', 'pocket', 'citizen', 'drugstore', 'astronomical', 'restasis', 'coupon', 'astonish', 'cheap', 'pollen', 'copays']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: best .. neighbors: ['best', 'try', 'good', 'great', 'work', 'bet', 'take', 'sacroiliac', 'ive', 'methe', 'tricyclic', 'help', 'youth', 'adherence', 'knowledgeable', 'sceptical', 'astelin', 'different', 'psychology', 'pharmacy']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: efficacy .. neighbors: ['efficacy', 'generess', 'buti', 'version', 'maintenance', 'unisom', 'weighti', 'threshold', 'simponi', 'approval', 'reluctantly', 'sufficient', 'psoriatic', 'accustom', 'national', 'intolerable', 'ssrisnris', 'fee', 'attest', 'pdoc']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: performance .. neighbors: ['performance', 'sexual', 'cialis', 'bph', 'semen', 'erection', 'ratio', 'distraction', 'climax', 'avodart', 'stendra', 'viagra', 'benign', 'smarter', 'elate', 'stamen', 'prostate', 'anorgasmia', 'marriage', 'adhdadd']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cancer .. neighbors: ['cancer', 'ibrance', 'radiation', 'chemo', 'lymph', 'node', 'tarceva', 'prostate', 'zoladex', 'femara', 'oncologist', 'chemotherapy', 'leukemia', 'sutent', 'alfuzosin', 'carcinoma', 'arimidex', 'casodex', 'psa', 'avastin']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: disease .. neighbors: ['disease', 'degenerative', 'crohn', 'autoimmune', 'lyme', 'obstructive', 'herniated', 'diagnose', 'fistula', 'classify', 'lumbar', 'hashimotos', 'disc', 'remicade', 'currently', 'inflammatory', 'ankylose', 'kapidex', 'herniation', 'leukemia']\n",
      "####################################################################################################\n",
      "Epoch: 6 .. batch: 2/135 .. LR: 0.005 .. KL_theta: 0.57 .. Rec_loss: 308.26 .. NELBO: 308.83\n",
      "Epoch: 6 .. batch: 4/135 .. LR: 0.005 .. KL_theta: 0.55 .. Rec_loss: 303.88 .. NELBO: 304.43\n",
      "Epoch: 6 .. batch: 6/135 .. LR: 0.005 .. KL_theta: 0.57 .. Rec_loss: 303.8 .. NELBO: 304.37\n",
      "Epoch: 6 .. batch: 8/135 .. LR: 0.005 .. KL_theta: 0.55 .. Rec_loss: 304.58 .. NELBO: 305.13\n",
      "Epoch: 6 .. batch: 10/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.44 .. NELBO: 303.97\n",
      "Epoch: 6 .. batch: 12/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 305.43 .. NELBO: 305.95\n",
      "Epoch: 6 .. batch: 14/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 306.98 .. NELBO: 307.48\n",
      "Epoch: 6 .. batch: 16/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 306.85 .. NELBO: 307.35\n",
      "Epoch: 6 .. batch: 18/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 306.39 .. NELBO: 306.89\n",
      "Epoch: 6 .. batch: 20/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.78 .. NELBO: 306.28\n",
      "Epoch: 6 .. batch: 22/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 307.05 .. NELBO: 307.56\n",
      "Epoch: 6 .. batch: 24/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 307.68 .. NELBO: 308.19\n",
      "Epoch: 6 .. batch: 26/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 309.4 .. NELBO: 309.91\n",
      "Epoch: 6 .. batch: 28/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 309.72 .. NELBO: 310.23\n",
      "Epoch: 6 .. batch: 30/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 309.37 .. NELBO: 309.88\n",
      "Epoch: 6 .. batch: 32/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 308.58 .. NELBO: 309.08\n",
      "Epoch: 6 .. batch: 34/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 308.79 .. NELBO: 309.29\n",
      "Epoch: 6 .. batch: 36/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 308.58 .. NELBO: 309.08\n",
      "Epoch: 6 .. batch: 38/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 308.04 .. NELBO: 308.54\n",
      "Epoch: 6 .. batch: 40/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 307.26 .. NELBO: 307.76\n",
      "Epoch: 6 .. batch: 42/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 307.56 .. NELBO: 308.06\n",
      "Epoch: 6 .. batch: 44/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 306.48 .. NELBO: 306.98\n",
      "Epoch: 6 .. batch: 46/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 306.45 .. NELBO: 306.95\n",
      "Epoch: 6 .. batch: 48/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 306.48 .. NELBO: 306.98\n",
      "Epoch: 6 .. batch: 50/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 306.43 .. NELBO: 306.93\n",
      "Epoch: 6 .. batch: 52/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.96 .. NELBO: 306.46\n",
      "Epoch: 6 .. batch: 54/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 306.01 .. NELBO: 306.52\n",
      "Epoch: 6 .. batch: 56/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 305.5 .. NELBO: 306.01\n",
      "Epoch: 6 .. batch: 58/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.35 .. NELBO: 305.85\n",
      "Epoch: 6 .. batch: 60/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 304.8 .. NELBO: 305.3\n",
      "Epoch: 6 .. batch: 62/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.64 .. NELBO: 306.14\n",
      "Epoch: 6 .. batch: 64/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.72 .. NELBO: 306.22\n",
      "Epoch: 6 .. batch: 66/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.54 .. NELBO: 306.04\n",
      "Epoch: 6 .. batch: 68/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.35 .. NELBO: 305.85\n",
      "Epoch: 6 .. batch: 70/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.38 .. NELBO: 305.88\n",
      "Epoch: 6 .. batch: 72/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.05 .. NELBO: 305.55\n",
      "Epoch: 6 .. batch: 74/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.4 .. NELBO: 305.9\n",
      "Epoch: 6 .. batch: 76/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.44 .. NELBO: 305.94\n",
      "Epoch: 6 .. batch: 78/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.34 .. NELBO: 305.84\n",
      "Epoch: 6 .. batch: 80/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.58 .. NELBO: 306.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 .. batch: 82/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.55 .. NELBO: 306.05\n",
      "Epoch: 6 .. batch: 84/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.68 .. NELBO: 306.18\n",
      "Epoch: 6 .. batch: 86/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.12 .. NELBO: 302.62\n",
      "Epoch: 6 .. batch: 88/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.0 .. NELBO: 302.5\n",
      "Epoch: 6 .. batch: 90/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.87 .. NELBO: 302.37\n",
      "Epoch: 6 .. batch: 92/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.86 .. NELBO: 302.36\n",
      "Epoch: 6 .. batch: 94/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.71 .. NELBO: 302.21\n",
      "Epoch: 6 .. batch: 96/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.62 .. NELBO: 302.12\n",
      "Epoch: 6 .. batch: 98/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.79 .. NELBO: 302.29\n",
      "Epoch: 6 .. batch: 100/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.39 .. NELBO: 301.89\n",
      "Epoch: 6 .. batch: 102/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.36 .. NELBO: 301.86\n",
      "Epoch: 6 .. batch: 104/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.5 .. NELBO: 302.0\n",
      "Epoch: 6 .. batch: 106/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 301.86 .. NELBO: 302.36\n",
      "Epoch: 6 .. batch: 108/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.02 .. NELBO: 302.52\n",
      "Epoch: 6 .. batch: 110/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.46 .. NELBO: 302.96\n",
      "Epoch: 6 .. batch: 112/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.76 .. NELBO: 303.26\n",
      "Epoch: 6 .. batch: 114/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.3 .. NELBO: 303.8\n",
      "Epoch: 6 .. batch: 116/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.54 .. NELBO: 304.04\n",
      "Epoch: 6 .. batch: 118/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.39 .. NELBO: 303.89\n",
      "Epoch: 6 .. batch: 120/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.68 .. NELBO: 304.18\n",
      "Epoch: 6 .. batch: 122/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.45 .. NELBO: 303.95\n",
      "Epoch: 6 .. batch: 124/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.45 .. NELBO: 303.95\n",
      "Epoch: 6 .. batch: 126/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.44 .. NELBO: 303.94\n",
      "Epoch: 6 .. batch: 128/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.22 .. NELBO: 303.72\n",
      "Epoch: 6 .. batch: 130/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.3 .. NELBO: 303.8\n",
      "Epoch: 6 .. batch: 132/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.13 .. NELBO: 303.63\n",
      "Epoch: 6 .. batch: 134/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.06 .. NELBO: 303.56\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 303.06 .. NELBO: 303.56\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 882.6\n",
      "****************************************************************************************************\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0: ['laydown', 'chillstripled', 'postmeal', 'yearstried', 'fogwas', 'quotdoctor', 'buckswhich', 'wellbutrine', 'angst']\n",
      "Topic 1: ['postmeal', 'quotdoctor', 'chillstripled', 'fogwas', 'wellbutrine', 'laydown', 'buckswhich', 'resultsplaques', 'upscale']\n",
      "Topic 2: ['quotdoctor', 'postmeal', 'chillstripled', 'fogwas', 'yearstried', 'wellbutrine', 'buckswhich', 'angst', 'laydown']\n",
      "Topic 3: ['quotdoctor', 'postmeal', 'chillstripled', 'laydown', 'fogwas', 'yearstried', 'wellbutrine', 'buckswhich', 'upscale']\n",
      "Topic 4: ['chillstripled', 'postmeal', 'fogwas', 'quotdoctor', 'upscale', 'wellbutrine', 'laydown', 'yearstried', 'buckswhich']\n",
      "Topic 5: ['postmeal', 'quotdoctor', 'fogwas', 'chillstripled', 'laydown', 'wellbutrine', 'yearstried', 'gi', 'buckswhich']\n",
      "Topic 6: ['period', 'gain', 'weight', 'swing', 'acne', 'sex', 'birth', 'bleeding', 'spot']\n",
      "Topic 7: ['fogwas', 'chillstripled', 'postmeal', 'laydown', 'yearstried', 'quotdoctor', 'buckswhich', 'wellbutrine', 'resultsplaques']\n",
      "Topic 8: ['chillstripled', 'postmeal', 'quotdoctor', 'wellbutrine', 'fogwas', 'laydown', 'gi', 'yearstried', 'angst']\n",
      "Topic 9: ['quotdoctor', 'postmeal', 'chillstripled', 'laydown', 'fogwas', 'wellbutrine', 'yearstried', 'angst', 'resultsplaques']\n",
      "Topic 10: ['chillstripled', 'postmeal', 'laydown', 'yearstried', 'quotdoctor', 'fogwas', 'valiumis', 'wellbutrine', 'buckswhich']\n",
      "Topic 11: ['postmeal', 'quotdoctor', 'chillstripled', 'laydown', 'fogwas', 'yearstried', 'wellbutrine', 'buckswhich', 'quotmyquot']\n",
      "Topic 12: ['quotdoctor', 'postmeal', 'chillstripled', 'fogwas', 'wellbutrine', 'laydown', 'upscale', 'buckswhich', 'resultsplaques']\n",
      "Topic 13: ['postmeal', 'laydown', 'chillstripled', 'quotdoctor', 'yearstried', 'fogwas', 'wellbutrine', 'buckswhich', 'angst']\n",
      "Topic 14: ['quotdoctor', 'postmeal', 'chillstripled', 'laydown', 'fogwas', 'yearstried', 'wellbutrine', 'buckswhich', 'ensures']\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: skin .. neighbors: ['skin', 'face', 'clear', 'acne', 'sunscreen', 'breakout', 'pimple', 'peel', 'flake', 'redness', 'epiduo', 'retin', 'papule', 'ziana', 'oily', 'neutrogena', 'clinique', 'acanya', 'complexion', 'moisturiser']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cycle .. neighbors: ['cycle', 'period', 'menstrual', 'lighter', 'safyral', 'month', 'bleeding', 'menstruate', 'hormone', 'cramp', 'heavy', 'tracker', 'predictable', 'natazia', 'amethia', 'spot', 'bleed', 'mononessa', 'heavier', 'pill']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: price .. neighbors: ['price', 'pay', 'retail', 'formulary', 'insurance', 'paid', 'fee', 'dollar', 'usa', 'tag', 'pocket', 'citizen', 'drugstore', 'astronomical', 'restasis', 'coupon', 'astonish', 'cheap', 'pollen', 'copays']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: best .. neighbors: ['best', 'try', 'good', 'great', 'work', 'bet', 'take', 'sacroiliac', 'ive', 'methe', 'tricyclic', 'help', 'youth', 'adherence', 'knowledgeable', 'sceptical', 'astelin', 'different', 'psychology', 'pharmacy']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: efficacy .. neighbors: ['efficacy', 'generess', 'buti', 'version', 'maintenance', 'unisom', 'weighti', 'threshold', 'simponi', 'approval', 'reluctantly', 'sufficient', 'psoriatic', 'accustom', 'national', 'intolerable', 'ssrisnris', 'fee', 'attest', 'pdoc']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: performance .. neighbors: ['performance', 'sexual', 'cialis', 'bph', 'semen', 'erection', 'ratio', 'distraction', 'climax', 'avodart', 'stendra', 'viagra', 'benign', 'smarter', 'elate', 'stamen', 'prostate', 'anorgasmia', 'marriage', 'adhdadd']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cancer .. neighbors: ['cancer', 'ibrance', 'radiation', 'chemo', 'lymph', 'node', 'tarceva', 'prostate', 'zoladex', 'femara', 'oncologist', 'chemotherapy', 'leukemia', 'sutent', 'alfuzosin', 'carcinoma', 'arimidex', 'casodex', 'psa', 'avastin']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: disease .. neighbors: ['disease', 'degenerative', 'crohn', 'autoimmune', 'lyme', 'obstructive', 'herniated', 'diagnose', 'fistula', 'classify', 'lumbar', 'hashimotos', 'disc', 'remicade', 'currently', 'inflammatory', 'ankylose', 'kapidex', 'herniation', 'leukemia']\n",
      "####################################################################################################\n",
      "Epoch: 7 .. batch: 2/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 310.47 .. NELBO: 310.97\n",
      "Epoch: 7 .. batch: 4/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 305.22 .. NELBO: 305.74\n",
      "Epoch: 7 .. batch: 6/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 297.32 .. NELBO: 297.83\n",
      "Epoch: 7 .. batch: 8/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 303.56 .. NELBO: 304.07\n",
      "Epoch: 7 .. batch: 10/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 305.03 .. NELBO: 305.54\n",
      "Epoch: 7 .. batch: 12/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.64 .. NELBO: 303.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 .. batch: 14/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 305.49 .. NELBO: 306.0\n",
      "Epoch: 7 .. batch: 16/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 304.11 .. NELBO: 304.63\n",
      "Epoch: 7 .. batch: 18/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 302.91 .. NELBO: 303.43\n",
      "Epoch: 7 .. batch: 20/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.46 .. NELBO: 303.99\n",
      "Epoch: 7 .. batch: 22/135 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 303.19 .. NELBO: 303.72\n",
      "Epoch: 7 .. batch: 24/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 304.07 .. NELBO: 304.59\n",
      "Epoch: 7 .. batch: 26/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 306.51 .. NELBO: 307.03\n",
      "Epoch: 7 .. batch: 28/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 305.25 .. NELBO: 305.76\n",
      "Epoch: 7 .. batch: 30/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 304.14 .. NELBO: 304.64\n",
      "Epoch: 7 .. batch: 32/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 304.52 .. NELBO: 305.02\n",
      "Epoch: 7 .. batch: 34/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 305.2 .. NELBO: 305.71\n",
      "Epoch: 7 .. batch: 36/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 296.56 .. NELBO: 297.05\n",
      "Epoch: 7 .. batch: 38/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 297.34 .. NELBO: 297.83\n",
      "Epoch: 7 .. batch: 40/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 297.35 .. NELBO: 297.84\n",
      "Epoch: 7 .. batch: 42/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 297.03 .. NELBO: 297.52\n",
      "Epoch: 7 .. batch: 44/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 297.29 .. NELBO: 297.78\n",
      "Epoch: 7 .. batch: 46/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 297.32 .. NELBO: 297.81\n",
      "Epoch: 7 .. batch: 48/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 297.11 .. NELBO: 297.6\n",
      "Epoch: 7 .. batch: 50/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 296.56 .. NELBO: 297.05\n",
      "Epoch: 7 .. batch: 52/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 296.47 .. NELBO: 296.96\n",
      "Epoch: 7 .. batch: 54/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 296.74 .. NELBO: 297.24\n",
      "Epoch: 7 .. batch: 56/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 296.96 .. NELBO: 297.46\n",
      "Epoch: 7 .. batch: 58/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 297.32 .. NELBO: 297.81\n",
      "Epoch: 7 .. batch: 60/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 297.79 .. NELBO: 298.28\n",
      "Epoch: 7 .. batch: 62/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 298.34 .. NELBO: 298.83\n",
      "Epoch: 7 .. batch: 64/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 299.3 .. NELBO: 299.79\n",
      "Epoch: 7 .. batch: 66/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 299.33 .. NELBO: 299.82\n",
      "Epoch: 7 .. batch: 68/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 299.11 .. NELBO: 299.61\n",
      "Epoch: 7 .. batch: 70/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 299.96 .. NELBO: 300.46\n",
      "Epoch: 7 .. batch: 72/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 299.89 .. NELBO: 300.39\n",
      "Epoch: 7 .. batch: 74/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 300.59 .. NELBO: 301.08\n",
      "Epoch: 7 .. batch: 76/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 300.48 .. NELBO: 300.97\n",
      "Epoch: 7 .. batch: 78/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 300.82 .. NELBO: 301.31\n",
      "Epoch: 7 .. batch: 80/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 300.84 .. NELBO: 301.33\n",
      "Epoch: 7 .. batch: 82/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 301.26 .. NELBO: 301.75\n",
      "Epoch: 7 .. batch: 84/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 301.54 .. NELBO: 302.03\n",
      "Epoch: 7 .. batch: 86/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 301.71 .. NELBO: 302.2\n",
      "Epoch: 7 .. batch: 88/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.38 .. NELBO: 302.87\n",
      "Epoch: 7 .. batch: 90/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.07 .. NELBO: 302.56\n",
      "Epoch: 7 .. batch: 92/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.08 .. NELBO: 302.57\n",
      "Epoch: 7 .. batch: 94/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.21 .. NELBO: 302.7\n",
      "Epoch: 7 .. batch: 96/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 302.41 .. NELBO: 302.89\n",
      "Epoch: 7 .. batch: 98/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 302.14 .. NELBO: 302.62\n",
      "Epoch: 7 .. batch: 100/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 301.97 .. NELBO: 302.45\n",
      "Epoch: 7 .. batch: 102/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.34 .. NELBO: 302.83\n",
      "Epoch: 7 .. batch: 104/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.04 .. NELBO: 302.53\n",
      "Epoch: 7 .. batch: 106/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.14 .. NELBO: 302.63\n",
      "Epoch: 7 .. batch: 108/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.19 .. NELBO: 302.68\n",
      "Epoch: 7 .. batch: 110/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.03 .. NELBO: 302.52\n",
      "Epoch: 7 .. batch: 112/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.12 .. NELBO: 302.61\n",
      "Epoch: 7 .. batch: 114/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 302.21 .. NELBO: 302.69\n",
      "Epoch: 7 .. batch: 116/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 302.07 .. NELBO: 302.55\n",
      "Epoch: 7 .. batch: 118/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 302.32 .. NELBO: 302.8\n",
      "Epoch: 7 .. batch: 120/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 302.2 .. NELBO: 302.68\n",
      "Epoch: 7 .. batch: 122/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 302.26 .. NELBO: 302.74\n",
      "Epoch: 7 .. batch: 124/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.41 .. NELBO: 302.9\n",
      "Epoch: 7 .. batch: 126/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.56 .. NELBO: 303.05\n",
      "Epoch: 7 .. batch: 128/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.62 .. NELBO: 303.11\n",
      "Epoch: 7 .. batch: 130/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.7 .. NELBO: 303.19\n",
      "Epoch: 7 .. batch: 132/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.68 .. NELBO: 303.17\n",
      "Epoch: 7 .. batch: 134/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.85 .. NELBO: 303.34\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.85 .. NELBO: 303.34\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 879.9\n",
      "****************************************************************************************************\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0: ['quotdoctor', 'laydown', 'chillstripled', 'postmeal', 'fogwas', 'buckswhich', 'yearstried', 'resultsplaques', 'upscale']\n",
      "Topic 1: ['quotdoctor', 'postmeal', 'chillstripled', 'laydown', 'fogwas', 'upscale', 'yearstried', 'wellbutrine', 'angst']\n",
      "Topic 2: ['postmeal', 'fogwas', 'quotdoctor', 'laydown', 'buckswhich', 'chillstripled', 'yearstried', 'wellbutrine', 'upscale']\n",
      "Topic 3: ['quotdoctor', 'postmeal', 'chillstripled', 'fogwas', 'laydown', 'wellbutrine', 'yearstried', 'buckswhich', 'upscale']\n",
      "Topic 4: ['postmeal', 'quotdoctor', 'buckswhich', 'laydown', 'chillstripled', 'upscale', 'fogwas', 'wellbutrine', 'yearstried']\n",
      "Topic 5: ['postmeal', 'fogwas', 'quotdoctor', 'chillstripled', 'laydown', 'wellbutrine', 'buckswhich', 'upscale', 'yearstried']\n",
      "Topic 6: ['period', 'gain', 'weight', 'acne', 'swing', 'cramp', 'birth', 'bleeding', 'sex']\n",
      "Topic 7: ['quotdoctor', 'postmeal', 'chillstripled', 'buckswhich', 'wellbutrine', 'upscale', 'laydown', 'yearstried', 'fogwas']\n",
      "Topic 8: ['postmeal', 'quotdoctor', 'laydown', 'fogwas', 'wellbutrine', 'chillstripled', 'buckswhich', 'upscale', 'yearstried']\n",
      "Topic 9: ['quotdoctor', 'postmeal', 'chillstripled', 'laydown', 'fogwas', 'wellbutrine', 'yearstried', 'upscale', 'buckswhich']\n",
      "Topic 10: ['quotdoctor', 'postmeal', 'laydown', 'chillstripled', 'fogwas', 'yearstried', 'wellbutrine', 'upscale', 'buckswhich']\n",
      "Topic 11: ['quotdoctor', 'postmeal', 'chillstripled', 'laydown', 'fogwas', 'buckswhich', 'wellbutrine', 'yearstried', 'valiumis']\n",
      "Topic 12: ['quotdoctor', 'postmeal', 'chillstripled', 'fogwas', 'yearstried', 'wellbutrine', 'laydown', 'upscale', 'buckswhich']\n",
      "Topic 13: ['postmeal', 'quotdoctor', 'chillstripled', 'laydown', 'fogwas', 'yearstried', 'wellbutrine', 'buckswhich', 'upscale']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 14: ['quotdoctor', 'postmeal', 'laydown', 'fogwas', 'buckswhich', 'chillstripled', 'upscale', 'wellbutrine', 'yearstried']\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: skin .. neighbors: ['skin', 'face', 'clear', 'acne', 'sunscreen', 'breakout', 'pimple', 'peel', 'flake', 'redness', 'epiduo', 'retin', 'papule', 'ziana', 'oily', 'neutrogena', 'clinique', 'acanya', 'complexion', 'moisturiser']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cycle .. neighbors: ['cycle', 'period', 'menstrual', 'lighter', 'safyral', 'month', 'bleeding', 'menstruate', 'hormone', 'cramp', 'heavy', 'tracker', 'predictable', 'natazia', 'amethia', 'spot', 'bleed', 'mononessa', 'heavier', 'pill']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: price .. neighbors: ['price', 'pay', 'retail', 'formulary', 'insurance', 'paid', 'fee', 'dollar', 'usa', 'tag', 'pocket', 'citizen', 'drugstore', 'astronomical', 'restasis', 'coupon', 'astonish', 'cheap', 'pollen', 'copays']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: best .. neighbors: ['best', 'try', 'good', 'great', 'work', 'bet', 'take', 'sacroiliac', 'ive', 'methe', 'tricyclic', 'help', 'youth', 'adherence', 'knowledgeable', 'sceptical', 'astelin', 'different', 'psychology', 'pharmacy']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: efficacy .. neighbors: ['efficacy', 'generess', 'buti', 'version', 'maintenance', 'unisom', 'weighti', 'threshold', 'simponi', 'approval', 'reluctantly', 'sufficient', 'psoriatic', 'accustom', 'national', 'intolerable', 'ssrisnris', 'fee', 'attest', 'pdoc']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: performance .. neighbors: ['performance', 'sexual', 'cialis', 'bph', 'semen', 'erection', 'ratio', 'distraction', 'climax', 'avodart', 'stendra', 'viagra', 'benign', 'smarter', 'elate', 'stamen', 'prostate', 'anorgasmia', 'marriage', 'adhdadd']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cancer .. neighbors: ['cancer', 'ibrance', 'radiation', 'chemo', 'lymph', 'node', 'tarceva', 'prostate', 'zoladex', 'femara', 'oncologist', 'chemotherapy', 'leukemia', 'sutent', 'alfuzosin', 'carcinoma', 'arimidex', 'casodex', 'psa', 'avastin']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: disease .. neighbors: ['disease', 'degenerative', 'crohn', 'autoimmune', 'lyme', 'obstructive', 'herniated', 'diagnose', 'fistula', 'classify', 'lumbar', 'hashimotos', 'disc', 'remicade', 'currently', 'inflammatory', 'ankylose', 'kapidex', 'herniation', 'leukemia']\n",
      "####################################################################################################\n",
      "Epoch: 8 .. batch: 2/135 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 291.11 .. NELBO: 291.58\n",
      "Epoch: 8 .. batch: 4/135 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 295.69 .. NELBO: 296.16\n",
      "Epoch: 8 .. batch: 6/135 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 295.26 .. NELBO: 295.73\n",
      "Epoch: 8 .. batch: 8/135 .. LR: 0.005 .. KL_theta: 0.42 .. Rec_loss: 262.02 .. NELBO: 262.44\n",
      "Epoch: 8 .. batch: 10/135 .. LR: 0.005 .. KL_theta: 0.44 .. Rec_loss: 272.58 .. NELBO: 273.02\n",
      "Epoch: 8 .. batch: 12/135 .. LR: 0.005 .. KL_theta: 0.44 .. Rec_loss: 277.44 .. NELBO: 277.88\n",
      "Epoch: 8 .. batch: 14/135 .. LR: 0.005 .. KL_theta: 0.46 .. Rec_loss: 283.4 .. NELBO: 283.86\n",
      "Epoch: 8 .. batch: 16/135 .. LR: 0.005 .. KL_theta: 0.46 .. Rec_loss: 283.57 .. NELBO: 284.03\n",
      "Epoch: 8 .. batch: 18/135 .. LR: 0.005 .. KL_theta: 0.46 .. Rec_loss: 286.64 .. NELBO: 287.1\n",
      "Epoch: 8 .. batch: 20/135 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 287.25 .. NELBO: 287.72\n",
      "Epoch: 8 .. batch: 22/135 .. LR: 0.005 .. KL_theta: 0.46 .. Rec_loss: 288.91 .. NELBO: 289.37\n",
      "Epoch: 8 .. batch: 24/135 .. LR: 0.005 .. KL_theta: 0.46 .. Rec_loss: 290.83 .. NELBO: 291.29\n",
      "Epoch: 8 .. batch: 26/135 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 292.57 .. NELBO: 293.04\n",
      "Epoch: 8 .. batch: 28/135 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 293.63 .. NELBO: 294.1\n",
      "Epoch: 8 .. batch: 30/135 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 293.36 .. NELBO: 293.83\n",
      "Epoch: 8 .. batch: 32/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 293.81 .. NELBO: 294.29\n",
      "Epoch: 8 .. batch: 34/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 294.3 .. NELBO: 294.78\n",
      "Epoch: 8 .. batch: 36/135 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 294.64 .. NELBO: 295.11\n",
      "Epoch: 8 .. batch: 38/135 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 295.62 .. NELBO: 296.09\n",
      "Epoch: 8 .. batch: 40/135 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 296.1 .. NELBO: 296.57\n",
      "Epoch: 8 .. batch: 42/135 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 296.5 .. NELBO: 296.97\n",
      "Epoch: 8 .. batch: 44/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 296.56 .. NELBO: 297.04\n",
      "Epoch: 8 .. batch: 46/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 296.8 .. NELBO: 297.28\n",
      "Epoch: 8 .. batch: 48/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 296.19 .. NELBO: 296.67\n",
      "Epoch: 8 .. batch: 50/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 296.65 .. NELBO: 297.14\n",
      "Epoch: 8 .. batch: 52/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 296.73 .. NELBO: 297.22\n",
      "Epoch: 8 .. batch: 54/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 296.26 .. NELBO: 296.74\n",
      "Epoch: 8 .. batch: 56/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 295.92 .. NELBO: 296.4\n",
      "Epoch: 8 .. batch: 58/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 296.35 .. NELBO: 296.83\n",
      "Epoch: 8 .. batch: 60/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 296.8 .. NELBO: 297.28\n",
      "Epoch: 8 .. batch: 62/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 296.53 .. NELBO: 297.01\n",
      "Epoch: 8 .. batch: 64/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 297.06 .. NELBO: 297.54\n",
      "Epoch: 8 .. batch: 66/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 297.67 .. NELBO: 298.15\n",
      "Epoch: 8 .. batch: 68/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 298.0 .. NELBO: 298.48\n",
      "Epoch: 8 .. batch: 70/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 298.04 .. NELBO: 298.52\n",
      "Epoch: 8 .. batch: 72/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 297.85 .. NELBO: 298.33\n",
      "Epoch: 8 .. batch: 74/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 298.48 .. NELBO: 298.96\n",
      "Epoch: 8 .. batch: 76/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 298.36 .. NELBO: 298.84\n",
      "Epoch: 8 .. batch: 78/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 298.67 .. NELBO: 299.15\n",
      "Epoch: 8 .. batch: 80/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 298.4 .. NELBO: 298.88\n",
      "Epoch: 8 .. batch: 82/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 298.61 .. NELBO: 299.09\n",
      "Epoch: 8 .. batch: 84/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 298.5 .. NELBO: 298.99\n",
      "Epoch: 8 .. batch: 86/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 299.01 .. NELBO: 299.5\n",
      "Epoch: 8 .. batch: 88/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 299.75 .. NELBO: 300.24\n",
      "Epoch: 8 .. batch: 90/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 300.09 .. NELBO: 300.58\n",
      "Epoch: 8 .. batch: 92/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 300.28 .. NELBO: 300.77\n",
      "Epoch: 8 .. batch: 94/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 300.64 .. NELBO: 301.13\n",
      "Epoch: 8 .. batch: 96/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 300.99 .. NELBO: 301.48\n",
      "Epoch: 8 .. batch: 98/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 301.15 .. NELBO: 301.64\n",
      "Epoch: 8 .. batch: 100/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 301.36 .. NELBO: 301.85\n",
      "Epoch: 8 .. batch: 102/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 300.71 .. NELBO: 301.2\n",
      "Epoch: 8 .. batch: 104/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 300.68 .. NELBO: 301.17\n",
      "Epoch: 8 .. batch: 106/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 300.85 .. NELBO: 301.34\n",
      "Epoch: 8 .. batch: 108/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 301.03 .. NELBO: 301.52\n",
      "Epoch: 8 .. batch: 110/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 301.13 .. NELBO: 301.62\n",
      "Epoch: 8 .. batch: 112/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 301.6 .. NELBO: 302.09\n",
      "Epoch: 8 .. batch: 114/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 301.82 .. NELBO: 302.31\n",
      "Epoch: 8 .. batch: 116/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 301.84 .. NELBO: 302.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 .. batch: 118/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 301.97 .. NELBO: 302.46\n",
      "Epoch: 8 .. batch: 120/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.13 .. NELBO: 302.62\n",
      "Epoch: 8 .. batch: 122/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.16 .. NELBO: 302.65\n",
      "Epoch: 8 .. batch: 124/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 301.99 .. NELBO: 302.48\n",
      "Epoch: 8 .. batch: 126/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 301.95 .. NELBO: 302.44\n",
      "Epoch: 8 .. batch: 128/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 301.89 .. NELBO: 302.38\n",
      "Epoch: 8 .. batch: 130/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.21 .. NELBO: 302.7\n",
      "Epoch: 8 .. batch: 132/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.81 .. NELBO: 303.3\n",
      "Epoch: 8 .. batch: 134/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.83 .. NELBO: 303.32\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 302.83 .. NELBO: 303.32\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 878.2\n",
      "****************************************************************************************************\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0: ['fogwas', 'chillstripled', 'postmeal', 'quotdoctor', 'wellbutrine', 'laydown', 'yearstried', 'upscale', 'buckswhich']\n",
      "Topic 1: ['quotdoctor', 'postmeal', 'fogwas', 'chillstripled', 'buckswhich', 'wellbutrine', 'yearstried', 'valiumis', 'laydown']\n",
      "Topic 2: ['quotdoctor', 'postmeal', 'fogwas', 'chillstripled', 'wellbutrine', 'laydown', 'yearstried', 'valiumis', 'quotmyquot']\n",
      "Topic 3: ['quotdoctor', 'postmeal', 'fogwas', 'chillstripled', 'yearstried', 'wellbutrine', 'laydown', 'quotmyquot', 'upscale']\n",
      "Topic 4: ['postmeal', 'quotdoctor', 'fogwas', 'wellbutrine', 'chillstripled', 'yearstried', 'buckswhich', 'laydown', 'quotmyquot']\n",
      "Topic 5: ['postmeal', 'quotdoctor', 'fogwas', 'chillstripled', 'laydown', 'wellbutrine', 'yearstried', 'valiumis', 'upscale']\n",
      "Topic 6: ['period', 'gain', 'weight', 'acne', 'cramp', 'birth', 'swing', 'bleeding', 'sex']\n",
      "Topic 7: ['quotdoctor', 'postmeal', 'wellbutrine', 'chillstripled', 'fogwas', 'laydown', 'upscale', 'yearstried', 'valiumis']\n",
      "Topic 8: ['postmeal', 'chillstripled', 'wellbutrine', 'laydown', 'quotdoctor', 'fogwas', 'yearstried', 'quotmyquot', 'buckswhich']\n",
      "Topic 9: ['quotdoctor', 'postmeal', 'chillstripled', 'fogwas', 'yearstried', 'wellbutrine', 'upscale', 'laydown', 'buckswhich']\n",
      "Topic 10: ['postmeal', 'quotdoctor', 'chillstripled', 'fogwas', 'wellbutrine', 'yearstried', 'upscale', 'laydown', 'valiumis']\n",
      "Topic 11: ['postmeal', 'quotdoctor', 'chillstripled', 'fogwas', 'laydown', 'yearstried', 'wellbutrine', 'upscale', 'valiumis']\n",
      "Topic 12: ['postmeal', 'chillstripled', 'quotdoctor', 'wellbutrine', 'yearstried', 'fogwas', 'laydown', 'upscale', 'buckswhich']\n",
      "Topic 13: ['postmeal', 'quotdoctor', 'fogwas', 'chillstripled', 'laydown', 'wellbutrine', 'yearstried', 'upscale', 'buckswhich']\n",
      "Topic 14: ['postmeal', 'quotdoctor', 'chillstripled', 'wellbutrine', 'yearstried', 'laydown', 'fogwas', 'buckswhich', 'valiumis']\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: skin .. neighbors: ['skin', 'face', 'clear', 'acne', 'sunscreen', 'breakout', 'pimple', 'peel', 'flake', 'redness', 'epiduo', 'retin', 'papule', 'ziana', 'oily', 'neutrogena', 'clinique', 'acanya', 'complexion', 'moisturiser']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cycle .. neighbors: ['cycle', 'period', 'menstrual', 'lighter', 'safyral', 'month', 'bleeding', 'menstruate', 'hormone', 'cramp', 'heavy', 'tracker', 'predictable', 'natazia', 'amethia', 'spot', 'bleed', 'mononessa', 'heavier', 'pill']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: price .. neighbors: ['price', 'pay', 'retail', 'formulary', 'insurance', 'paid', 'fee', 'dollar', 'usa', 'tag', 'pocket', 'citizen', 'drugstore', 'astronomical', 'restasis', 'coupon', 'astonish', 'cheap', 'pollen', 'copays']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: best .. neighbors: ['best', 'try', 'good', 'great', 'work', 'bet', 'take', 'sacroiliac', 'ive', 'methe', 'tricyclic', 'help', 'youth', 'adherence', 'knowledgeable', 'sceptical', 'astelin', 'different', 'psychology', 'pharmacy']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: efficacy .. neighbors: ['efficacy', 'generess', 'buti', 'version', 'maintenance', 'unisom', 'weighti', 'threshold', 'simponi', 'approval', 'reluctantly', 'sufficient', 'psoriatic', 'accustom', 'national', 'intolerable', 'ssrisnris', 'fee', 'attest', 'pdoc']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: performance .. neighbors: ['performance', 'sexual', 'cialis', 'bph', 'semen', 'erection', 'ratio', 'distraction', 'climax', 'avodart', 'stendra', 'viagra', 'benign', 'smarter', 'elate', 'stamen', 'prostate', 'anorgasmia', 'marriage', 'adhdadd']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cancer .. neighbors: ['cancer', 'ibrance', 'radiation', 'chemo', 'lymph', 'node', 'tarceva', 'prostate', 'zoladex', 'femara', 'oncologist', 'chemotherapy', 'leukemia', 'sutent', 'alfuzosin', 'carcinoma', 'arimidex', 'casodex', 'psa', 'avastin']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: disease .. neighbors: ['disease', 'degenerative', 'crohn', 'autoimmune', 'lyme', 'obstructive', 'herniated', 'diagnose', 'fistula', 'classify', 'lumbar', 'hashimotos', 'disc', 'remicade', 'currently', 'inflammatory', 'ankylose', 'kapidex', 'herniation', 'leukemia']\n",
      "####################################################################################################\n",
      "Epoch: 9 .. batch: 2/135 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 304.25 .. NELBO: 304.74\n",
      "Epoch: 9 .. batch: 4/135 .. LR: 0.005 .. KL_theta: 0.46 .. Rec_loss: 303.81 .. NELBO: 304.27\n",
      "Epoch: 9 .. batch: 6/135 .. LR: 0.005 .. KL_theta: 0.45 .. Rec_loss: 302.57 .. NELBO: 303.02\n",
      "Epoch: 9 .. batch: 8/135 .. LR: 0.005 .. KL_theta: 0.45 .. Rec_loss: 302.75 .. NELBO: 303.2\n",
      "Epoch: 9 .. batch: 10/135 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 302.88 .. NELBO: 303.35\n",
      "Epoch: 9 .. batch: 12/135 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 304.03 .. NELBO: 304.51\n",
      "Epoch: 9 .. batch: 14/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.67 .. NELBO: 306.17\n",
      "Epoch: 9 .. batch: 16/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 305.18 .. NELBO: 305.7\n",
      "Epoch: 9 .. batch: 18/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 305.04 .. NELBO: 305.56\n",
      "Epoch: 9 .. batch: 20/135 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 304.1 .. NELBO: 304.62\n",
      "Epoch: 9 .. batch: 22/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.6 .. NELBO: 303.11\n",
      "Epoch: 9 .. batch: 24/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.31 .. NELBO: 302.82\n",
      "Epoch: 9 .. batch: 26/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 303.3 .. NELBO: 303.81\n",
      "Epoch: 9 .. batch: 28/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.57 .. NELBO: 303.08\n",
      "Epoch: 9 .. batch: 30/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.34 .. NELBO: 302.85\n",
      "Epoch: 9 .. batch: 32/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 302.96 .. NELBO: 303.47\n",
      "Epoch: 9 .. batch: 34/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 303.59 .. NELBO: 304.1\n",
      "Epoch: 9 .. batch: 36/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 304.23 .. NELBO: 304.74\n",
      "Epoch: 9 .. batch: 38/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 303.56 .. NELBO: 304.07\n",
      "Epoch: 9 .. batch: 40/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 303.78 .. NELBO: 304.29\n",
      "Epoch: 9 .. batch: 42/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 304.22 .. NELBO: 304.73\n",
      "Epoch: 9 .. batch: 44/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 304.73 .. NELBO: 305.24\n",
      "Epoch: 9 .. batch: 46/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 304.8 .. NELBO: 305.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 .. batch: 48/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 304.61 .. NELBO: 305.12\n",
      "Epoch: 9 .. batch: 50/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 304.44 .. NELBO: 304.95\n",
      "Epoch: 9 .. batch: 52/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 304.22 .. NELBO: 304.73\n",
      "Epoch: 9 .. batch: 54/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 304.68 .. NELBO: 305.19\n",
      "Epoch: 9 .. batch: 56/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 305.13 .. NELBO: 305.64\n",
      "Epoch: 9 .. batch: 58/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 305.03 .. NELBO: 305.54\n",
      "Epoch: 9 .. batch: 60/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 304.93 .. NELBO: 305.43\n",
      "Epoch: 9 .. batch: 62/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 304.95 .. NELBO: 305.45\n",
      "Epoch: 9 .. batch: 64/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.15 .. NELBO: 305.65\n",
      "Epoch: 9 .. batch: 66/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.37 .. NELBO: 305.87\n",
      "Epoch: 9 .. batch: 68/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.38 .. NELBO: 305.88\n",
      "Epoch: 9 .. batch: 70/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.02 .. NELBO: 305.52\n",
      "Epoch: 9 .. batch: 72/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.0 .. NELBO: 305.5\n",
      "Epoch: 9 .. batch: 74/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.23 .. NELBO: 305.73\n",
      "Epoch: 9 .. batch: 76/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.49 .. NELBO: 305.99\n",
      "Epoch: 9 .. batch: 78/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.24 .. NELBO: 305.74\n",
      "Epoch: 9 .. batch: 80/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.14 .. NELBO: 305.64\n",
      "Epoch: 9 .. batch: 82/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.19 .. NELBO: 305.69\n",
      "Epoch: 9 .. batch: 84/135 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 305.31 .. NELBO: 305.82\n",
      "Epoch: 9 .. batch: 86/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.04 .. NELBO: 305.54\n",
      "Epoch: 9 .. batch: 88/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 304.93 .. NELBO: 305.43\n",
      "Epoch: 9 .. batch: 90/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 304.79 .. NELBO: 305.29\n",
      "Epoch: 9 .. batch: 92/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 304.89 .. NELBO: 305.39\n",
      "Epoch: 9 .. batch: 94/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.41 .. NELBO: 305.91\n",
      "Epoch: 9 .. batch: 96/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.36 .. NELBO: 305.86\n",
      "Epoch: 9 .. batch: 98/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.25 .. NELBO: 305.75\n",
      "Epoch: 9 .. batch: 100/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.46 .. NELBO: 305.96\n",
      "Epoch: 9 .. batch: 102/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.44 .. NELBO: 305.94\n",
      "Epoch: 9 .. batch: 104/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.5 .. NELBO: 306.0\n",
      "Epoch: 9 .. batch: 106/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.25 .. NELBO: 305.75\n",
      "Epoch: 9 .. batch: 108/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.29 .. NELBO: 305.79\n",
      "Epoch: 9 .. batch: 110/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.14 .. NELBO: 305.64\n",
      "Epoch: 9 .. batch: 112/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.28 .. NELBO: 305.78\n",
      "Epoch: 9 .. batch: 114/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.34 .. NELBO: 305.84\n",
      "Epoch: 9 .. batch: 116/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.01 .. NELBO: 305.51\n",
      "Epoch: 9 .. batch: 118/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 304.99 .. NELBO: 305.49\n",
      "Epoch: 9 .. batch: 120/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 304.7 .. NELBO: 305.2\n",
      "Epoch: 9 .. batch: 122/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 304.75 .. NELBO: 305.25\n",
      "Epoch: 9 .. batch: 124/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 304.97 .. NELBO: 305.47\n",
      "Epoch: 9 .. batch: 126/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 305.33 .. NELBO: 305.83\n",
      "Epoch: 9 .. batch: 130/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.92 .. NELBO: 303.42\n",
      "Epoch: 9 .. batch: 132/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.8 .. NELBO: 303.3\n",
      "Epoch: 9 .. batch: 134/135 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.82 .. NELBO: 303.32\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 302.82 .. NELBO: 303.32\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 876.8\n",
      "****************************************************************************************************\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0: ['quotdoctor', 'postmeal', 'fogwas', 'yearstried', 'upscale', 'chillstripled', 'laydown', 'wellbutrine', 'valiumis']\n",
      "Topic 1: ['postmeal', 'quotdoctor', 'yearstried', 'upscale', 'chillstripled', 'laydown', 'wellbutrine', 'fogwas', 'quotmyquot']\n",
      "Topic 2: ['quotdoctor', 'chillstripled', 'postmeal', 'upscale', 'wellbutrine', 'laydown', 'yearstried', 'fogwas', 'quotmyquot']\n",
      "Topic 3: ['quotdoctor', 'postmeal', 'chillstripled', 'wellbutrine', 'upscale', 'yearstried', 'fogwas', 'laydown', 'quotmyquot']\n",
      "Topic 4: ['quotdoctor', 'postmeal', 'upscale', 'laydown', 'chillstripled', 'wellbutrine', 'fogwas', 'yearstried', 'quotmyquot']\n",
      "Topic 5: ['postmeal', 'upscale', 'quotdoctor', 'laydown', 'chillstripled', 'yearstried', 'fogwas', 'valiumis', 'wellbutrine']\n",
      "Topic 6: ['period', 'gain', 'weight', 'acne', 'birth', 'cramp', 'swing', 'sex', 'bleeding']\n",
      "Topic 7: ['quotdoctor', 'postmeal', 'upscale', 'laydown', 'chillstripled', 'fogwas', 'yearstried', 'wellbutrine', 'resultsplaques']\n",
      "Topic 8: ['quotdoctor', 'postmeal', 'chillstripled', 'fogwas', 'laydown', 'upscale', 'wellbutrine', 'yearstried', 'valiumis']\n",
      "Topic 9: ['quotdoctor', 'chillstripled', 'postmeal', 'upscale', 'laydown', 'yearstried', 'wellbutrine', 'fogwas', 'resultsplaques']\n",
      "Topic 10: ['quotdoctor', 'postmeal', 'chillstripled', 'wellbutrine', 'upscale', 'laydown', 'yearstried', 'fogwas', 'buckswhich']\n",
      "Topic 11: ['quotdoctor', 'postmeal', 'chillstripled', 'upscale', 'yearstried', 'laydown', 'wellbutrine', 'fogwas', 'propaganda']\n",
      "Topic 12: ['quotdoctor', 'postmeal', 'chillstripled', 'upscale', 'laydown', 'wellbutrine', 'fogwas', 'yearstried', 'buckswhich']\n",
      "Topic 13: ['quotdoctor', 'postmeal', 'chillstripled', 'fogwas', 'wellbutrine', 'upscale', 'yearstried', 'laydown', 'quotmyquot']\n",
      "Topic 14: ['quotdoctor', 'postmeal', 'chillstripled', 'fogwas', 'laydown', 'wellbutrine', 'upscale', 'yearstried', 'quotmyquot']\n",
      "####################################################################################################\n",
      "Visualize word embeddings by using output embedding matrix\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: skin .. neighbors: ['skin', 'face', 'clear', 'acne', 'sunscreen', 'breakout', 'pimple', 'peel', 'flake', 'redness', 'epiduo', 'retin', 'papule', 'ziana', 'oily', 'neutrogena', 'clinique', 'acanya', 'complexion', 'moisturiser']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cycle .. neighbors: ['cycle', 'period', 'menstrual', 'lighter', 'safyral', 'month', 'bleeding', 'menstruate', 'hormone', 'cramp', 'heavy', 'tracker', 'predictable', 'natazia', 'amethia', 'spot', 'bleed', 'mononessa', 'heavier', 'pill']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: price .. neighbors: ['price', 'pay', 'retail', 'formulary', 'insurance', 'paid', 'fee', 'dollar', 'usa', 'tag', 'pocket', 'citizen', 'drugstore', 'astronomical', 'restasis', 'coupon', 'astonish', 'cheap', 'pollen', 'copays']\n",
      "querry doesn't exist!!\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: best .. neighbors: ['best', 'try', 'good', 'great', 'work', 'bet', 'take', 'sacroiliac', 'ive', 'methe', 'tricyclic', 'help', 'youth', 'adherence', 'knowledgeable', 'sceptical', 'astelin', 'different', 'psychology', 'pharmacy']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: efficacy .. neighbors: ['efficacy', 'generess', 'buti', 'version', 'maintenance', 'unisom', 'weighti', 'threshold', 'simponi', 'approval', 'reluctantly', 'sufficient', 'psoriatic', 'accustom', 'national', 'intolerable', 'ssrisnris', 'fee', 'attest', 'pdoc']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: performance .. neighbors: ['performance', 'sexual', 'cialis', 'bph', 'semen', 'erection', 'ratio', 'distraction', 'climax', 'avodart', 'stendra', 'viagra', 'benign', 'smarter', 'elate', 'stamen', 'prostate', 'anorgasmia', 'marriage', 'adhdadd']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: cancer .. neighbors: ['cancer', 'ibrance', 'radiation', 'chemo', 'lymph', 'node', 'tarceva', 'prostate', 'zoladex', 'femara', 'oncologist', 'chemotherapy', 'leukemia', 'sutent', 'alfuzosin', 'carcinoma', 'arimidex', 'casodex', 'psa', 'avastin']\n",
      "vectors:  (19856, 200)\n",
      "query:  (200,)\n",
      "word: disease .. neighbors: ['disease', 'degenerative', 'crohn', 'autoimmune', 'lyme', 'obstructive', 'herniated', 'diagnose', 'fistula', 'classify', 'lumbar', 'hashimotos', 'disc', 'remicade', 'currently', 'inflammatory', 'ankylose', 'kapidex', 'herniation', 'leukemia']\n",
      "####################################################################################################\n",
      "****************************************************************************************************\n",
      "VAL Doc Completion PPL: 876.8\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if mode == 'train':\n",
    "    ## train model on data \n",
    "    best_epoch = 0\n",
    "    best_val_ppl = 1e9\n",
    "    all_val_ppls = []\n",
    "    print('\\n')\n",
    "    print('Visualizing model quality before training...')\n",
    "    visualize(model)\n",
    "    print('\\n')\n",
    "    for epoch in range(1, epochs):\n",
    "        train(epoch)\n",
    "        val_ppl = evaluate(model, 'val')\n",
    "        if val_ppl < best_val_ppl:\n",
    "            with open(ckpt, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_epoch = epoch\n",
    "            best_val_ppl = val_ppl\n",
    "        else:\n",
    "            ## check whether to anneal lr\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            if anneal_lr and (len(all_val_ppls) > nonmono and val_ppl > min(all_val_ppls[:-nonmono]) and lr > 1e-5):\n",
    "                optimizer.param_groups[0]['lr'] /= lr_factor\n",
    "        if epoch % visualize_every == 0:\n",
    "            visualize(model)\n",
    "        all_val_ppls.append(val_ppl)\n",
    "    with open(ckpt, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    model = model.to(device)\n",
    "    val_ppl = evaluate(model, 'val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "TEST Doc Completion PPL: 876.8\n",
      "****************************************************************************************************\n",
      "batch: 100/135\n",
      "\n",
      "The 10 most used topics are [ 6  4  9  5 14 12  2 10  7 11]\n",
      "\n",
      "\n",
      "Topic 0: ['quotdoctor', 'postmeal', 'fogwas', 'yearstried', 'upscale', 'chillstripled', 'laydown', 'wellbutrine', 'valiumis']\n",
      "Topic 1: ['postmeal', 'quotdoctor', 'yearstried', 'upscale', 'chillstripled', 'laydown', 'wellbutrine', 'fogwas', 'quotmyquot']\n",
      "Topic 2: ['quotdoctor', 'chillstripled', 'postmeal', 'upscale', 'wellbutrine', 'laydown', 'yearstried', 'fogwas', 'quotmyquot']\n",
      "Topic 3: ['quotdoctor', 'postmeal', 'chillstripled', 'wellbutrine', 'upscale', 'yearstried', 'fogwas', 'laydown', 'quotmyquot']\n",
      "Topic 4: ['quotdoctor', 'postmeal', 'upscale', 'laydown', 'chillstripled', 'wellbutrine', 'fogwas', 'yearstried', 'quotmyquot']\n",
      "Topic 5: ['postmeal', 'upscale', 'quotdoctor', 'laydown', 'chillstripled', 'yearstried', 'fogwas', 'valiumis', 'wellbutrine']\n",
      "Topic 6: ['period', 'gain', 'weight', 'acne', 'birth', 'cramp', 'swing', 'sex', 'bleeding']\n",
      "Topic 7: ['quotdoctor', 'postmeal', 'upscale', 'laydown', 'chillstripled', 'fogwas', 'yearstried', 'wellbutrine', 'resultsplaques']\n",
      "Topic 8: ['quotdoctor', 'postmeal', 'chillstripled', 'fogwas', 'laydown', 'upscale', 'wellbutrine', 'yearstried', 'valiumis']\n",
      "Topic 9: ['quotdoctor', 'chillstripled', 'postmeal', 'upscale', 'laydown', 'yearstried', 'wellbutrine', 'fogwas', 'resultsplaques']\n",
      "Topic 10: ['quotdoctor', 'postmeal', 'chillstripled', 'wellbutrine', 'upscale', 'laydown', 'yearstried', 'fogwas', 'buckswhich']\n",
      "Topic 11: ['quotdoctor', 'postmeal', 'chillstripled', 'upscale', 'yearstried', 'laydown', 'wellbutrine', 'fogwas', 'propaganda']\n",
      "Topic 12: ['quotdoctor', 'postmeal', 'chillstripled', 'upscale', 'laydown', 'wellbutrine', 'fogwas', 'yearstried', 'buckswhich']\n",
      "Topic 13: ['quotdoctor', 'postmeal', 'chillstripled', 'fogwas', 'wellbutrine', 'upscale', 'yearstried', 'laydown', 'quotmyquot']\n",
      "Topic 14: ['quotdoctor', 'postmeal', 'chillstripled', 'fogwas', 'laydown', 'wellbutrine', 'upscale', 'yearstried', 'quotmyquot']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(ckpt, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ## get document completion perplexities\n",
    "    test_ppl = evaluate(model, 'test', tc=tc, td=td)\n",
    "\n",
    "    ## get most used topics\n",
    "    indices = torch.tensor(range(num_docs_train))\n",
    "    indices = torch.split(indices, batch_size)\n",
    "    thetaAvg = torch.zeros(1, num_topics).to(device)\n",
    "    thetaWeightedAvg = torch.zeros(1, num_topics).to(device)\n",
    "    cnt = 0\n",
    "    for idx, ind in enumerate(indices):\n",
    "        try:\n",
    "            data_batch = data.get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "            sums = data_batch.sum(1).unsqueeze(1)\n",
    "            cnt += sums.sum(0).squeeze().cpu().numpy()\n",
    "            if bow_norm:\n",
    "                normalized_data_batch = data_batch / sums\n",
    "            else:\n",
    "                normalized_data_batch = data_batch\n",
    "            theta, _ = model.get_theta(normalized_data_batch)\n",
    "            thetaAvg += theta.sum(0).unsqueeze(0) / num_docs_train\n",
    "            weighed_theta = sums * theta\n",
    "            thetaWeightedAvg += weighed_theta.sum(0).unsqueeze(0)\n",
    "            if idx % 100 == 0 and idx > 0:\n",
    "                print('batch: {}/{}'.format(idx, len(indices)))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    thetaWeightedAvg = thetaWeightedAvg.squeeze().cpu().numpy() / cnt\n",
    "    print('\\nThe 10 most used topics are {}'.format(thetaWeightedAvg.argsort()[::-1][:10]))\n",
    "\n",
    "    ## show topics\n",
    "    beta = model.get_beta()\n",
    "    topic_indices = list(np.random.choice(num_topics, 10)) # 10 random topics\n",
    "    print('\\n')\n",
    "    for k in range(num_topics):#topic_indices:\n",
    "        gamma = beta[k]\n",
    "        top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "        topic_words = [vocab[a] for a in top_words]\n",
    "        print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "    if train_embeddings:\n",
    "        ## show etm embeddings \n",
    "        try:\n",
    "            rho_etm = model.rho.weight.cpu()\n",
    "        except:\n",
    "            rho_etm = model.rho.cpu()\n",
    "        queries = ['andrew', 'woman', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                        'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "        print('\\n')\n",
    "        print('ETM embeddings...')\n",
    "        for word in queries:\n",
    "            print('word: {} .. etm neighbors: {}'.format(word, nearest_neighbors(word, rho_etm, vocab)))\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python main.py --mode train --dataset 20ng --data_path data/20ng --num_topics 50 --train_embeddings 1 --epochs 1000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AZ Social Media Analytics)",
   "language": "python",
   "name": "atsc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
